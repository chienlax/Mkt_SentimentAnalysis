{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d936df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from spellchecker import SpellChecker\n",
    "import emoji\n",
    "\n",
    "# Load spaCy model (optional, if using spaCy for tokenization/lemmatization)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize lemmatizer and stemmer (if using NLTK)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Get standard English stop words (customize this list!)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- CRITICAL: Customize Stopwords ---\n",
    "# Remove words that might be important for sentiment (negations, intensifiers)\n",
    "words_to_keep = {\"no\", \"not\", \"nor\", \"very\", \"too\", \"against\", \"don't\", \"doesn't\", \"didn't\", \"shouldn't\", \"couldn't\", \"won't\", \"wouldn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\"}\n",
    "stop_words = stop_words - words_to_keep\n",
    "\n",
    "# Add any other domain-specific stop words if needed\n",
    "# stop_words.add(\"book\") # Example if 'book' is too common and uninformative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b6086",
   "metadata": {},
   "source": [
    "Removing stop words, stemming, and lemmatization simplifies the data but can sometimes remove important nuances, especially for sophisticated DL models. It's often best to start with minimal processing **(cleaning, lowercasing, tokenizing)** and add steps like stop word removal or lemmatization only if needed based on initial model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeaa457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Cleaning ---\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Removes HTML tags from text.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Removes URLs from text.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# --- Lowercasing ---\n",
    "def lowercase_text(text):\n",
    "    \"\"\"Converts text to lowercase.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    return text\n",
    "\n",
    "# --- Handling Rare Words and Slang (Requires Custom Dictionary) ---\n",
    "# This is highly domain/data specific. Example placeholder:\n",
    "rare_word_map = {\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"brb\": \"be right back\"\n",
    "}\n",
    "def handle_rare_slang(text, mapping_dict):\n",
    "    \"\"\"Replaces known slang/rare words based on a dictionary.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        corrected_words = [mapping_dict.get(word, word) for word in words]\n",
    "        return \" \".join(corrected_words)\n",
    "    return text\n",
    "\n",
    "# --- Expanding Contractions ---\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expands contractions like \"don't\" to \"do not\".\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "# --- Removing Punctuation ---\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Removes standard punctuation. Keeps apostrophes within words.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Keep apostrophes for contractions handled earlier, remove others\n",
    "        translator = str.maketrans('', '', string.punctuation.replace(\"'\", \"\"))\n",
    "        return text.translate(translator)\n",
    "    return text\n",
    "    # --- Alternative: Keep specific punctuation like '!' or '?' ---\n",
    "    # punctuation_to_keep = \"!?\"\n",
    "    # punctuation_to_remove = ''.join([p for p in string.punctuation if p not in punctuation_to_keep])\n",
    "    # translator = str.maketrans('', '', punctuation_to_remove)\n",
    "    # return text.translate(translator)\n",
    "\n",
    "\n",
    "# --- Tokenization ---\n",
    "def tokenize_text_nltk(text):\n",
    "    \"\"\"Tokenizes text into words using NLTK.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return word_tokenize(text)\n",
    "    return [] # Return empty list for non-string input\n",
    "\n",
    "# --- Removing Stop Words ---\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Removes stop words from a list of tokens.\"\"\"\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 1] # Also remove single chars\n",
    "\n",
    "# --- Stemming and Lemmatization ---\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Applies Porter stemming to a list of tokens.\"\"\"\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Applies WordNet lemmatization to a list of tokens.\"\"\"\n",
    "    # Note: Lemmatization is often more accurate with Part-of-Speech tags,\n",
    "    # but that adds complexity (requires PoS tagging first).\n",
    "    # This basic version assumes words are nouns by default if tag unknown.\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# --- Spell Checking and Correction (Use with Caution!) ---\n",
    "def correct_spelling(tokens):\n",
    "    \"\"\"Corrects spelling for a list of tokens. Can be slow and sometimes inaccurate.\"\"\"\n",
    "    # Find potentially misspelled words\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    corrected_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Only correct if a correction is found, otherwise keep original\n",
    "            corrected_tokens.append(corrected_word if corrected_word else word)\n",
    "        else:\n",
    "            corrected_tokens.append(word)\n",
    "    return corrected_tokens\n",
    "\n",
    "# --- Handling Emojis and Emoticons ---\n",
    "def handle_emojis(text):\n",
    "    \"\"\"Converts emojis to descriptive text (e.g., :smile:) or removes them.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Option 1: Convert to text description\n",
    "        text = emoji.demojize(text, delimiters=(\" _EMOJI_\", \"_ \")) # Add spaces for tokenization\n",
    "        # Option 2: Remove emojis completely\n",
    "        # text = emoji.replace_emoji(text, replace='')\n",
    "        return text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84633948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_pipeline(text,\n",
    "                             remove_html=True,\n",
    "                             remove_url=True,\n",
    "                             handle_emoji=True, # Convert emojis to text\n",
    "                             expand_contract=True,\n",
    "                             do_lowercase=True,\n",
    "                             handle_slang=False, # Set to True if you have a good slang_map\n",
    "                             slang_map=rare_word_map,\n",
    "                             remove_punct=True,\n",
    "                             do_spell_correct=False, # CAUTION: Slow and potentially error-prone\n",
    "                             do_tokenize=True,\n",
    "                             remove_stop=True,\n",
    "                             do_lemmatize=False, # Choose lemmatize OR stem\n",
    "                             do_stem=False):\n",
    "    \"\"\"Applies a sequence of preprocessing steps to raw text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" # Or return empty list if tokenizing\n",
    "\n",
    "    # 1. Clean HTML and URLs\n",
    "    if remove_html:\n",
    "        text = remove_html_tags(text)\n",
    "    if remove_url:\n",
    "        text = remove_urls(text)\n",
    "\n",
    "    # 2. Handle Emojis (convert to text before lowercasing/punctuation removal)\n",
    "    if handle_emoji:\n",
    "        text = handle_emojis(text)\n",
    "\n",
    "    # 3. Expand Contractions (before punctuation removal)\n",
    "    if expand_contract:\n",
    "        text = expand_contractions(text)\n",
    "\n",
    "    # 4. Lowercase\n",
    "    if do_lowercase:\n",
    "        text = lowercase_text(text)\n",
    "\n",
    "    # 5. Handle Slang/Rare words (after lowercasing)\n",
    "    if handle_slang:\n",
    "        text = handle_rare_slang(text, slang_map)\n",
    "\n",
    "    # 6. Remove Punctuation (after contractions)\n",
    "    if remove_punct:\n",
    "        text = remove_punctuation(text)\n",
    "\n",
    "    # --- Tokenization is the central point ---\n",
    "    if not do_tokenize:\n",
    "        # If not tokenizing, usually return the cleaned string\n",
    "        # Remove extra whitespace that might result from removals\n",
    "        return ' '.join(text.split())\n",
    "    else:\n",
    "        tokens = tokenize_text_nltk(text) # Use NLTK tokenizer\n",
    "\n",
    "        # 7. Spell Correction (on tokens - USE WITH CAUTION)\n",
    "        if do_spell_correct:\n",
    "            tokens = correct_spelling(tokens)\n",
    "\n",
    "        # 8. Remove Stopwords (on tokens)\n",
    "        if remove_stop:\n",
    "            tokens = remove_stopwords(tokens)\n",
    "\n",
    "        # 9. Lemmatize OR Stem (on tokens)\n",
    "        if do_lemmatize:\n",
    "            tokens = lemmatize_tokens(tokens)\n",
    "        elif do_stem:\n",
    "            tokens = stem_tokens(tokens)\n",
    "\n",
    "        # Return list of processed tokens\n",
    "        return tokens # Or join back into string: ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d88ea",
   "metadata": {},
   "source": [
    "### For book review:\n",
    "- Handle irony for negative review\n",
    "- Handling abbreviations, correcting improper punctuation, and rectifying spelling mistakes (text writen by human so there will 100% be grammatical or lexical errors, how to handle that?)\n",
    "\n",
    "### For financial text:\n",
    "- All the standard preprocessing steps mentioned above (cleaning, lowercasing, tokenization, stop word removal, stemming/lemmatization, handling special characters, etc.) are generally applied.   \n",
    "- Handling Financial Jargon and Abbreviations: Financial texts contain specific terminology and abbreviations that might need to be handled appropriately, possibly through expansion or standardization.   \n",
    "- Normalization of Financial Terms: Ensuring consistent representation of financial entities and concepts.\n",
    "- Removal of Financial Indicators: Depending on the task, symbols like '$', '€', or stock tickers might be removed or treated specially.\n",
    "- Domain-Specific Stop Word Lists: Using stop word lists tailored to the financial domain, as some common words might carry sentiment in a financial context.   \n",
    "- Handling Numerical Data: Deciding how to treat numerical values, which can be significant in financial texts.\n",
    "- Negation Handling: Crucial in finance as negations can significantly alter the sentiment (e.g., \"not profitable\"). Specific negation handling algorithms might be employed.   \n",
    "- Domain Adaptation: When labeled financial data is scarce, techniques like domain adaptation might be used to leverage sentiment knowledge from other domains.  \n",
    "\n",
    "These shit will be handle later, now I have to build a simple pre-processing pipeline to run and test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9ac4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_reviews = pd.read_csv('../data/processed/book_reviews/book_reviews_sample.csv', encoding='utf-8-sig')\n",
    "financial_news = pd.read_csv('../data/processed/financial_news/financial_news_train.csv', encoding='latin-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
