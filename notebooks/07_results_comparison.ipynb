{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885b079c",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\") # Example color palette\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e2135",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "RESULTS_DIR = \"../results\"\n",
    "ML_RESULTS_CSV = os.path.join(RESULTS_DIR, \"ml_results_summary.csv\")\n",
    "DL_RESULTS_CSV = os.path.join(RESULTS_DIR, \"dl_results_summary.csv\")\n",
    "LLM_RESULTS_CSV = os.path.join(RESULTS_DIR, \"llm_results_summary.csv\")\n",
    "\n",
    "FINAL_RESULTS_CSV = os.path.join(RESULTS_DIR, \"comparison\", \"all_models_comparison.csv\")\n",
    "PLOTS_SAVE_DIR = os.path.join(RESULTS_DIR, \"comparison\", \"plots\")\n",
    "\n",
    "# --- Comparison Settings ---\n",
    "PRIMARY_METRIC = \"F1 (Macro)\" # Metric to use for ranking models ('Accuracy', 'F1 (Weighted)', etc.)\n",
    "METRICS_TO_PLOT = [\"Accuracy\", \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\"]\n",
    "TIME_METRICS_TO_PLOT = [\"Train Time (s)\", \"Eval Time (s)\"]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(FINAL_RESULTS_CSV), exist_ok=True)\n",
    "os.makedirs(PLOTS_SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45172e6",
   "metadata": {},
   "source": [
    "# 3. Load Individual Results Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml, df_dl, df_llm = None, None, None\n",
    "\n",
    "try:\n",
    "    df_ml = pd.read_csv(ML_RESULTS_CSV)\n",
    "    print(f\"Loaded ML results: {df_ml.shape[0]} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: ML results file not found at {ML_RESULTS_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ML results: {e}\")\n",
    "\n",
    "try:\n",
    "    df_dl = pd.read_csv(DL_RESULTS_CSV)\n",
    "    print(f\"Loaded DL results: {df_dl.shape[0]} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: DL results file not found at {DL_RESULTS_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DL results: {e}\")\n",
    "\n",
    "try:\n",
    "    df_llm = pd.read_csv(LLM_RESULTS_CSV)\n",
    "    print(f\"Loaded LLM results: {df_llm.shape[0]} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: LLM results file not found at {LLM_RESULTS_CSV}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5d1b1",
   "metadata": {},
   "source": [
    "# 4. Standardize and Combine DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db273b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "final_cols = ['Domain', 'Method_Category', 'Model_Name', 'Accuracy',\n",
    "              'F1 (Macro)', 'Precision (Macro)', 'Recall (Macro)',\n",
    "              'F1 (Weighted)', 'Precision (Weighted)', 'Recall (Weighted)',\n",
    "              'Train Time (s)', 'Eval Time (s)']\n",
    "\n",
    "# --- Process ML DataFrame ---\n",
    "if df_ml is not None:\n",
    "    df_ml_std = df_ml.copy()\n",
    "    df_ml_std['Method_Category'] = 'ML'\n",
    "    # Create a more descriptive model name\n",
    "    df_ml_std['Model_Name'] = df_ml_std['Model'] + '_' + df_ml_std['Feature Extractor']\n",
    "    # Select and rename columns\n",
    "    df_ml_std = df_ml_std.rename(columns={\n",
    "        'F1 (Macro)': 'F1 (Macro)', # Ensure consistency if names differ slightly\n",
    "        'Precision (Macro)': 'Precision (Macro)',\n",
    "        'Recall (Macro)': 'Recall (Macro)',\n",
    "        'F1 (Weighted)': 'F1 (Weighted)',\n",
    "        'Precision (Weighted)': 'Precision (Weighted)',\n",
    "        'Recall (Weighted)': 'Recall (Weighted)',\n",
    "        'Train Time (s)': 'Train Time (s)',\n",
    "        'Eval Time (s)': 'Eval Time (s)'\n",
    "    })\n",
    "    # Keep only the final desired columns, adding NaNs if any are missing\n",
    "    for col in final_cols:\n",
    "        if col not in df_ml_std.columns: df_ml_std[col] = np.nan\n",
    "    all_dfs.append(df_ml_std[final_cols])\n",
    "    print(\"Processed ML DataFrame.\")\n",
    "\n",
    "# --- Process DL DataFrame ---\n",
    "if df_dl is not None:\n",
    "    df_dl_std = df_dl.copy()\n",
    "    df_dl_std['Method_Category'] = 'DL'\n",
    "    df_dl_std['Model_Name'] = df_dl_std['Model'] + '_' + df_dl_std['Embedding']\n",
    "    df_dl_std = df_dl_std.rename(columns={\n",
    "        'F1 (Macro)': 'F1 (Macro)',\n",
    "        'Precision (Macro)': 'Precision (Macro)',\n",
    "        'Recall (Macro)': 'Recall (Macro)',\n",
    "        'F1 (Weighted)': 'F1 (Weighted)',\n",
    "        'Precision (Weighted)': 'Precision (Weighted)',\n",
    "        'Recall (Weighted)': 'Recall (Weighted)',\n",
    "        'Train Time (s)': 'Train Time (s)',\n",
    "        'Eval Time (s)': 'Eval Time (s)'\n",
    "    })\n",
    "    for col in final_cols:\n",
    "        if col not in df_dl_std.columns: df_dl_std[col] = np.nan\n",
    "    all_dfs.append(df_dl_std[final_cols])\n",
    "    print(\"Processed DL DataFrame.\")\n",
    "\n",
    "# --- Process LLM DataFrame ---\n",
    "if df_llm is not None:\n",
    "    df_llm_std = df_llm.copy()\n",
    "    # Create Method Category based on 'Method' column\n",
    "    method_map = {\n",
    "        'finetune': 'LLM_FT',\n",
    "        'lora': 'LLM_LoRA',\n",
    "        'feature_extraction': 'LLM_FE'\n",
    "    }\n",
    "    df_llm_std['Method_Category'] = df_llm_std['Method'].map(method_map).fillna('LLM_Other')\n",
    "    # Use 'Short Name' as the Model_Name for LLMs\n",
    "    df_llm_std['Model_Name'] = df_llm_std['Short Name']\n",
    "    df_llm_std = df_llm_std.rename(columns={\n",
    "        'F1 (Macro)': 'F1 (Macro)',\n",
    "        'Precision (Macro)': 'Precision (Macro)',\n",
    "        'Recall (Macro)': 'Recall (Macro)',\n",
    "        'F1 (Weighted)': 'F1 (Weighted)',\n",
    "        'Precision (Weighted)': 'Precision (Weighted)',\n",
    "        'Recall (Weighted)': 'Recall (Weighted)',\n",
    "        'Train Time (s)': 'Train Time (s)',\n",
    "        'Eval Time (s)': 'Eval Time (s)'\n",
    "    })\n",
    "    for col in final_cols:\n",
    "        if col not in df_llm_std.columns: df_llm_std[col] = np.nan\n",
    "    all_dfs.append(df_llm_std[final_cols])\n",
    "    print(\"Processed LLM DataFrame.\")\n",
    "\n",
    "# --- Combine ---\n",
    "if not all_dfs:\n",
    "    print(\"No dataframes to combine. Exiting.\")\n",
    "    # Exit or handle appropriately\n",
    "    combined_df = pd.DataFrame(columns=final_cols)\n",
    "else:\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # Optional: Round numeric columns for display\n",
    "    numeric_cols = combined_df.select_dtypes(include=np.number).columns\n",
    "    combined_df[numeric_cols] = combined_df[numeric_cols].round(4)\n",
    "    print(f\"\\nCombined DataFrame shape: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b1898",
   "metadata": {},
   "source": [
    "# 5. Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Combined Results ---\")\n",
    "# Display full dataframe using to_string for better notebook output\n",
    "print(combined_df.to_string())\n",
    "\n",
    "# --- Find Best Model per Domain ---\n",
    "print(f\"\\n--- Best Model per Domain based on {PRIMARY_METRIC} ---\")\n",
    "best_models = combined_df.loc[combined_df.groupby('Domain')[PRIMARY_METRIC].idxmax()]\n",
    "print(best_models[['Domain', 'Method_Category', 'Model_Name', PRIMARY_METRIC]].to_string())\n",
    "\n",
    "# --- Average Performance by Method Category ---\n",
    "print(f\"\\n--- Average Performance by Method Category (using {PRIMARY_METRIC}) ---\")\n",
    "avg_performance = combined_df.groupby('Method_Category')[PRIMARY_METRIC].mean().sort_values(ascending=False)\n",
    "print(avg_performance.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583178e3",
   "metadata": {},
   "source": [
    "# 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    domains = combined_df['Domain'].unique()\n",
    "\n",
    "    # --- Plot Performance Metrics per Domain ---\n",
    "    print(\"\\n--- Generating Performance Plots per Domain ---\")\n",
    "    for domain in domains:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        domain_df = combined_df[combined_df['Domain'] == domain].sort_values(by=PRIMARY_METRIC, ascending=False)\n",
    "\n",
    "        # Melt dataframe for easier plotting with seaborn\n",
    "        plot_df = domain_df.melt(id_vars=['Model_Name'], value_vars=METRICS_TO_PLOT,\n",
    "                                 var_name='Metric', value_name='Score')\n",
    "\n",
    "        sns.barplot(data=plot_df, x='Model_Name', y='Score', hue='Metric', palette='viridis')\n",
    "        plt.title(f'Performance Metrics for Domain: {domain} (Ordered by {PRIMARY_METRIC})')\n",
    "        plt.xlabel('Model Configuration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=75, ha='right')\n",
    "        plt.ylim(0, 1.05) # Set y-axis limit for scores between 0 and 1\n",
    "        plt.legend(title='Metric', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(PLOTS_SAVE_DIR, f\"performance_{domain}.png\")\n",
    "        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved plot: {plot_filename}\")\n",
    "        plt.close() # Close plot to avoid displaying inline if not desired / save memory\n",
    "\n",
    "    # --- Plot Average Performance by Method Category ---\n",
    "    print(\"\\n--- Generating Average Performance Plot by Category ---\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    avg_perf_df = combined_df.groupby('Method_Category')[METRICS_TO_PLOT].mean().reset_index()\n",
    "    plot_df_avg = avg_perf_df.melt(id_vars=['Method_Category'], value_vars=METRICS_TO_PLOT,\n",
    "                                   var_name='Metric', value_name='Average Score')\n",
    "    # Order categories by primary metric performance\n",
    "    category_order = avg_performance.index.tolist()\n",
    "\n",
    "    sns.barplot(data=plot_df_avg, x='Method_Category', y='Average Score', hue='Metric',\n",
    "                order=category_order, palette='viridis') # Use calculated order\n",
    "    plt.title(f'Average Performance by Method Category (Ordered by Avg {PRIMARY_METRIC})')\n",
    "    plt.xlabel('Method Category')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend(title='Metric', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(PLOTS_SAVE_DIR, f\"average_performance_by_category.png\")\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot: {plot_filename}\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- Plot Training and Evaluation Times ---\n",
    "    print(\"\\n--- Generating Time Comparison Plots ---\")\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    time_df = combined_df.sort_values(by='Train Time (s)', ascending=True) # Order by train time\n",
    "    plot_df_time = time_df.melt(id_vars=['Model_Name', 'Domain'], value_vars=TIME_METRICS_TO_PLOT,\n",
    "                                var_name='Time Type', value_name='Seconds')\n",
    "\n",
    "    sns.barplot(data=plot_df_time, x='Model_Name', y='Seconds', hue='Domain', dodge=True) # Use dodge for grouped bars\n",
    "    plt.title('Training and Evaluation Time per Model (Ordered by Avg Train Time)')\n",
    "    plt.xlabel('Model Configuration')\n",
    "    plt.ylabel('Time (Seconds)')\n",
    "    plt.xticks(rotation=75, ha='right')\n",
    "    plt.yscale('log') # Use log scale for potentially large time differences\n",
    "    plt.legend(title='Domain', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plot_filename = os.path.join(PLOTS_SAVE_DIR, f\"time_comparison.png\")\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved plot: {plot_filename}\")\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(\"Combined DataFrame is empty, skipping analysis and visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c3cbf",
   "metadata": {},
   "source": [
    "# 7. Save Final Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137661ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_df.empty:\n",
    "    try:\n",
    "        combined_df.to_csv(FINAL_RESULTS_CSV, index=False)\n",
    "        print(f\"\\nSuccessfully saved combined results to {FINAL_RESULTS_CSV}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving final combined results: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo combined results to save.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
