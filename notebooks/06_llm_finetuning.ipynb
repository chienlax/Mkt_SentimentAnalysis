{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff2def3",
   "metadata": {},
   "source": [
    "# 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b155b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel, # For feature extraction\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType # For LoRA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import evaluate \n",
    "import time\n",
    "import os\n",
    "import joblib \n",
    "import logging\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# --- Basic Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- Limit CPU Usage ---\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78372810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Directory Structure ---\n",
    "BASE_DIR = \"..\" # Assuming the notebook is in a 'notebooks' or similar folder\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "# Models and results saved within dataset-specific folders\n",
    "MODEL_OUTPUT_BASE_DIR = os.path.join(BASE_DIR, \"models\", \"llm\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"result\")\n",
    "\n",
    "# --- Specific Dataset Paths ---\n",
    "BOOK_REVIEW_DATA_DIR = os.path.join(DATA_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_DATA_DIR = os.path.join(DATA_DIR, \"financial_news\")\n",
    "\n",
    "# --- Model/Result Output Dirs (Ensure they exist) ---\n",
    "BOOK_REVIEW_MODEL_DIR = os.path.join(MODEL_OUTPUT_BASE_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_MODEL_DIR = os.path.join(MODEL_OUTPUT_BASE_DIR, \"financial_news\")\n",
    "BOOK_REVIEW_RESULT_DIR = os.path.join(RESULT_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_RESULT_DIR = os.path.join(RESULT_DIR, \"financial_news\")\n",
    "\n",
    "os.makedirs(BOOK_REVIEW_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(BOOK_REVIEW_RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Names ---\n",
    "TRAIN_FN = \"train.csv\"\n",
    "VAL_FN = \"val.csv\"\n",
    "TEST_FN = \"test.csv\"\n",
    "\n",
    "# --- Column Names ---\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"score\" # Assumes string labels like 'positive', 'negative', 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7398b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:24:54,431 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Model & Training Hyperparameters ---\n",
    "RANDOM_STATE = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Tokenizer params\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Feature Extractor Params\n",
    "FEATURE_EXTRACTOR_BATCH_SIZE = 64\n",
    "LOGREG_MAX_ITER = 2000\n",
    "\n",
    "# Fine-tuning params\n",
    "LEARNING_RATE = 2e-5 \n",
    "WEIGHT_DECAY = 0.01\n",
    "TRAIN_BATCH_SIZE = 128 \n",
    "EVAL_BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 300\n",
    "FP16 = torch.cuda.is_available()\n",
    "\n",
    "# PEFT (LoRA) params\n",
    "USE_LORA = True # Flag to control if LoRA runs are included\n",
    "LORA_R = 8 # LoRA rank (dimension)\n",
    "LORA_ALPHA = 16 # LoRA alpha scaling\n",
    "LORA_DROPOUT = 0.1\n",
    "# Target modules vary by model, common ones for BERT/RoBERTa:\n",
    "LORA_TARGET_MODULES = [\"query\", \"value\"] # Common target layers for attention\n",
    "\n",
    "# --- Label Mapping (Essential for Transformers) ---\n",
    "LABEL_LIST = ['negative', 'neutral', 'positive'] # Define explicit order\n",
    "LABEL2ID = {label: i for i, label in enumerate(LABEL_LIST)}\n",
    "ID2LABEL = {i: label for i, label in enumerate(LABEL_LIST)}\n",
    "NUM_CLASSES = len(LABEL_LIST)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "METRICS_TO_CALCULATE = [\n",
    "    \"Accuracy\",\n",
    "    \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "    \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "    \"Train Time (s)\", \"Eval Time (s)\"\n",
    "]\n",
    "METRIC_FOR_BEST_MODEL = \"f1_macro\" # Metric to monitor for early stopping/best model saving\n",
    "\n",
    "# --- Datasets Configuration ---\n",
    "DATASETS_TO_PROCESS = {\n",
    "    # \"Book Review\": {\n",
    "    #     \"train_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TRAIN_FN}'),\n",
    "    #     \"val_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{VAL_FN}'),\n",
    "    #     \"test_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TEST_FN}'),\n",
    "    #     \"model_dir\": BOOK_REVIEW_MODEL_DIR,\n",
    "    #     \"result_dir\": BOOK_REVIEW_RESULT_DIR,\n",
    "    # },\n",
    "    \"Financial News\": {\n",
    "        \"train_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TEST_FN}'),\n",
    "        \"model_dir\": FINANCIAL_NEWS_MODEL_DIR,\n",
    "        \"result_dir\": FINANCIAL_NEWS_RESULT_DIR,\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Model Configurations to Run ---\n",
    "# Define the models and settings for the experiment loop\n",
    "# Format: ('Experiment Name', 'HuggingFace Model ID', use_lora_flag, is_feature_extractor_run)\n",
    "MODEL_CONFIGURATIONS = [\n",
    "    # Feature Extractors\n",
    "    # ('DistilBERT Feature Extractor + LR', 'distilbert-base-uncased', False, True),\n",
    "    # ('BERT Feature Extractor + LR',       'bert-base-uncased',       False, True),\n",
    "\n",
    "    # # Full Fine-tuning\n",
    "    # ('DistilBERT Full FT', 'distilbert-base-uncased', False, False),\n",
    "    # ('BERT Full FT',       'bert-base-uncased',       False, False),\n",
    "    # ('RoBERTa Full FT',    'roberta-base',            False, False),\n",
    "    # ('FinBERT Full FT',    'ProsusAI/finbert',        False, False), # Domain-specific\n",
    "\n",
    "    # LoRA Fine-tuning (only run if USE_LORA is True)\n",
    "    ('BERT LoRA FT',       'bert-base-uncased',       True, False),\n",
    "    ('RoBERTa LoRA FT',    'roberta-base',            True, False),\n",
    "    # ('FinBERT LoRA FT',    'ProsusAI/finbert',        True, False), # Can also apply LoRA to FinBERT\n",
    "] if USE_LORA else [ # Exclude LoRA runs if USE_LORA is False\n",
    "    ('DistilBERT Feature Extractor + LR', 'distilbert-base-uncased', False, True),\n",
    "    ('BERT Feature Extractor + LR',       'bert-base-uncased',       False, True),\n",
    "    ('DistilBERT Full FT', 'distilbert-base-uncased', False, False),\n",
    "    ('BERT Full FT',       'bert-base-uncased',       False, False),\n",
    "    ('RoBERTa Full FT',    'roberta-base',            False, False),\n",
    "    ('FinBERT Full FT',    'ProsusAI/finbert',        False, False),\n",
    "]\n",
    "\n",
    "# Check if FinBERT model ID needs adjustment (sometimes name changes)\n",
    "# Example alternative: 'yiyanghkust/finbert-tone'\n",
    "FINBERT_MODEL_ID = 'ProsusAI/finbert'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e209c0",
   "metadata": {},
   "source": [
    "# 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28006188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_hf(path):\n",
    "    \"\"\"Loads a single CSV into a Hugging Face Dataset.\"\"\"\n",
    "    try:\n",
    "        # Load directly using datasets library\n",
    "        dataset = load_dataset('csv', data_files=path, split='train')\n",
    "        # Rename target column to 'label' (expected by Trainer) and map string labels to integers\n",
    "        if TARGET_COLUMN != 'label':\n",
    "            dataset = dataset.rename_column(TARGET_COLUMN, 'label')\n",
    "        dataset = dataset.map(lambda examples: {'label': LABEL2ID.get(str(examples['label']), -1)}, # Handle potential non-string labels robustly\n",
    "                              desc=\"Mapping labels to IDs\")\n",
    "        # Filter out examples where label mapping failed (label == -1)\n",
    "        original_size = len(dataset)\n",
    "        dataset = dataset.filter(lambda example: example['label'] != -1, desc=\"Filtering invalid labels\")\n",
    "        if len(dataset) < original_size:\n",
    "            logging.warning(f\"Filtered out {original_size - len(dataset)} examples with invalid labels from {path}.\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading dataset from {path}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def create_dataset_dict(train_path, val_path, test_path):\n",
    "    \"\"\"Loads train, validation, and test CSVs into a DatasetDict.\"\"\"\n",
    "    train_ds = load_data_hf(train_path)\n",
    "    val_ds = load_data_hf(val_path)\n",
    "    test_ds = load_data_hf(test_path)\n",
    "    if train_ds and val_ds and test_ds:\n",
    "        logging.info(f\"Loaded Train data: {len(train_ds)} examples\")\n",
    "        logging.info(f\"Loaded Validation data: {len(val_ds)} examples\")\n",
    "        logging.info(f\"Loaded Test data: {len(test_ds)} examples\")\n",
    "        return DatasetDict({\n",
    "            'train': train_ds,\n",
    "            'validation': val_ds,\n",
    "            'test': test_ds\n",
    "        })\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Tokenizes text data.\"\"\"\n",
    "    # Ensure text is string, handle potential None values\n",
    "    texts = [str(text) if text is not None else \"\" for text in examples[TEXT_COLUMN]]\n",
    "    return tokenizer(texts, truncation=True, padding=False, max_length=MAX_LENGTH) # Padding handled by DataCollator\n",
    "\n",
    "# Define metric computation function for Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    prec_macro = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    rec_macro = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "    prec_weighted = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    rec_weighted = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_macro': prec_macro,\n",
    "        'recall_macro': rec_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_weighted': prec_weighted,\n",
    "        'recall_weighted': rec_weighted,\n",
    "    }\n",
    "\n",
    "def calculate_metrics_from_preds(y_true, y_pred):\n",
    "    \"\"\"Calculates evaluation metrics from direct predictions.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    # Print the classification report for detailed metrics\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 (Macro)\": f1_macro,\n",
    "        \"Precision (Macro)\": precision_macro,\n",
    "        \"Recall (Macro)\": recall_macro,\n",
    "        \"F1 (Weighted)\": f1_weighted,\n",
    "        \"Precision (Weighted)\": precision_weighted,\n",
    "        \"Recall (Weighted)\": recall_weighted,\n",
    "    }\n",
    "\n",
    "# Function to extract features (CLS token)\n",
    "def extract_hidden_states(batch, model, tokenizer, device):\n",
    "    # Ensure input_ids and attention_mask are tensors on the correct device\n",
    "    inputs = {k: v.to(device) for k, v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return the representation of the [CLS] token (first token)\n",
    "    # Move back to CPU to accumulate results if needed outside GPU loop\n",
    "    return last_hidden_state[:, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ba404",
   "metadata": {},
   "source": [
    "# 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8404b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982b5b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:24:54,497 - INFO - Processing Dataset: Financial News\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================= Processing Dataset: Financial News =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:24:55,963 - INFO - Loaded Train data: 3392 examples\n",
      "2025-05-03 09:24:55,964 - INFO - Loaded Validation data: 727 examples\n",
      "2025-05-03 09:24:55,964 - INFO - Loaded Test data: 727 examples\n",
      "2025-05-03 09:24:55,965 - INFO - Starting run for BERT LoRA FT on Financial News\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Model: BERT LoRA FT ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:24:56,368 - INFO - Tokenizing data using bert-base-uncased tokenizer...\n",
      "2025-05-03 09:24:56,384 - INFO - Tokenization complete.\n",
      "2025-05-03 09:24:56,384 - INFO - Running in Fine-tuning mode (LoRA: True).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-03 09:24:56,761 - INFO - Applying LoRA configuration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 109,781,766 || trainable%: 0.2707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "2025-05-03 09:24:57,423 - INFO - Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using evaluation_strategy: IntervalStrategy.EPOCH\n",
      "Using save_strategy: SaveStrategy.EPOCH\n",
      "Using load_best_model_at_end: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2052' max='8100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2052/8100 12:00 < 35:24, 2.85 it/s, Epoch 76/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.140900</td>\n",
       "      <td>1.080298</td>\n",
       "      <td>0.464924</td>\n",
       "      <td>0.255785</td>\n",
       "      <td>0.221725</td>\n",
       "      <td>0.304784</td>\n",
       "      <td>0.400419</td>\n",
       "      <td>0.352236</td>\n",
       "      <td>0.464924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.036800</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>0.591472</td>\n",
       "      <td>0.247982</td>\n",
       "      <td>0.197974</td>\n",
       "      <td>0.331790</td>\n",
       "      <td>0.442069</td>\n",
       "      <td>0.352922</td>\n",
       "      <td>0.591472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.970700</td>\n",
       "      <td>0.947288</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.939400</td>\n",
       "      <td>0.933011</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>0.926836</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.921469</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>0.914479</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.907200</td>\n",
       "      <td>0.905603</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>0.894867</td>\n",
       "      <td>0.596974</td>\n",
       "      <td>0.255361</td>\n",
       "      <td>0.531954</td>\n",
       "      <td>0.336585</td>\n",
       "      <td>0.449190</td>\n",
       "      <td>0.636056</td>\n",
       "      <td>0.596974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.878890</td>\n",
       "      <td>0.598349</td>\n",
       "      <td>0.259262</td>\n",
       "      <td>0.366389</td>\n",
       "      <td>0.338211</td>\n",
       "      <td>0.453299</td>\n",
       "      <td>0.497030</td>\n",
       "      <td>0.598349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>0.859090</td>\n",
       "      <td>0.634113</td>\n",
       "      <td>0.344558</td>\n",
       "      <td>0.402282</td>\n",
       "      <td>0.384760</td>\n",
       "      <td>0.534110</td>\n",
       "      <td>0.540199</td>\n",
       "      <td>0.634113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.843600</td>\n",
       "      <td>0.838185</td>\n",
       "      <td>0.640990</td>\n",
       "      <td>0.374999</td>\n",
       "      <td>0.385202</td>\n",
       "      <td>0.404852</td>\n",
       "      <td>0.563092</td>\n",
       "      <td>0.534864</td>\n",
       "      <td>0.640990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.817400</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>0.643741</td>\n",
       "      <td>0.399997</td>\n",
       "      <td>0.496525</td>\n",
       "      <td>0.417754</td>\n",
       "      <td>0.576282</td>\n",
       "      <td>0.580149</td>\n",
       "      <td>0.643741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.802867</td>\n",
       "      <td>0.647868</td>\n",
       "      <td>0.422614</td>\n",
       "      <td>0.511591</td>\n",
       "      <td>0.431429</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.590530</td>\n",
       "      <td>0.647868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>0.787351</td>\n",
       "      <td>0.651994</td>\n",
       "      <td>0.439716</td>\n",
       "      <td>0.515631</td>\n",
       "      <td>0.442540</td>\n",
       "      <td>0.598277</td>\n",
       "      <td>0.596581</td>\n",
       "      <td>0.651994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.766300</td>\n",
       "      <td>0.773118</td>\n",
       "      <td>0.657497</td>\n",
       "      <td>0.459972</td>\n",
       "      <td>0.536434</td>\n",
       "      <td>0.457471</td>\n",
       "      <td>0.611293</td>\n",
       "      <td>0.610057</td>\n",
       "      <td>0.657497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.751900</td>\n",
       "      <td>0.755931</td>\n",
       "      <td>0.664374</td>\n",
       "      <td>0.491054</td>\n",
       "      <td>0.557743</td>\n",
       "      <td>0.480631</td>\n",
       "      <td>0.625641</td>\n",
       "      <td>0.624270</td>\n",
       "      <td>0.664374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.739033</td>\n",
       "      <td>0.668501</td>\n",
       "      <td>0.509313</td>\n",
       "      <td>0.571216</td>\n",
       "      <td>0.495528</td>\n",
       "      <td>0.633755</td>\n",
       "      <td>0.632424</td>\n",
       "      <td>0.668501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.721972</td>\n",
       "      <td>0.671252</td>\n",
       "      <td>0.514887</td>\n",
       "      <td>0.581044</td>\n",
       "      <td>0.499635</td>\n",
       "      <td>0.637845</td>\n",
       "      <td>0.637646</td>\n",
       "      <td>0.671252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.705592</td>\n",
       "      <td>0.679505</td>\n",
       "      <td>0.551695</td>\n",
       "      <td>0.605576</td>\n",
       "      <td>0.532110</td>\n",
       "      <td>0.657634</td>\n",
       "      <td>0.655966</td>\n",
       "      <td>0.679505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.684205</td>\n",
       "      <td>0.693260</td>\n",
       "      <td>0.557853</td>\n",
       "      <td>0.617429</td>\n",
       "      <td>0.537748</td>\n",
       "      <td>0.668053</td>\n",
       "      <td>0.668094</td>\n",
       "      <td>0.693260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.664101</td>\n",
       "      <td>0.707015</td>\n",
       "      <td>0.600852</td>\n",
       "      <td>0.646917</td>\n",
       "      <td>0.579058</td>\n",
       "      <td>0.688760</td>\n",
       "      <td>0.688503</td>\n",
       "      <td>0.707015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.638900</td>\n",
       "      <td>0.643715</td>\n",
       "      <td>0.719395</td>\n",
       "      <td>0.635935</td>\n",
       "      <td>0.673722</td>\n",
       "      <td>0.613847</td>\n",
       "      <td>0.708106</td>\n",
       "      <td>0.707140</td>\n",
       "      <td>0.719395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.621927</td>\n",
       "      <td>0.727648</td>\n",
       "      <td>0.655545</td>\n",
       "      <td>0.677818</td>\n",
       "      <td>0.640711</td>\n",
       "      <td>0.718907</td>\n",
       "      <td>0.716813</td>\n",
       "      <td>0.727648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.604183</td>\n",
       "      <td>0.724897</td>\n",
       "      <td>0.660791</td>\n",
       "      <td>0.672260</td>\n",
       "      <td>0.652236</td>\n",
       "      <td>0.719535</td>\n",
       "      <td>0.717064</td>\n",
       "      <td>0.724897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.583264</td>\n",
       "      <td>0.738652</td>\n",
       "      <td>0.680208</td>\n",
       "      <td>0.688218</td>\n",
       "      <td>0.675467</td>\n",
       "      <td>0.733400</td>\n",
       "      <td>0.731533</td>\n",
       "      <td>0.738652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.568898</td>\n",
       "      <td>0.744154</td>\n",
       "      <td>0.699792</td>\n",
       "      <td>0.698809</td>\n",
       "      <td>0.701758</td>\n",
       "      <td>0.742467</td>\n",
       "      <td>0.741525</td>\n",
       "      <td>0.744154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.550943</td>\n",
       "      <td>0.757909</td>\n",
       "      <td>0.714494</td>\n",
       "      <td>0.714542</td>\n",
       "      <td>0.717416</td>\n",
       "      <td>0.754885</td>\n",
       "      <td>0.754194</td>\n",
       "      <td>0.757909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>0.537107</td>\n",
       "      <td>0.766162</td>\n",
       "      <td>0.724107</td>\n",
       "      <td>0.721483</td>\n",
       "      <td>0.732550</td>\n",
       "      <td>0.763377</td>\n",
       "      <td>0.764375</td>\n",
       "      <td>0.766162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.508900</td>\n",
       "      <td>0.522367</td>\n",
       "      <td>0.774415</td>\n",
       "      <td>0.735326</td>\n",
       "      <td>0.731269</td>\n",
       "      <td>0.743161</td>\n",
       "      <td>0.772803</td>\n",
       "      <td>0.773439</td>\n",
       "      <td>0.774415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.510794</td>\n",
       "      <td>0.777166</td>\n",
       "      <td>0.737193</td>\n",
       "      <td>0.731085</td>\n",
       "      <td>0.747267</td>\n",
       "      <td>0.776315</td>\n",
       "      <td>0.777524</td>\n",
       "      <td>0.777166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.503491</td>\n",
       "      <td>0.784044</td>\n",
       "      <td>0.749585</td>\n",
       "      <td>0.738044</td>\n",
       "      <td>0.765903</td>\n",
       "      <td>0.784593</td>\n",
       "      <td>0.787153</td>\n",
       "      <td>0.784044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.498431</td>\n",
       "      <td>0.788171</td>\n",
       "      <td>0.756618</td>\n",
       "      <td>0.742075</td>\n",
       "      <td>0.776276</td>\n",
       "      <td>0.789686</td>\n",
       "      <td>0.793466</td>\n",
       "      <td>0.788171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.464800</td>\n",
       "      <td>0.490326</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.759380</td>\n",
       "      <td>0.746884</td>\n",
       "      <td>0.774970</td>\n",
       "      <td>0.790840</td>\n",
       "      <td>0.793566</td>\n",
       "      <td>0.789546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.486704</td>\n",
       "      <td>0.788171</td>\n",
       "      <td>0.758491</td>\n",
       "      <td>0.744179</td>\n",
       "      <td>0.777130</td>\n",
       "      <td>0.789693</td>\n",
       "      <td>0.793224</td>\n",
       "      <td>0.788171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.476638</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.768108</td>\n",
       "      <td>0.760836</td>\n",
       "      <td>0.779803</td>\n",
       "      <td>0.800062</td>\n",
       "      <td>0.801763</td>\n",
       "      <td>0.800550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.485059</td>\n",
       "      <td>0.788171</td>\n",
       "      <td>0.761733</td>\n",
       "      <td>0.741723</td>\n",
       "      <td>0.790684</td>\n",
       "      <td>0.791403</td>\n",
       "      <td>0.800282</td>\n",
       "      <td>0.788171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.477587</td>\n",
       "      <td>0.793673</td>\n",
       "      <td>0.767779</td>\n",
       "      <td>0.749015</td>\n",
       "      <td>0.793771</td>\n",
       "      <td>0.796560</td>\n",
       "      <td>0.804047</td>\n",
       "      <td>0.793673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>0.466079</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.770476</td>\n",
       "      <td>0.759005</td>\n",
       "      <td>0.786924</td>\n",
       "      <td>0.802423</td>\n",
       "      <td>0.805030</td>\n",
       "      <td>0.801926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.466342</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.770549</td>\n",
       "      <td>0.755090</td>\n",
       "      <td>0.791279</td>\n",
       "      <td>0.802308</td>\n",
       "      <td>0.806523</td>\n",
       "      <td>0.800550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>0.461129</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.774010</td>\n",
       "      <td>0.758790</td>\n",
       "      <td>0.793759</td>\n",
       "      <td>0.803644</td>\n",
       "      <td>0.807612</td>\n",
       "      <td>0.801926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.429100</td>\n",
       "      <td>0.468224</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.782110</td>\n",
       "      <td>0.765602</td>\n",
       "      <td>0.805721</td>\n",
       "      <td>0.804898</td>\n",
       "      <td>0.814534</td>\n",
       "      <td>0.801926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.459159</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>0.778851</td>\n",
       "      <td>0.761093</td>\n",
       "      <td>0.802876</td>\n",
       "      <td>0.806743</td>\n",
       "      <td>0.812103</td>\n",
       "      <td>0.804677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.457360</td>\n",
       "      <td>0.803301</td>\n",
       "      <td>0.781386</td>\n",
       "      <td>0.765044</td>\n",
       "      <td>0.803075</td>\n",
       "      <td>0.805817</td>\n",
       "      <td>0.812730</td>\n",
       "      <td>0.803301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.403200</td>\n",
       "      <td>0.453472</td>\n",
       "      <td>0.806052</td>\n",
       "      <td>0.783202</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>0.803764</td>\n",
       "      <td>0.808340</td>\n",
       "      <td>0.814212</td>\n",
       "      <td>0.806052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.453594</td>\n",
       "      <td>0.810179</td>\n",
       "      <td>0.786170</td>\n",
       "      <td>0.769051</td>\n",
       "      <td>0.808525</td>\n",
       "      <td>0.812212</td>\n",
       "      <td>0.817324</td>\n",
       "      <td>0.810179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.454893</td>\n",
       "      <td>0.806052</td>\n",
       "      <td>0.783850</td>\n",
       "      <td>0.764250</td>\n",
       "      <td>0.810851</td>\n",
       "      <td>0.808517</td>\n",
       "      <td>0.815665</td>\n",
       "      <td>0.806052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.393300</td>\n",
       "      <td>0.452510</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.777068</td>\n",
       "      <td>0.759181</td>\n",
       "      <td>0.801046</td>\n",
       "      <td>0.803088</td>\n",
       "      <td>0.809809</td>\n",
       "      <td>0.800550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.450613</td>\n",
       "      <td>0.806052</td>\n",
       "      <td>0.784633</td>\n",
       "      <td>0.767838</td>\n",
       "      <td>0.806696</td>\n",
       "      <td>0.808424</td>\n",
       "      <td>0.814825</td>\n",
       "      <td>0.806052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.444770</td>\n",
       "      <td>0.811554</td>\n",
       "      <td>0.788271</td>\n",
       "      <td>0.772594</td>\n",
       "      <td>0.808074</td>\n",
       "      <td>0.813450</td>\n",
       "      <td>0.818005</td>\n",
       "      <td>0.811554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.442808</td>\n",
       "      <td>0.811554</td>\n",
       "      <td>0.788544</td>\n",
       "      <td>0.772536</td>\n",
       "      <td>0.808928</td>\n",
       "      <td>0.813571</td>\n",
       "      <td>0.818551</td>\n",
       "      <td>0.811554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.447336</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>0.782878</td>\n",
       "      <td>0.765526</td>\n",
       "      <td>0.805924</td>\n",
       "      <td>0.807116</td>\n",
       "      <td>0.813831</td>\n",
       "      <td>0.804677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.449948</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.780176</td>\n",
       "      <td>0.759943</td>\n",
       "      <td>0.808619</td>\n",
       "      <td>0.803222</td>\n",
       "      <td>0.811576</td>\n",
       "      <td>0.800550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.433921</td>\n",
       "      <td>0.812930</td>\n",
       "      <td>0.788604</td>\n",
       "      <td>0.774507</td>\n",
       "      <td>0.805913</td>\n",
       "      <td>0.814661</td>\n",
       "      <td>0.818494</td>\n",
       "      <td>0.812930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.434155</td>\n",
       "      <td>0.810179</td>\n",
       "      <td>0.786071</td>\n",
       "      <td>0.770857</td>\n",
       "      <td>0.805224</td>\n",
       "      <td>0.812172</td>\n",
       "      <td>0.816894</td>\n",
       "      <td>0.810179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.439939</td>\n",
       "      <td>0.803301</td>\n",
       "      <td>0.780409</td>\n",
       "      <td>0.763822</td>\n",
       "      <td>0.802221</td>\n",
       "      <td>0.805719</td>\n",
       "      <td>0.812145</td>\n",
       "      <td>0.803301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.812930</td>\n",
       "      <td>0.790705</td>\n",
       "      <td>0.772582</td>\n",
       "      <td>0.814709</td>\n",
       "      <td>0.814953</td>\n",
       "      <td>0.820469</td>\n",
       "      <td>0.812930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.440373</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>0.786483</td>\n",
       "      <td>0.768228</td>\n",
       "      <td>0.811420</td>\n",
       "      <td>0.807120</td>\n",
       "      <td>0.814968</td>\n",
       "      <td>0.804677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.436260</td>\n",
       "      <td>0.806052</td>\n",
       "      <td>0.784768</td>\n",
       "      <td>0.765577</td>\n",
       "      <td>0.810851</td>\n",
       "      <td>0.808376</td>\n",
       "      <td>0.815103</td>\n",
       "      <td>0.806052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.436926</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>0.783314</td>\n",
       "      <td>0.763405</td>\n",
       "      <td>0.810934</td>\n",
       "      <td>0.807192</td>\n",
       "      <td>0.814774</td>\n",
       "      <td>0.804677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.432656</td>\n",
       "      <td>0.810179</td>\n",
       "      <td>0.791042</td>\n",
       "      <td>0.772968</td>\n",
       "      <td>0.814875</td>\n",
       "      <td>0.812286</td>\n",
       "      <td>0.818380</td>\n",
       "      <td>0.810179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.440132</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.780264</td>\n",
       "      <td>0.759329</td>\n",
       "      <td>0.810328</td>\n",
       "      <td>0.803327</td>\n",
       "      <td>0.812558</td>\n",
       "      <td>0.800550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>0.811554</td>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.773369</td>\n",
       "      <td>0.811860</td>\n",
       "      <td>0.813538</td>\n",
       "      <td>0.818726</td>\n",
       "      <td>0.811554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.437160</td>\n",
       "      <td>0.797799</td>\n",
       "      <td>0.777910</td>\n",
       "      <td>0.757006</td>\n",
       "      <td>0.807931</td>\n",
       "      <td>0.800574</td>\n",
       "      <td>0.809785</td>\n",
       "      <td>0.797799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.431649</td>\n",
       "      <td>0.807428</td>\n",
       "      <td>0.788989</td>\n",
       "      <td>0.769840</td>\n",
       "      <td>0.815041</td>\n",
       "      <td>0.809743</td>\n",
       "      <td>0.817120</td>\n",
       "      <td>0.807428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.427529</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>0.793492</td>\n",
       "      <td>0.775455</td>\n",
       "      <td>0.817190</td>\n",
       "      <td>0.816271</td>\n",
       "      <td>0.821790</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.442429</td>\n",
       "      <td>0.797799</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.758859</td>\n",
       "      <td>0.811348</td>\n",
       "      <td>0.800654</td>\n",
       "      <td>0.811466</td>\n",
       "      <td>0.797799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.427172</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>0.790691</td>\n",
       "      <td>0.772340</td>\n",
       "      <td>0.815481</td>\n",
       "      <td>0.816252</td>\n",
       "      <td>0.821650</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.439811</td>\n",
       "      <td>0.799175</td>\n",
       "      <td>0.781046</td>\n",
       "      <td>0.759128</td>\n",
       "      <td>0.813343</td>\n",
       "      <td>0.802005</td>\n",
       "      <td>0.812390</td>\n",
       "      <td>0.799175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.328600</td>\n",
       "      <td>0.422144</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>0.793229</td>\n",
       "      <td>0.775534</td>\n",
       "      <td>0.816335</td>\n",
       "      <td>0.816158</td>\n",
       "      <td>0.821255</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.785698</td>\n",
       "      <td>0.764829</td>\n",
       "      <td>0.815741</td>\n",
       "      <td>0.804585</td>\n",
       "      <td>0.814363</td>\n",
       "      <td>0.801926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.331600</td>\n",
       "      <td>0.429333</td>\n",
       "      <td>0.801926</td>\n",
       "      <td>0.783834</td>\n",
       "      <td>0.764130</td>\n",
       "      <td>0.811100</td>\n",
       "      <td>0.804355</td>\n",
       "      <td>0.812330</td>\n",
       "      <td>0.801926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.332900</td>\n",
       "      <td>0.421377</td>\n",
       "      <td>0.815681</td>\n",
       "      <td>0.792938</td>\n",
       "      <td>0.774810</td>\n",
       "      <td>0.817107</td>\n",
       "      <td>0.817554</td>\n",
       "      <td>0.822755</td>\n",
       "      <td>0.815681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.429874</td>\n",
       "      <td>0.803301</td>\n",
       "      <td>0.784354</td>\n",
       "      <td>0.763412</td>\n",
       "      <td>0.813949</td>\n",
       "      <td>0.805783</td>\n",
       "      <td>0.814065</td>\n",
       "      <td>0.803301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.429006</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>0.785405</td>\n",
       "      <td>0.764573</td>\n",
       "      <td>0.814721</td>\n",
       "      <td>0.807112</td>\n",
       "      <td>0.815137</td>\n",
       "      <td>0.804677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.426860</td>\n",
       "      <td>0.811554</td>\n",
       "      <td>0.791353</td>\n",
       "      <td>0.771467</td>\n",
       "      <td>0.818579</td>\n",
       "      <td>0.813724</td>\n",
       "      <td>0.820434</td>\n",
       "      <td>0.811554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:36:58,271 - INFO - Fine-tuning completed in 720.85s\n",
      "2025-05-03 09:36:58,272 - INFO - Evaluating model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:36:59,800 - INFO - Fine-tuning - Test Set Performance:\n",
      "2025-05-03 09:36:59,801 - INFO - {'eval_loss': 0.42158979177474976, 'eval_accuracy': 0.8376891334250344, 'eval_f1_macro': 0.8225959271751351, 'eval_precision_macro': 0.8025425425425426, 'eval_recall_macro': 0.8492951311578762, 'eval_f1_weighted': 0.8391884172319162, 'eval_precision_weighted': 0.8445922401080587, 'eval_recall_weighted': 0.8376891334250344, 'eval_runtime': 0.7595, 'eval_samples_per_second': 957.225, 'eval_steps_per_second': 30.284, 'epoch': 76.0}\n",
      "2025-05-03 09:36:59,802 - INFO - Fine-tuning - Test Set Confusion Matrix:\n",
      "2025-05-03 09:36:59,811 - INFO - Confusion matrix saved to ..\\result\\financial_news\\Financial_News_BERT_LoRA_FT_confusion_matrix.csv\n",
      "2025-05-03 09:36:59,812 - INFO - Saving LoRA adapter model to ..\\models\\llm\\financial_news\\BERT_LoRA_FT\\final_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          negative  neutral  positive\n",
      "negative        82        6         3\n",
      "neutral         22      362        48\n",
      "positive         7       32       165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:37:00,628 - INFO - Tokenizer saved to ..\\models\\llm\\financial_news\\BERT_LoRA_FT\\final_model\n",
      "2025-05-03 09:37:00,771 - INFO - Starting run for RoBERTa LoRA FT on Financial News\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Model: RoBERTa LoRA FT ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:37:01,131 - INFO - Tokenizing data using roberta-base tokenizer...\n",
      "2025-05-03 09:37:01,188 - INFO - Tokenization complete.\n",
      "2025-05-03 09:37:01,188 - INFO - Running in Fine-tuning mode (LoRA: True).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-03 09:37:01,516 - INFO - Applying LoRA configuration...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,811 || all params: 125,535,750 || trainable%: 0.7072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "2025-05-03 09:37:01,833 - INFO - Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using evaluation_strategy: IntervalStrategy.EPOCH\n",
      "Using save_strategy: SaveStrategy.EPOCH\n",
      "Using load_best_model_at_end: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='8100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/8100 08:12 < 40:09, 2.79 it/s, Epoch 51/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.014300</td>\n",
       "      <td>0.972216</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.959200</td>\n",
       "      <td>0.934804</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>0.923067</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.936200</td>\n",
       "      <td>0.915565</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.925900</td>\n",
       "      <td>0.905991</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.594223</td>\n",
       "      <td>0.248490</td>\n",
       "      <td>0.198074</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.442975</td>\n",
       "      <td>0.353101</td>\n",
       "      <td>0.594223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.830294</td>\n",
       "      <td>0.640990</td>\n",
       "      <td>0.347677</td>\n",
       "      <td>0.407659</td>\n",
       "      <td>0.388618</td>\n",
       "      <td>0.539366</td>\n",
       "      <td>0.546785</td>\n",
       "      <td>0.640990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.790800</td>\n",
       "      <td>0.730383</td>\n",
       "      <td>0.661623</td>\n",
       "      <td>0.423181</td>\n",
       "      <td>0.403319</td>\n",
       "      <td>0.449748</td>\n",
       "      <td>0.608941</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>0.661623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.628773</td>\n",
       "      <td>0.716644</td>\n",
       "      <td>0.635998</td>\n",
       "      <td>0.711980</td>\n",
       "      <td>0.605236</td>\n",
       "      <td>0.694377</td>\n",
       "      <td>0.709727</td>\n",
       "      <td>0.716644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.581800</td>\n",
       "      <td>0.534381</td>\n",
       "      <td>0.746905</td>\n",
       "      <td>0.709729</td>\n",
       "      <td>0.716470</td>\n",
       "      <td>0.713942</td>\n",
       "      <td>0.739543</td>\n",
       "      <td>0.741494</td>\n",
       "      <td>0.746905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.773040</td>\n",
       "      <td>0.749266</td>\n",
       "      <td>0.734580</td>\n",
       "      <td>0.769749</td>\n",
       "      <td>0.773360</td>\n",
       "      <td>0.776099</td>\n",
       "      <td>0.773040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.479285</td>\n",
       "      <td>0.790922</td>\n",
       "      <td>0.762492</td>\n",
       "      <td>0.748705</td>\n",
       "      <td>0.787353</td>\n",
       "      <td>0.791101</td>\n",
       "      <td>0.796118</td>\n",
       "      <td>0.790922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.458491</td>\n",
       "      <td>0.793673</td>\n",
       "      <td>0.759876</td>\n",
       "      <td>0.753950</td>\n",
       "      <td>0.774605</td>\n",
       "      <td>0.792740</td>\n",
       "      <td>0.796554</td>\n",
       "      <td>0.793673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.446885</td>\n",
       "      <td>0.804677</td>\n",
       "      <td>0.773294</td>\n",
       "      <td>0.764995</td>\n",
       "      <td>0.790914</td>\n",
       "      <td>0.804354</td>\n",
       "      <td>0.808659</td>\n",
       "      <td>0.804677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.438704</td>\n",
       "      <td>0.796424</td>\n",
       "      <td>0.774875</td>\n",
       "      <td>0.757939</td>\n",
       "      <td>0.797760</td>\n",
       "      <td>0.797686</td>\n",
       "      <td>0.801699</td>\n",
       "      <td>0.796424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.426875</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>0.790252</td>\n",
       "      <td>0.780466</td>\n",
       "      <td>0.804005</td>\n",
       "      <td>0.814359</td>\n",
       "      <td>0.816150</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.439951</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.773205</td>\n",
       "      <td>0.751236</td>\n",
       "      <td>0.804893</td>\n",
       "      <td>0.791701</td>\n",
       "      <td>0.799727</td>\n",
       "      <td>0.789546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>0.421418</td>\n",
       "      <td>0.806052</td>\n",
       "      <td>0.786284</td>\n",
       "      <td>0.770581</td>\n",
       "      <td>0.806579</td>\n",
       "      <td>0.807124</td>\n",
       "      <td>0.810353</td>\n",
       "      <td>0.806052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.420230</td>\n",
       "      <td>0.807428</td>\n",
       "      <td>0.789463</td>\n",
       "      <td>0.770148</td>\n",
       "      <td>0.815778</td>\n",
       "      <td>0.808933</td>\n",
       "      <td>0.814064</td>\n",
       "      <td>0.807428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.357200</td>\n",
       "      <td>0.404887</td>\n",
       "      <td>0.817056</td>\n",
       "      <td>0.796980</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.818248</td>\n",
       "      <td>0.818153</td>\n",
       "      <td>0.821584</td>\n",
       "      <td>0.817056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.347600</td>\n",
       "      <td>0.412301</td>\n",
       "      <td>0.814305</td>\n",
       "      <td>0.801206</td>\n",
       "      <td>0.781313</td>\n",
       "      <td>0.827695</td>\n",
       "      <td>0.815835</td>\n",
       "      <td>0.821575</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.394243</td>\n",
       "      <td>0.828061</td>\n",
       "      <td>0.812687</td>\n",
       "      <td>0.800690</td>\n",
       "      <td>0.826615</td>\n",
       "      <td>0.828891</td>\n",
       "      <td>0.830895</td>\n",
       "      <td>0.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.405235</td>\n",
       "      <td>0.817056</td>\n",
       "      <td>0.804474</td>\n",
       "      <td>0.785822</td>\n",
       "      <td>0.828869</td>\n",
       "      <td>0.818610</td>\n",
       "      <td>0.824332</td>\n",
       "      <td>0.817056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.406639</td>\n",
       "      <td>0.821183</td>\n",
       "      <td>0.808653</td>\n",
       "      <td>0.790576</td>\n",
       "      <td>0.832039</td>\n",
       "      <td>0.822688</td>\n",
       "      <td>0.828123</td>\n",
       "      <td>0.821183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.417212</td>\n",
       "      <td>0.815681</td>\n",
       "      <td>0.805621</td>\n",
       "      <td>0.785076</td>\n",
       "      <td>0.834447</td>\n",
       "      <td>0.817581</td>\n",
       "      <td>0.826329</td>\n",
       "      <td>0.815681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.398075</td>\n",
       "      <td>0.823934</td>\n",
       "      <td>0.811054</td>\n",
       "      <td>0.793370</td>\n",
       "      <td>0.833582</td>\n",
       "      <td>0.825264</td>\n",
       "      <td>0.829971</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.403792</td>\n",
       "      <td>0.819807</td>\n",
       "      <td>0.808846</td>\n",
       "      <td>0.790973</td>\n",
       "      <td>0.832121</td>\n",
       "      <td>0.821329</td>\n",
       "      <td>0.827149</td>\n",
       "      <td>0.819807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.403570</td>\n",
       "      <td>0.818432</td>\n",
       "      <td>0.807561</td>\n",
       "      <td>0.789840</td>\n",
       "      <td>0.830495</td>\n",
       "      <td>0.819923</td>\n",
       "      <td>0.825493</td>\n",
       "      <td>0.818432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.401612</td>\n",
       "      <td>0.823934</td>\n",
       "      <td>0.813166</td>\n",
       "      <td>0.793214</td>\n",
       "      <td>0.839446</td>\n",
       "      <td>0.825227</td>\n",
       "      <td>0.830689</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.394260</td>\n",
       "      <td>0.825309</td>\n",
       "      <td>0.814240</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.840218</td>\n",
       "      <td>0.826568</td>\n",
       "      <td>0.831818</td>\n",
       "      <td>0.825309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.385087</td>\n",
       "      <td>0.830812</td>\n",
       "      <td>0.819097</td>\n",
       "      <td>0.803137</td>\n",
       "      <td>0.838663</td>\n",
       "      <td>0.831714</td>\n",
       "      <td>0.834756</td>\n",
       "      <td>0.830812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.389542</td>\n",
       "      <td>0.836314</td>\n",
       "      <td>0.823196</td>\n",
       "      <td>0.808649</td>\n",
       "      <td>0.840895</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.839197</td>\n",
       "      <td>0.836314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.393291</td>\n",
       "      <td>0.825309</td>\n",
       "      <td>0.814692</td>\n",
       "      <td>0.796439</td>\n",
       "      <td>0.838140</td>\n",
       "      <td>0.826606</td>\n",
       "      <td>0.831664</td>\n",
       "      <td>0.825309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.392851</td>\n",
       "      <td>0.825309</td>\n",
       "      <td>0.813924</td>\n",
       "      <td>0.795194</td>\n",
       "      <td>0.838140</td>\n",
       "      <td>0.826611</td>\n",
       "      <td>0.831734</td>\n",
       "      <td>0.825309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.398360</td>\n",
       "      <td>0.822558</td>\n",
       "      <td>0.813449</td>\n",
       "      <td>0.794222</td>\n",
       "      <td>0.839160</td>\n",
       "      <td>0.824020</td>\n",
       "      <td>0.830755</td>\n",
       "      <td>0.822558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.391621</td>\n",
       "      <td>0.829436</td>\n",
       "      <td>0.819381</td>\n",
       "      <td>0.801539</td>\n",
       "      <td>0.842164</td>\n",
       "      <td>0.830665</td>\n",
       "      <td>0.835620</td>\n",
       "      <td>0.829436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.399251</td>\n",
       "      <td>0.828061</td>\n",
       "      <td>0.820571</td>\n",
       "      <td>0.801621</td>\n",
       "      <td>0.846033</td>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.836662</td>\n",
       "      <td>0.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.285500</td>\n",
       "      <td>0.408252</td>\n",
       "      <td>0.823934</td>\n",
       "      <td>0.815969</td>\n",
       "      <td>0.795490</td>\n",
       "      <td>0.844572</td>\n",
       "      <td>0.825602</td>\n",
       "      <td>0.834166</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>0.405288</td>\n",
       "      <td>0.828061</td>\n",
       "      <td>0.819113</td>\n",
       "      <td>0.798634</td>\n",
       "      <td>0.847742</td>\n",
       "      <td>0.829790</td>\n",
       "      <td>0.838448</td>\n",
       "      <td>0.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>0.399258</td>\n",
       "      <td>0.829436</td>\n",
       "      <td>0.821372</td>\n",
       "      <td>0.800806</td>\n",
       "      <td>0.848882</td>\n",
       "      <td>0.830765</td>\n",
       "      <td>0.837273</td>\n",
       "      <td>0.829436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.382676</td>\n",
       "      <td>0.841816</td>\n",
       "      <td>0.826755</td>\n",
       "      <td>0.813482</td>\n",
       "      <td>0.842389</td>\n",
       "      <td>0.842652</td>\n",
       "      <td>0.844943</td>\n",
       "      <td>0.841816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.409283</td>\n",
       "      <td>0.828061</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>0.799064</td>\n",
       "      <td>0.850674</td>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.839108</td>\n",
       "      <td>0.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.399139</td>\n",
       "      <td>0.834938</td>\n",
       "      <td>0.825439</td>\n",
       "      <td>0.805662</td>\n",
       "      <td>0.852454</td>\n",
       "      <td>0.836552</td>\n",
       "      <td>0.844293</td>\n",
       "      <td>0.834938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.276700</td>\n",
       "      <td>0.410114</td>\n",
       "      <td>0.826685</td>\n",
       "      <td>0.817495</td>\n",
       "      <td>0.798262</td>\n",
       "      <td>0.844892</td>\n",
       "      <td>0.828607</td>\n",
       "      <td>0.838018</td>\n",
       "      <td>0.826685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.406755</td>\n",
       "      <td>0.828061</td>\n",
       "      <td>0.817558</td>\n",
       "      <td>0.797938</td>\n",
       "      <td>0.844810</td>\n",
       "      <td>0.829873</td>\n",
       "      <td>0.838307</td>\n",
       "      <td>0.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.398170</td>\n",
       "      <td>0.830812</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.800239</td>\n",
       "      <td>0.844644</td>\n",
       "      <td>0.832426</td>\n",
       "      <td>0.839118</td>\n",
       "      <td>0.830812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.401373</td>\n",
       "      <td>0.829436</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.797470</td>\n",
       "      <td>0.844727</td>\n",
       "      <td>0.831260</td>\n",
       "      <td>0.839188</td>\n",
       "      <td>0.829436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.396842</td>\n",
       "      <td>0.833563</td>\n",
       "      <td>0.822726</td>\n",
       "      <td>0.802957</td>\n",
       "      <td>0.849119</td>\n",
       "      <td>0.835134</td>\n",
       "      <td>0.841844</td>\n",
       "      <td>0.833563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.391230</td>\n",
       "      <td>0.833563</td>\n",
       "      <td>0.823213</td>\n",
       "      <td>0.805247</td>\n",
       "      <td>0.847042</td>\n",
       "      <td>0.835178</td>\n",
       "      <td>0.841840</td>\n",
       "      <td>0.833563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.395512</td>\n",
       "      <td>0.836314</td>\n",
       "      <td>0.826657</td>\n",
       "      <td>0.807920</td>\n",
       "      <td>0.851517</td>\n",
       "      <td>0.837886</td>\n",
       "      <td>0.844544</td>\n",
       "      <td>0.836314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.404761</td>\n",
       "      <td>0.829436</td>\n",
       "      <td>0.822225</td>\n",
       "      <td>0.801360</td>\n",
       "      <td>0.852300</td>\n",
       "      <td>0.831350</td>\n",
       "      <td>0.841405</td>\n",
       "      <td>0.829436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:45:15,163 - INFO - Fine-tuning completed in 493.33s\n",
      "2025-05-03 09:45:15,163 - INFO - Evaluating model on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:45:16,720 - INFO - Fine-tuning - Test Set Performance:\n",
      "2025-05-03 09:45:16,720 - INFO - {'eval_loss': 0.36540836095809937, 'eval_accuracy': 0.8404401650618982, 'eval_f1_macro': 0.8337329377480232, 'eval_precision_macro': 0.8142779142779143, 'eval_recall_macro': 0.8589544079740158, 'eval_f1_weighted': 0.8409677564831212, 'eval_precision_weighted': 0.8444349063331181, 'eval_recall_weighted': 0.8404401650618982, 'eval_runtime': 0.7678, 'eval_samples_per_second': 946.902, 'eval_steps_per_second': 29.957, 'epoch': 51.0}\n",
      "2025-05-03 09:45:16,721 - INFO - Fine-tuning - Test Set Confusion Matrix:\n",
      "2025-05-03 09:45:16,724 - INFO - Confusion matrix saved to ..\\result\\financial_news\\Financial_News_RoBERTa_LoRA_FT_confusion_matrix.csv\n",
      "2025-05-03 09:45:16,725 - INFO - Saving LoRA adapter model to ..\\models\\llm\\financial_news\\RoBERTa_LoRA_FT\\final_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          negative  neutral  positive\n",
      "negative        86        4         1\n",
      "neutral         20      364        48\n",
      "positive         4       39       161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 09:45:17,288 - INFO - Tokenizer saved to ..\\models\\llm\\financial_news\\RoBERTa_LoRA_FT\\final_model\n"
     ]
    }
   ],
   "source": [
    "# --- Loop through each dataset defined in the configuration ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    print(f\"\\n{'='*25} Processing Dataset: {dataset_name} {'='*25}\")\n",
    "    logging.info(f\"Processing Dataset: {dataset_name}\")\n",
    "\n",
    "    # 1. Load Data using Hugging Face Datasets\n",
    "    raw_datasets = create_dataset_dict(config['train_path'], config['val_path'], config['test_path'])\n",
    "    if not raw_datasets:\n",
    "        logging.error(f\"Could not load data for {dataset_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Loop through each model configuration ---\n",
    "    for model_label, model_id, use_lora, is_feature_extractor in MODEL_CONFIGURATIONS:\n",
    "\n",
    "        # --- Skip FinBERT for non-financial data ---\n",
    "        if model_id == FINBERT_MODEL_ID and dataset_name != \"Financial News\":\n",
    "            logging.info(f\"Skipping {model_label} for {dataset_name} (Model is domain-specific).\")\n",
    "            continue\n",
    "\n",
    "        # --- Skip LoRA runs if flag is off ---\n",
    "        if use_lora and not USE_LORA:\n",
    "            logging.info(f\"Skipping LoRA run {model_label} as USE_LORA is False.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing Model: {model_label} ---\")\n",
    "        logging.info(f\"Starting run for {model_label} on {dataset_name}\")\n",
    "        run_results = {\"Dataset\": dataset_name, \"Model\": model_label}\n",
    "        train_time = 0.0\n",
    "        eval_time = 0.0\n",
    "\n",
    "        # Create specific output dirs for this run's checkpoints/models\n",
    "        run_model_dir = os.path.join(config['model_dir'], model_label.replace(' ', '_').replace('+', ''))\n",
    "        os.makedirs(run_model_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # 2. Load Tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "            # 3. Tokenize Datasets\n",
    "            logging.info(f\"Tokenizing data using {model_id} tokenizer...\")\n",
    "            # Apply tokenization in batches\n",
    "            tokenized_datasets = raw_datasets.map(\n",
    "                lambda batch: preprocess_function(batch, tokenizer),\n",
    "                batched=True,\n",
    "                remove_columns=[TEXT_COLUMN], # Remove original text column\n",
    "                desc=\"Running tokenizer on dataset\"\n",
    "            )\n",
    "            # Data collator handles dynamic padding\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            logging.info(\"Tokenization complete.\")\n",
    "\n",
    "            # ===========================================\n",
    "            # === 4.A Feature Extraction + Classifier ===\n",
    "            # ===========================================\n",
    "            if is_feature_extractor:\n",
    "                logging.info(\"Running in Feature Extraction mode.\")\n",
    "                # Load base model (no classification head)\n",
    "                model = AutoModel.from_pretrained(model_id).to(DEVICE)\n",
    "                model.eval() # Set to evaluation mode\n",
    "\n",
    "                # --- Extract Features ---\n",
    "                logging.info(\"Extracting features from datasets...\")\n",
    "                start_extract_time = time.time()\n",
    "\n",
    "                # Need dataloaders for batching feature extraction\n",
    "                tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "                train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=FEATURE_EXTRACTOR_BATCH_SIZE, collate_fn=data_collator)\n",
    "                val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=FEATURE_EXTRACTOR_BATCH_SIZE, collate_fn=data_collator)\n",
    "                test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=FEATURE_EXTRACTOR_BATCH_SIZE, collate_fn=data_collator)\n",
    "\n",
    "                X_train_features = []\n",
    "                y_train_labels = []\n",
    "                for batch in train_dataloader:\n",
    "                    y_train_labels.extend(batch['labels'].tolist())\n",
    "                    batch_features = extract_hidden_states(batch, model, tokenizer, DEVICE)\n",
    "                    X_train_features.append(batch_features)\n",
    "\n",
    "                X_val_features = []\n",
    "                y_val_labels = []\n",
    "                for batch in val_dataloader:\n",
    "                    y_val_labels.extend(batch['labels'].tolist())\n",
    "                    batch_features = extract_hidden_states(batch, model, tokenizer, DEVICE)\n",
    "                    X_val_features.append(batch_features)\n",
    "\n",
    "                X_test_features = []\n",
    "                y_test_labels = []\n",
    "                for batch in test_dataloader:\n",
    "                    y_test_labels.extend(batch['labels'].tolist())\n",
    "                    batch_features = extract_hidden_states(batch, model, tokenizer, DEVICE)\n",
    "                    X_test_features.append(batch_features)\n",
    "\n",
    "                X_train_features = np.concatenate(X_train_features)\n",
    "                X_val_features = np.concatenate(X_val_features)\n",
    "                X_test_features = np.concatenate(X_test_features)\n",
    "                end_extract_time = time.time()\n",
    "                logging.info(f\"Feature extraction took {end_extract_time - start_extract_time:.2f}s\")\n",
    "                logging.info(f\"Train features shape: {X_train_features.shape}\")\n",
    "\n",
    "                # --- Train Classifier ---\n",
    "                logging.info(\"Training Logistic Regression classifier...\")\n",
    "                classifier = LogisticRegression(max_iter=LOGREG_MAX_ITER, random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "                start_train_time = time.time()\n",
    "                # Combine train + val features for final classifier training? Or tune on val? Simpler: train on train, eval on test.\n",
    "                classifier.fit(X_train_features, y_train_labels)\n",
    "                end_train_time = time.time()\n",
    "                train_time = end_train_time - start_train_time\n",
    "                logging.info(f\"Classifier training took {train_time:.2f}s\")\n",
    "\n",
    "                # --- Evaluate Classifier ---\n",
    "                start_eval_time = time.time()\n",
    "                y_pred_test = classifier.predict(X_test_features)\n",
    "                end_eval_time = time.time()\n",
    "                eval_time = end_eval_time - start_eval_time\n",
    "\n",
    "                test_metrics = calculate_metrics_from_preds(y_test_labels, y_pred_test)\n",
    "                run_results.update(test_metrics)\n",
    "\n",
    "                logging.info(\"Feature Extractor + LR - Test Set Performance:\")\n",
    "                report_str = classification_report(y_test_labels, y_pred_test, target_names=LABEL_LIST, zero_division=0)\n",
    "                print(report_str)\n",
    "\n",
    "                cm = confusion_matrix(y_test_labels, y_pred_test, labels=list(range(NUM_CLASSES))) # Ensure labels are ordered\n",
    "                cm_df = pd.DataFrame(cm, index=LABEL_LIST, columns=LABEL_LIST)\n",
    "                print(\"Confusion Matrix (Test Set):\")\n",
    "                print(cm_df)\n",
    "\n",
    "                cm_filename = f\"{dataset_name.replace(' ', '_')}_{model_label.replace(' ', '_').replace('+','')}_confusion_matrix.csv\"\n",
    "                cm_save_path = os.path.join(config['result_dir'], cm_filename)\n",
    "                try:\n",
    "                    cm_df.to_csv(cm_save_path)\n",
    "                    logging.info(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "                except Exception as cm_e:\n",
    "                    logging.error(f\"Failed to save confusion matrix to {cm_save_path}: {cm_e}\")\n",
    "\n",
    "\n",
    "                # Save the classifier\n",
    "                clf_save_path = os.path.join(run_model_dir, f\"{dataset_name.replace(' ', '_')}_{model_label.replace(' ', '_')}_LR_classifier.joblib\")\n",
    "                joblib.dump(classifier, clf_save_path)\n",
    "                logging.info(f\"Logistic Regression classifier saved to {clf_save_path}\")\n",
    "\n",
    "                # Cleanup GPU memory used by the base model\n",
    "                del model\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            # ===========================================\n",
    "            # === 4.B Fine-tuning (Full or LoRA)     ====\n",
    "            # ===========================================\n",
    "            else:\n",
    "                logging.info(f\"Running in Fine-tuning mode (LoRA: {use_lora}).\")\n",
    "                # Load model with sequence classification head\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_id,\n",
    "                    num_labels=NUM_CLASSES,\n",
    "                    id2label=ID2LABEL,\n",
    "                    label2id=LABEL2ID\n",
    "                )\n",
    "\n",
    "                # --- Apply LoRA if specified ---\n",
    "                if use_lora:\n",
    "                    logging.info(\"Applying LoRA configuration...\")\n",
    "                    peft_config = LoraConfig(\n",
    "                        task_type=TaskType.SEQ_CLS, # Specify task type\n",
    "                        r=LORA_R,\n",
    "                        lora_alpha=LORA_ALPHA,\n",
    "                        lora_dropout=LORA_DROPOUT,\n",
    "                        target_modules=LORA_TARGET_MODULES,\n",
    "                        bias=\"none\" # Usually set bias to 'none' or 'all'\n",
    "                    )\n",
    "                    model = get_peft_model(model, peft_config)\n",
    "                    model.print_trainable_parameters() # Verify LoRA application\n",
    "\n",
    "                model.to(DEVICE) # Move model to GPU before Trainer\n",
    "\n",
    "                # --- Define Training Arguments ---\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=os.path.join(run_model_dir, \"checkpoints\"),\n",
    "                    logging_dir=os.path.join(run_model_dir, \"logs\"),\n",
    "                    report_to=\"none\", # Disable wandb/tensorboard reporting unless configured\n",
    "                    num_train_epochs=NUM_EPOCHS,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    weight_decay=WEIGHT_DECAY,\n",
    "                    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "                    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "\n",
    "                    # --- Strategies ---\n",
    "                    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "                    save_strategy=\"epoch\",        # Save a checkpoint at the end of each epoch\n",
    "                    logging_strategy=\"epoch\",     # Log metrics at the end of each epoch (consistent)\n",
    "\n",
    "                    # --- Explicitly disable step-based intervals when using epoch strategy ---\n",
    "                    eval_steps=None,              # Do not evaluate every N steps\n",
    "                    save_steps=None,              # Do not save every N steps\n",
    "                    logging_steps=None,           # Do not log every N steps (use logging_strategy=\"epoch\")\n",
    "                    # Note: If you WANT step-based logging while using epoch eval/save, you can set\n",
    "                    # logging_strategy=\"steps\" and provide a value for logging_steps, but keep\n",
    "                    # eval_steps=None and save_steps=None.\n",
    "\n",
    "                    # --- Best model loading ---\n",
    "                    load_best_model_at_end=True, # Load the best model based on metric_for_best_model\n",
    "                    metric_for_best_model=METRIC_FOR_BEST_MODEL, # e.g., \"f1_macro\"\n",
    "                    greater_is_better=True,      # F1 score is better when higher\n",
    "                    save_total_limit=2,          # Only keep the best and the latest checkpoint\n",
    "\n",
    "                    # --- Other settings ---\n",
    "                    fp16=FP16,                   # Enable mixed precision training if GPU supports it\n",
    "                    # logging_steps=50,          # Remove or comment out if using logging_strategy=\"epoch\"\n",
    "                    # dataloader_num_workers=2,  # Optional\n",
    "                    gradient_accumulation_steps=1,\n",
    "                    seed=RANDOM_STATE,\n",
    "                    remove_unused_columns=True, # Default is True, good practice\n",
    "                )\n",
    "\n",
    "                print(f\"Using evaluation_strategy: {training_args.eval_strategy}\") # Add this print statement\n",
    "                print(f\"Using save_strategy: {training_args.save_strategy}\")\n",
    "                print(f\"Using load_best_model_at_end: {training_args.load_best_model_at_end}\")\n",
    "\n",
    "                # --- Define Trainer ---\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_datasets[\"train\"],\n",
    "                    eval_dataset=tokenized_datasets[\"validation\"], # Use validation set for evaluation during training\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=data_collator,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.001)] # Stop if metric doesn't improve enough\n",
    "                )\n",
    "\n",
    "                # --- Train the Model ---\n",
    "                logging.info(\"Starting fine-tuning...\")\n",
    "                start_train_time = time.time()\n",
    "                train_result = trainer.train()\n",
    "                end_train_time = time.time()\n",
    "                train_time = end_train_time - start_train_time\n",
    "                logging.info(f\"Fine-tuning completed in {train_time:.2f}s\")\n",
    "\n",
    "                # --- Evaluate on Test Set ---\n",
    "                logging.info(\"Evaluating model on the test set...\")\n",
    "                start_eval_time = time.time()\n",
    "                # Evaluate first to get metrics like loss\n",
    "                test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "                # Then predict to get raw predictions for confusion matrix\n",
    "                predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "                end_eval_time = time.time()\n",
    "                eval_time = end_eval_time - start_eval_time\n",
    "\n",
    "                # Extract predictions and labels\n",
    "                y_pred_test = np.argmax(predictions.predictions, axis=1)\n",
    "                y_true_test = predictions.label_ids\n",
    "\n",
    "                # Map trainer metric names (e.g., 'eval_f1_macro') to our standard names\n",
    "                run_results[\"Accuracy\"] = test_results.get('eval_accuracy', np.nan)\n",
    "                run_results[\"F1 (Macro)\"] = test_results.get('eval_f1_macro', np.nan)\n",
    "                run_results[\"Precision (Macro)\"] = test_results.get('eval_precision_macro', np.nan)\n",
    "                run_results[\"Recall (Macro)\"] = test_results.get('eval_recall_macro', np.nan)\n",
    "                run_results[\"F1 (Weighted)\"] = test_results.get('eval_f1_weighted', np.nan)\n",
    "                run_results[\"Precision (Weighted)\"] = test_results.get('eval_precision_weighted', np.nan)\n",
    "                run_results[\"Recall (Weighted)\"] = test_results.get('eval_recall_weighted', np.nan)\n",
    "\n",
    "                logging.info(\"Fine-tuning - Test Set Performance:\")\n",
    "                logging.info(test_results) # Log the full results dict from trainer\n",
    "\n",
    "                # Generate, print, and save the confusion matrix using predictions\n",
    "                logging.info(\"Fine-tuning - Test Set Confusion Matrix:\")\n",
    "                cm = confusion_matrix(y_true_test, y_pred_test, labels=list(range(NUM_CLASSES))) # Ensure labels are ordered\n",
    "                cm_df = pd.DataFrame(cm, index=LABEL_LIST, columns=LABEL_LIST)\n",
    "                print(cm_df)\n",
    "\n",
    "                # Save the confusion matrix\n",
    "                cm_filename = f\"{dataset_name.replace(' ', '_')}_{model_label.replace(' ', '_').replace('+','')}_confusion_matrix.csv\"\n",
    "                cm_save_path = os.path.join(config['result_dir'], cm_filename)\n",
    "                try:\n",
    "                    cm_df.to_csv(cm_save_path, mode='w+')\n",
    "                    logging.info(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "                except Exception as cm_e:\n",
    "                    logging.error(f\"Failed to save confusion matrix to {cm_save_path}: {cm_e}\")\n",
    "\n",
    "                # --- Save the Final Model & Tokenizer ---\n",
    "                # Trainer already saved the best checkpoint based on validation set.\n",
    "                # For LoRA, the main model is saved by Trainer, adapters need separate save\n",
    "                final_model_save_path = os.path.join(run_model_dir, \"final_model\")\n",
    "                if use_lora:\n",
    "                    logging.info(f\"Saving LoRA adapter model to {final_model_save_path}\")\n",
    "                    model.save_pretrained(final_model_save_path) # Saves only the adapter\n",
    "                else:\n",
    "                    # If not LoRA, trainer saved the full best model, we can optionally save it again here\n",
    "                    # under a consistent name if needed, but load_best_model_at_end handles loading it.\n",
    "                    # Saving explicitly:\n",
    "                    # trainer.save_model(final_model_save_path)\n",
    "                    logging.info(f\"Best model loaded by Trainer. Checkpoint saved in {training_args.output_dir}\")\n",
    "\n",
    "\n",
    "                tokenizer.save_pretrained(final_model_save_path) # Save tokenizer with the model/adapter\n",
    "                logging.info(f\"Tokenizer saved to {final_model_save_path}\")\n",
    "\n",
    "            # --- Store Timings and Finalize Results ---\n",
    "            run_results[\"Train Time (s)\"] = round(train_time, 3)\n",
    "            run_results[\"Eval Time (s)\"] = round(eval_time, 3)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"!!! An error occurred while processing {model_label} for {dataset_name}: {e}\", exc_info=True)\n",
    "            # Record partial results if possible\n",
    "            run_results[\"Accuracy\"] = np.nan\n",
    "            run_results[\"F1 (Macro)\"] = np.nan\n",
    "            # Fill other metrics with NaN or error messages\n",
    "            for metric in METRICS_TO_CALCULATE:\n",
    "                if metric not in run_results:\n",
    "                    run_results[metric] = np.nan if metric not in [\"Train Time (s)\", \"Eval Time (s)\"] else 0.0\n",
    "        finally:\n",
    "            all_results.append(run_results)\n",
    "            # Clean up memory aggressively after each run\n",
    "            del tokenizer\n",
    "            if 'model' in locals(): del model\n",
    "            if 'trainer' in locals(): del trainer\n",
    "            if 'classifier' in locals(): del classifier\n",
    "            if 'tokenized_datasets' in locals(): del tokenized_datasets\n",
    "            # if 'raw_datasets' in locals(): del raw_datasets\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Combine results into a DataFrame ---\n",
    "results_df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f17493",
   "metadata": {},
   "source": [
    "# 4. Results Summary and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870708ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Overall LLM Results Summary =====\n",
      "          Dataset            Model  Accuracy  F1 (Macro)  Precision (Macro)  Recall (Macro)  F1 (Weighted)  Precision (Weighted)  Recall (Weighted)  Train Time (s)  Eval Time (s)\n",
      "0  Financial News     BERT LoRA FT    0.8377      0.8226             0.8025          0.8493         0.8392                0.8446             0.8377        720.8480         1.5270\n",
      "1  Financial News  RoBERTa LoRA FT    0.8404      0.8337             0.8143          0.8590         0.8410                0.8444             0.8404        493.3290         1.5560\n",
      "\n",
      "Results for Financial News saved to ..\\result\\financial_news\\Financial_News_llm_transformers_results.csv\n",
      "\n",
      "Combined results saved to ..\\result\\combined_llm_transformers_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n===== Overall LLM Results Summary =====\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1400) # Wider display\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Ensure all expected columns exist, fill with NaN if necessary\n",
    "for col in METRICS_TO_CALCULATE:\n",
    "    if col not in results_df.columns:\n",
    "        results_df[col] = np.nan\n",
    "\n",
    "# Reorder columns for clarity\n",
    "column_order = [\"Dataset\", \"Model\"] + METRICS_TO_CALCULATE\n",
    "# Filter out columns not present if something went wrong during creation\n",
    "column_order = [col for col in column_order if col in results_df.columns]\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# --- Save results to CSV for each dataset ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    dataset_results_df = results_df[results_df['Dataset'] == dataset_name]\n",
    "    if not dataset_results_df.empty:\n",
    "        results_filename = f\"{dataset_name.replace(' ', '_')}_llm_transformers_results.csv\"\n",
    "        results_save_path = os.path.join(config['result_dir'], results_filename)\n",
    "        try:\n",
    "            dataset_results_df.to_csv(results_save_path, index=False, mode='w+')\n",
    "            print(f\"\\nResults for {dataset_name} saved to {results_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving results for {dataset_name} to {results_save_path}: {e}\")\n",
    "\n",
    "# --- Save combined results ---\n",
    "combined_results_path = os.path.join(RESULT_DIR, \"combined_llm_transformers_results.csv\")\n",
    "try:\n",
    "    results_df.to_csv(combined_results_path, index=False, mode='w+')\n",
    "    print(f\"\\nCombined results saved to {combined_results_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving combined results to {combined_results_path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
