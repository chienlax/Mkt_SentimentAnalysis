{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3db4142",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab12629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "from time import time\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Evaluation\n",
    "import evaluate # Hugging Face evaluate\n",
    "from sklearn.metrics import classification_report as sk_classification_report # For feature extraction part\n",
    "from sklearn.linear_model import LogisticRegression # For feature extraction part\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb5187",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "DATA_DIR = \"../data/processed\"\n",
    "MODEL_SAVE_DIR = \"../models/llm\" # Specific dir for LLM outputs\n",
    "RESULTS_SAVE_DIR = \"../results\"\n",
    "RESULTS_CSV_FILE = os.path.join(RESULTS_SAVE_DIR, \"llm_results_summary.csv\")\n",
    "\n",
    "# --- Experiment Setup ---\n",
    "DOMAINS = [\"book_reviews\", \"financial_news\"] # Add domain folder names\n",
    "\n",
    "# --- Models & Methods Configuration ---\n",
    "# Define models and the methods to apply to them\n",
    "EXPERIMENTS = [\n",
    "    # Feature Extraction\n",
    "    {\"method\": \"feature_extraction\", \"model_id\": \"distilbert-base-uncased\", \"short_name\": \"DistilBERT_FE\"},\n",
    "    {\"method\": \"feature_extraction\", \"model_id\": \"bert-base-uncased\", \"short_name\": \"BERT_FE\"},\n",
    "    # Full Fine-tuning\n",
    "    {\"method\": \"finetune\", \"model_id\": \"distilbert-base-uncased\", \"short_name\": \"DistilBERT_FT\"},\n",
    "    {\"method\": \"finetune\", \"model_id\": \"bert-base-uncased\", \"short_name\": \"BERT_FT\"},\n",
    "    {\"method\": \"finetune\", \"model_id\": \"roberta-base\", \"short_name\": \"RoBERTa_FT\"},\n",
    "    {\"method\": \"finetune\", \"model_id\": \"microsoft/deberta-v3-base\", \"short_name\": \"DeBERTaV3_FT\"},\n",
    "    {\"method\": \"finetune\", \"model_id\": \"ProsusAI/finbert\", \"short_name\": \"FinBERT_FT\", \"domain_filter\": \"financial_news\"}, # Domain specific\n",
    "    # LoRA Fine-tuning\n",
    "    {\"method\": \"lora\", \"model_id\": \"bert-base-uncased\", \"short_name\": \"BERT_LoRA\"},\n",
    "    {\"method\": \"lora\", \"model_id\": \"roberta-base\", \"short_name\": \"RoBERTa_LoRA\"},\n",
    "    # Add more experiments as needed\n",
    "]\n",
    "\n",
    "# --- Training Arguments (Defaults - can be overridden per experiment if needed) ---\n",
    "DEFAULT_TRAINING_ARGS = {\n",
    "    \"output_dir\": os.path.join(MODEL_SAVE_DIR, \"training_output\"), # Base output dir for trainer\n",
    "    \"num_train_epochs\": 3, # Adjust as needed\n",
    "    \"per_device_train_batch_size\": 16, # Adjust based on GPU memory\n",
    "    \"per_device_eval_batch_size\": 32, # Adjust based on GPU memory\n",
    "    \"gradient_accumulation_steps\": 2, # Increase effective batch size (train_batch_size * grad_accum)\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_dir\": os.path.join(MODEL_SAVE_DIR, \"logs\"),\n",
    "    \"logging_steps\": 50,\n",
    "    \"evaluation_strategy\": \"epoch\", # Evaluate at the end of each epoch\n",
    "    \"save_strategy\": \"epoch\",       # Save checkpoint at the end of each epoch\n",
    "    \"load_best_model_at_end\": True, # Load the best model based on validation metric\n",
    "    \"metric_for_best_model\": \"f1\",  # Use F1 score to select best model\n",
    "    \"greater_is_better\": True,\n",
    "    \"report_to\": \"none\", # Disable wandb/tensorboard logging unless configured\n",
    "    \"fp16\": torch.cuda.is_available(), # Use mixed precision if GPU available\n",
    "    # \"bf16\": torch.cuda.is_bf16_supported(), # Use bfloat16 if supported (Ampere GPUs+)\n",
    "}\n",
    "\n",
    "# --- PEFT Configuration (LoRA) ---\n",
    "DEFAULT_LORA_CONFIG = LoraConfig(\n",
    "    task_type=TaskType.SEQUENCE_CLASSIFICATION,\n",
    "    inference_mode=False,\n",
    "    r=8, # Rank of the update matrices\n",
    "    lora_alpha=16, # Alpha scaling factor\n",
    "    lora_dropout=0.1,\n",
    "    # target_modules usually identified automatically for common models,\n",
    "    # but might need specifying for others: e.g., [\"query\", \"value\"] for BERT\n",
    ")\n",
    "\n",
    "# --- Feature Extraction Configuration ---\n",
    "ML_CLASSIFIER = LogisticRegression(random_state=42, max_iter=1000) # Classifier for extracted features\n",
    "\n",
    "# --- Tokenization ---\n",
    "MAX_LENGTH = 256 # Max sequence length for tokenizer\n",
    "\n",
    "# --- Reproducibility & Device ---\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n",
    "for domain in DOMAINS:\n",
    "    os.makedirs(os.path.join(MODEL_SAVE_DIR, domain), exist_ok=True)\n",
    "    os.makedirs(os.path.join(RESULTS_SAVE_DIR, domain), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a276354",
   "metadata": {},
   "source": [
    "# 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_hf(domain_name):\n",
    "    \"\"\"Loads train, validation, and test data into Hugging Face Datasets.\"\"\"\n",
    "    print(f\"\\nLoading data for domain: {domain_name}...\")\n",
    "    try:\n",
    "        train_path = os.path.join(DATA_DIR, domain_name, \"train.csv\")\n",
    "        val_path = os.path.join(DATA_DIR, domain_name, \"validation.csv\")\n",
    "        test_path = os.path.join(DATA_DIR, domain_name, \"test.csv\")\n",
    "\n",
    "        # Load using pandas first to easily handle potential NaNs\n",
    "        train_df = pd.read_csv(train_path).dropna(subset=['text', 'label'])\n",
    "        val_df = pd.read_csv(val_path).dropna(subset=['text', 'label'])\n",
    "        test_df = pd.read_csv(test_path).dropna(subset=['text', 'label'])\n",
    "\n",
    "        # Convert labels to integers if they aren't already\n",
    "        train_df['label'] = train_df['label'].astype(int)\n",
    "        val_df['label'] = val_df['label'].astype(int)\n",
    "        test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "        # Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "        train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "        val_dataset = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "        test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "\n",
    "        print(f\"Dataset loaded: {dataset_dict}\")\n",
    "        return dataset_dict\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data for {domain_name}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data loading for {domain_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Tokenizes the text data.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes metrics for Trainer evaluation.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    precision_macro = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "    recall_macro = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1_weighted = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    precision_weighted = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall_weighted = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1\": f1_weighted, # Trainer uses 'f1' by default for best model\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"recall_weighted\": recall_weighted,\n",
    "    }\n",
    "\n",
    "# --- Feature Extraction Function ---\n",
    "def extract_transformer_features(model, tokenizer, texts, batch_size=32):\n",
    "    \"\"\"Extracts [CLS] token embeddings using a pre-trained model.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    all_features = []\n",
    "    print(f\"Extracting features for {len(texts)} texts...\")\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LENGTH).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Extract [CLS] token embedding (first token's last hidden state)\n",
    "        # Or use outputs.pooler_output if available and preferred\n",
    "        cls_features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_features.append(cls_features)\n",
    "        if (i // batch_size + 1) % 50 == 0:\n",
    "            print(f\"  Processed batch {i // batch_size + 1}\")\n",
    "\n",
    "\n",
    "    return np.vstack(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a8585",
   "metadata": {},
   "source": [
    "# 4. Main Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff337d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in DOMAINS:\n",
    "    # Load data using Hugging Face datasets\n",
    "    raw_datasets = load_data_hf(domain)\n",
    "    if raw_datasets is None:\n",
    "        print(f\"Skipping domain {domain} due to data loading error.\")\n",
    "        continue\n",
    "\n",
    "    # Determine number of labels dynamically\n",
    "    label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "    num_labels = len(label_list)\n",
    "    print(f\"Domain: {domain} | Number of labels: {num_labels} | Labels: {label_list}\")\n",
    "\n",
    "    # Map labels to IDs (important if labels are not 0, 1, ..., n-1)\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    # Pre-map labels in the dataset if necessary (Trainer usually handles this if labels are integers)\n",
    "    # def map_labels(example):\n",
    "    #     example['label'] = label2id[example['label']]\n",
    "    #     return example\n",
    "    # raw_datasets = raw_datasets.map(map_labels)\n",
    "\n",
    "\n",
    "    for exp_config in EXPERIMENTS:\n",
    "        model_id = exp_config[\"model_id\"]\n",
    "        method = exp_config[\"method\"]\n",
    "        short_name = exp_config[\"short_name\"]\n",
    "\n",
    "        # Check if model is domain-specific\n",
    "        if exp_config.get(\"domain_filter\") and domain != exp_config[\"domain_filter\"]:\n",
    "            print(f\"Skipping {short_name} for domain {domain} as it's domain-specific.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*10} Domain: {domain} | Model: {model_id} | Method: {method} {'='*10}\")\n",
    "\n",
    "        # --- Load Tokenizer ---\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR loading tokenizer {model_id}: {e}. Skipping experiment.\")\n",
    "            continue\n",
    "\n",
    "        # --- Tokenize Datasets ---\n",
    "        try:\n",
    "            tokenized_datasets = raw_datasets.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # Dynamic padding\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR tokenizing data for {model_id}: {e}. Skipping experiment.\")\n",
    "            continue\n",
    "\n",
    "        # --- Setup Model Save Path ---\n",
    "        model_output_dir = os.path.join(MODEL_SAVE_DIR, domain, short_name)\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "        # --- Run Experiment based on Method ---\n",
    "        exp_train_time = None\n",
    "        exp_eval_time = None\n",
    "        exp_results = {}\n",
    "\n",
    "        try:\n",
    "            if method == \"feature_extraction\":\n",
    "                print(\"Method: Feature Extraction + ML Classifier\")\n",
    "                # Load base model (no classification head)\n",
    "                model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "                # Extract features\n",
    "                t0 = time()\n",
    "                X_train_features = extract_transformer_features(model, tokenizer, raw_datasets[\"train\"][\"text\"])\n",
    "                X_test_features = extract_transformer_features(model, tokenizer, raw_datasets[\"test\"][\"text\"])\n",
    "                feature_ext_time = time() - t0\n",
    "                print(f\"Feature extraction time: {feature_ext_time:.2f}s\")\n",
    "\n",
    "                y_train = raw_datasets[\"train\"][\"label\"]\n",
    "                y_test = raw_datasets[\"test\"][\"label\"]\n",
    "\n",
    "                # Train ML Classifier\n",
    "                print(f\"Training ML Classifier ({ML_CLASSIFIER.__class__.__name__})...\")\n",
    "                t0 = time()\n",
    "                ml_classifier = ML_CLASSIFIER # Get fresh instance\n",
    "                ml_classifier.fit(X_train_features, y_train)\n",
    "                exp_train_time = time() - t0\n",
    "                print(f\"ML Training time: {exp_train_time:.2f}s\")\n",
    "\n",
    "                # Evaluate ML Classifier\n",
    "                t0 = time()\n",
    "                y_pred = ml_classifier.predict(X_test_features)\n",
    "                exp_eval_time = time() - t0\n",
    "                print(f\"ML Evaluation time: {exp_eval_time:.2f}s\")\n",
    "\n",
    "                report_dict = sk_classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "                exp_results = {\n",
    "                    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                    \"f1_macro\": report_dict['macro avg']['f1-score'],\n",
    "                    \"precision_macro\": report_dict['macro avg']['precision'],\n",
    "                    \"recall_macro\": report_dict['macro avg']['recall'],\n",
    "                    \"f1_weighted\": report_dict['weighted avg']['f1-score'],\n",
    "                    \"precision_weighted\": report_dict['weighted avg']['precision'],\n",
    "                    \"recall_weighted\": report_dict['weighted avg']['recall'],\n",
    "                }\n",
    "                print(sk_classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "                # Save ML model\n",
    "                ml_model_save_path = os.path.join(model_output_dir, \"ml_classifier.joblib\")\n",
    "                joblib.dump(ml_classifier, ml_model_save_path)\n",
    "                print(f\"Saved ML classifier to {ml_model_save_path}\")\n",
    "                del model # Free up GPU memory from base transformer\n",
    "\n",
    "            elif method == \"finetune\" or method == \"lora\":\n",
    "                print(f\"Method: {method.upper()}\")\n",
    "                # Load model for sequence classification\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_id,\n",
    "                    num_labels=num_labels,\n",
    "                    id2label=id2label,\n",
    "                    label2id=label2id\n",
    "                )\n",
    "\n",
    "                # Apply LoRA if specified\n",
    "                if method == \"lora\":\n",
    "                    print(\"Applying LoRA configuration...\")\n",
    "                    lora_config = DEFAULT_LORA_CONFIG\n",
    "                    # Ensure task type matches\n",
    "                    lora_config.task_type = TaskType.SEQUENCE_CLASSIFICATION\n",
    "                    model = get_peft_model(model, lora_config)\n",
    "                    model.print_trainable_parameters()\n",
    "\n",
    "                # Define Training Arguments\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=os.path.join(model_output_dir, \"training_output\"), # Specific output for this run\n",
    "                    **DEFAULT_TRAINING_ARGS # Use defaults, can override here\n",
    "                    # Example override: learning_rate=5e-5\n",
    "                )\n",
    "\n",
    "                # Initialize Trainer\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_datasets[\"train\"],\n",
    "                    eval_dataset=tokenized_datasets[\"validation\"], # Use validation set for eval during training\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=data_collator,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)] # Add early stopping\n",
    "                )\n",
    "\n",
    "                # Train\n",
    "                print(\"Starting training...\")\n",
    "                t0 = time()\n",
    "                train_result = trainer.train()\n",
    "                exp_train_time = time() - t0\n",
    "                print(f\"Training finished in {exp_train_time:.2f}s\")\n",
    "                trainer.save_model(model_output_dir) # Saves best model/adapters\n",
    "                print(f\"Saved best model/adapters to {model_output_dir}\")\n",
    "\n",
    "                # Evaluate on Test Set\n",
    "                print(\"Evaluating on test set...\")\n",
    "                t0 = time()\n",
    "                eval_result = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "                exp_eval_time = time() - t0\n",
    "                print(f\"Test evaluation finished in {exp_eval_time:.2f}s\")\n",
    "                print(f\"Test Results: {eval_result}\")\n",
    "\n",
    "                # Store metrics (key names match compute_metrics output)\n",
    "                exp_results = {\n",
    "                    \"accuracy\": eval_result.get(\"eval_accuracy\", np.nan),\n",
    "                    \"f1_macro\": eval_result.get(\"eval_f1_macro\", np.nan),\n",
    "                    \"precision_macro\": eval_result.get(\"eval_precision_macro\", np.nan),\n",
    "                    \"recall_macro\": eval_result.get(\"eval_recall_macro\", np.nan),\n",
    "                    \"f1_weighted\": eval_result.get(\"eval_f1\", np.nan), # Note: key is 'f1' from compute_metrics\n",
    "                    \"precision_weighted\": eval_result.get(\"eval_precision_weighted\", np.nan),\n",
    "                    \"recall_weighted\": eval_result.get(\"eval_recall_weighted\", np.nan),\n",
    "                }\n",
    "                del trainer\n",
    "                del model\n",
    "\n",
    "            else:\n",
    "                print(f\"Unknown method: {method}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Append results for this experiment\n",
    "            result_row = {\n",
    "                \"Domain\": domain,\n",
    "                \"Model ID\": model_id,\n",
    "                \"Method\": method,\n",
    "                \"Short Name\": short_name,\n",
    "                \"Accuracy\": exp_results.get(\"accuracy\"),\n",
    "                \"F1 (Macro)\": exp_results.get(\"f1_macro\"),\n",
    "                \"Precision (Macro)\": exp_results.get(\"precision_macro\"),\n",
    "                \"Recall (Macro)\": exp_results.get(\"recall_macro\"),\n",
    "                \"F1 (Weighted)\": exp_results.get(\"f1_weighted\"),\n",
    "                \"Precision (Weighted)\": exp_results.get(\"precision_weighted\"),\n",
    "                \"Recall (Weighted)\": exp_results.get(\"recall_weighted\"),\n",
    "                \"Train Time (s)\": exp_train_time,\n",
    "                \"Eval Time (s)\": exp_eval_time\n",
    "            }\n",
    "            all_results_list.append(result_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR during experiment {short_name} on domain {domain}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Print detailed error traceback\n",
    "\n",
    "        finally:\n",
    "            # Clean up GPU memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d74f7",
   "metadata": {},
   "source": [
    "# 5. Aggregate and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bfaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Experiment Finished ---\")\n",
    "if all_results_list:\n",
    "    results_df = pd.DataFrame(all_results_list)\n",
    "    # Define desired column order\n",
    "    cols_order = [\"Domain\", \"Model ID\", \"Method\", \"Short Name\", \"Accuracy\",\n",
    "                  \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "                  \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "                  \"Train Time (s)\", \"Eval Time (s)\"]\n",
    "    # Ensure all columns exist\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = np.nan\n",
    "    results_df = results_df[cols_order] # Reorder\n",
    "\n",
    "    print(\"\\nAggregated Results:\")\n",
    "    print(results_df.to_string()) # Print full dataframe\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df.to_csv(RESULTS_CSV_FILE, index=False)\n",
    "    print(f\"\\nResults saved to {RESULTS_CSV_FILE}\")\n",
    "else:\n",
    "    print(\"No results were generated.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
