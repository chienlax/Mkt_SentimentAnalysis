{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff2def3",
   "metadata": {},
   "source": [
    "# 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b155b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel, # For feature extraction\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType # For LoRA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import evaluate \n",
    "import time\n",
    "import os\n",
    "import joblib \n",
    "import logging\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# --- Basic Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- Limit CPU Usage ---\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78372810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Directory Structure ---\n",
    "BASE_DIR = \"..\" # Assuming the notebook is in a 'notebooks' or similar folder\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "# Models and results saved within dataset-specific folders\n",
    "MODEL_OUTPUT_BASE_DIR = os.path.join(BASE_DIR, \"models\", \"llm\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"result\")\n",
    "\n",
    "# --- Specific Dataset Paths ---\n",
    "BOOK_REVIEW_DATA_DIR = os.path.join(DATA_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_DATA_DIR = os.path.join(DATA_DIR, \"financial_news\")\n",
    "\n",
    "# --- Model/Result Output Dirs (Ensure they exist) ---\n",
    "BOOK_REVIEW_MODEL_DIR = os.path.join(MODEL_OUTPUT_BASE_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_MODEL_DIR = os.path.join(MODEL_OUTPUT_BASE_DIR, \"financial_news\")\n",
    "BOOK_REVIEW_RESULT_DIR = os.path.join(RESULT_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_RESULT_DIR = os.path.join(RESULT_DIR, \"financial_news\")\n",
    "\n",
    "os.makedirs(BOOK_REVIEW_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(BOOK_REVIEW_RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Names ---\n",
    "TRAIN_FN = \"train.csv\"\n",
    "VAL_FN = \"val.csv\"\n",
    "TEST_FN = \"test.csv\"\n",
    "\n",
    "# --- Column Names ---\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"score\" # Assumes string labels like 'positive', 'negative', 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model & Training Hyperparameters ---\n",
    "RANDOM_STATE = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Tokenizer params\n",
    "MAX_LENGTH = 256 # Max sequence length for transformers\n",
    "\n",
    "# Feature Extractor Params\n",
    "FEATURE_EXTRACTOR_BATCH_SIZE = 32 # Batch size for extracting features\n",
    "LOGREG_MAX_ITER = 1000 # Max iterations for Logistic Regression on features\n",
    "\n",
    "# Fine-tuning params (adjust based on resources and dataset size)\n",
    "LEARNING_RATE = 2e-5 # Common starting point for transformers\n",
    "WEIGHT_DECAY = 0.01\n",
    "TRAIN_BATCH_SIZE = 16 # Adjust based on GPU memory\n",
    "EVAL_BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 3 # Usually fewer epochs needed for fine-tuning\n",
    "FP16 = torch.cuda.is_available() # Enable mixed precision if GPU available\n",
    "\n",
    "# PEFT (LoRA) params\n",
    "USE_LORA = True # Flag to control if LoRA runs are included\n",
    "LORA_R = 8 # LoRA rank (dimension)\n",
    "LORA_ALPHA = 16 # LoRA alpha scaling\n",
    "LORA_DROPOUT = 0.1\n",
    "# Target modules vary by model, common ones for BERT/RoBERTa:\n",
    "LORA_TARGET_MODULES = [\"query\", \"value\"] # Common target layers for attention\n",
    "\n",
    "# --- Label Mapping (Essential for Transformers) ---\n",
    "LABEL_LIST = ['negative', 'neutral', 'positive'] # Define explicit order\n",
    "LABEL2ID = {label: i for i, label in enumerate(LABEL_LIST)}\n",
    "ID2LABEL = {i: label for i, label in enumerate(LABEL_LIST)}\n",
    "NUM_CLASSES = len(LABEL_LIST)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "METRICS_TO_CALCULATE = [\n",
    "    \"Accuracy\",\n",
    "    \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "    \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "    \"Train Time (s)\", \"Eval Time (s)\"\n",
    "]\n",
    "METRIC_FOR_BEST_MODEL = \"f1_macro\" # Metric to monitor for early stopping/best model saving\n",
    "\n",
    "# --- Datasets Configuration ---\n",
    "DATASETS_TO_PROCESS = {\n",
    "    \"Book Review\": {\n",
    "        \"train_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TEST_FN}'),\n",
    "        \"model_dir\": BOOK_REVIEW_MODEL_DIR,\n",
    "        \"result_dir\": BOOK_REVIEW_RESULT_DIR,\n",
    "    },\n",
    "    \"Financial News\": {\n",
    "        \"train_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TEST_FN}'),\n",
    "        \"model_dir\": FINANCIAL_NEWS_MODEL_DIR,\n",
    "        \"result_dir\": FINANCIAL_NEWS_RESULT_DIR,\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Model Configurations to Run ---\n",
    "# Define the models and settings for the experiment loop\n",
    "# Format: ('Experiment Name', 'HuggingFace Model ID', use_lora_flag, is_feature_extractor_run)\n",
    "MODEL_CONFIGURATIONS = [\n",
    "    # Feature Extractors\n",
    "    ('DistilBERT Feature Extractor + LR', 'distilbert-base-uncased', False, True),\n",
    "    ('BERT Feature Extractor + LR',       'bert-base-uncased',       False, True),\n",
    "\n",
    "    # Full Fine-tuning\n",
    "    ('DistilBERT Full FT', 'distilbert-base-uncased', False, False),\n",
    "    ('BERT Full FT',       'bert-base-uncased',       False, False),\n",
    "    ('RoBERTa Full FT',    'roberta-base',            False, False),\n",
    "    ('FinBERT Full FT',    'ProsusAI/finbert',        False, False), # Domain-specific\n",
    "\n",
    "    # LoRA Fine-tuning (only run if USE_LORA is True)\n",
    "    ('BERT LoRA FT',       'bert-base-uncased',       True, False),\n",
    "    ('RoBERTa LoRA FT',    'roberta-base',            True, False),\n",
    "    # ('FinBERT LoRA FT',    'ProsusAI/finbert',        True, False), # Can also apply LoRA to FinBERT\n",
    "] if USE_LORA else [ # Exclude LoRA runs if USE_LORA is False\n",
    "    ('DistilBERT Feature Extractor + LR', 'distilbert-base-uncased', False, True),\n",
    "    ('BERT Feature Extractor + LR',       'bert-base-uncased',       False, True),\n",
    "    ('DistilBERT Full FT', 'distilbert-base-uncased', False, False),\n",
    "    ('BERT Full FT',       'bert-base-uncased',       False, False),\n",
    "    ('RoBERTa Full FT',    'roberta-base',            False, False),\n",
    "    ('FinBERT Full FT',    'ProsusAI/finbert',        False, False),\n",
    "]\n",
    "\n",
    "# Check if FinBERT model ID needs adjustment (sometimes name changes)\n",
    "# Example alternative: 'yiyanghkust/finbert-tone'\n",
    "FINBERT_MODEL_ID = 'ProsusAI/finbert'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e209c0",
   "metadata": {},
   "source": [
    "# 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28006188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_hf(path):\n",
    "    \"\"\"Loads a single CSV into a Hugging Face Dataset.\"\"\"\n",
    "    try:\n",
    "        # Load directly using datasets library\n",
    "        dataset = load_dataset('csv', data_files=path, split='train')\n",
    "        # Rename target column to 'label' (expected by Trainer) and map string labels to integers\n",
    "        if TARGET_COLUMN != 'label':\n",
    "            dataset = dataset.rename_column(TARGET_COLUMN, 'label')\n",
    "        dataset = dataset.map(lambda examples: {'label': LABEL2ID.get(str(examples['label']), -1)}, # Handle potential non-string labels robustly\n",
    "                              desc=\"Mapping labels to IDs\")\n",
    "        # Filter out examples where label mapping failed (label == -1)\n",
    "        original_size = len(dataset)\n",
    "        dataset = dataset.filter(lambda example: example['label'] != -1, desc=\"Filtering invalid labels\")\n",
    "        if len(dataset) < original_size:\n",
    "            logging.warning(f\"Filtered out {original_size - len(dataset)} examples with invalid labels from {path}.\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading dataset from {path}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def create_dataset_dict(train_path, val_path, test_path):\n",
    "    \"\"\"Loads train, validation, and test CSVs into a DatasetDict.\"\"\"\n",
    "    train_ds = load_data_hf(train_path)\n",
    "    val_ds = load_data_hf(val_path)\n",
    "    test_ds = load_data_hf(test_path)\n",
    "    if train_ds and val_ds and test_ds:\n",
    "        logging.info(f\"Loaded Train data: {len(train_ds)} examples\")\n",
    "        logging.info(f\"Loaded Validation data: {len(val_ds)} examples\")\n",
    "        logging.info(f\"Loaded Test data: {len(test_ds)} examples\")\n",
    "        return DatasetDict({\n",
    "            'train': train_ds,\n",
    "            'validation': val_ds,\n",
    "            'test': test_ds\n",
    "        })\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Tokenizes text data.\"\"\"\n",
    "    # Ensure text is string, handle potential None values\n",
    "    texts = [str(text) if text is not None else \"\" for text in examples[TEXT_COLUMN]]\n",
    "    return tokenizer(texts, truncation=True, padding=False, max_length=MAX_LENGTH) # Padding handled by DataCollator\n",
    "\n",
    "# Define metric computation function for Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    prec_macro = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    rec_macro = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "    prec_weighted = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    rec_weighted = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_macro': prec_macro,\n",
    "        'recall_macro': rec_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'precision_weighted': prec_weighted,\n",
    "        'recall_weighted': rec_weighted,\n",
    "    }\n",
    "\n",
    "def calculate_metrics_from_preds(y_true, y_pred):\n",
    "    \"\"\"Calculates evaluation metrics from direct predictions.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    # Print the classification report for detailed metrics\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 (Macro)\": f1_macro,\n",
    "        \"Precision (Macro)\": precision_macro,\n",
    "        \"Recall (Macro)\": recall_macro,\n",
    "        \"F1 (Weighted)\": f1_weighted,\n",
    "        \"Precision (Weighted)\": precision_weighted,\n",
    "        \"Recall (Weighted)\": recall_weighted,\n",
    "    }\n",
    "\n",
    "# Function to extract features (CLS token)\n",
    "def extract_hidden_states(batch, model, tokenizer, device):\n",
    "    # Ensure input_ids and attention_mask are tensors on the correct device\n",
    "    inputs = {k: v.to(device) for k, v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return the representation of the [CLS] token (first token)\n",
    "    # Move back to CPU to accumulate results if needed outside GPU loop\n",
    "    return last_hidden_state[:, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ba404",
   "metadata": {},
   "source": [
    "# 3. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8404b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loop through each dataset defined in the configuration ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    print(f\"\\n{'='*25} Processing Dataset: {dataset_name} {'='*25}\")\n",
    "    logging.info(f\"Processing Dataset: {dataset_name}\")\n",
    "\n",
    "    # 1. Load Data using Hugging Face Datasets\n",
    "    raw_datasets = create_dataset_dict(config['train_path'], config['val_path'], config['test_path'])\n",
    "    if not raw_datasets:\n",
    "        logging.error(f\"Could not load data for {dataset_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Loop through each model configuration ---\n",
    "    for model_label, model_id, use_lora, is_feature_extractor in MODEL_CONFIGURATIONS:\n",
    "\n",
    "        # --- Skip FinBERT for non-financial data ---\n",
    "        if model_id == FINBERT_MODEL_ID and dataset_name != \"Financial News\":\n",
    "            logging.info(f\"Skipping {model_label} for {dataset_name} (Model is domain-specific).\")\n",
    "            continue\n",
    "\n",
    "        # --- Skip LoRA runs if flag is off ---\n",
    "        if use_lora and not USE_LORA:\n",
    "            logging.info(f\"Skipping LoRA run {model_label} as USE_LORA is False.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing Model: {model_label} ---\")\n",
    "        logging.info(f\"Starting run for {model_label} on {dataset_name}\")\n",
    "        run_results = {\"Dataset\": dataset_name, \"Model\": model_label}\n",
    "        train_time = 0.0\n",
    "        eval_time = 0.0\n",
    "\n",
    "        # Create specific output dirs for this run's checkpoints/models\n",
    "        run_model_dir = os.path.join(config['model_dir'], model_label.replace(' ', '_').replace('+', ''))\n",
    "        os.makedirs(run_model_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # 2. Load Tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "            # 3. Tokenize Datasets\n",
    "            logging.info(f\"Tokenizing data using {model_id} tokenizer...\")\n",
    "            # Apply tokenization in batches\n",
    "            tokenized_datasets = raw_datasets.map(\n",
    "                lambda batch: preprocess_function(batch, tokenizer),\n",
    "                batched=True,\n",
    "                remove_columns=[TEXT_COLUMN], # Remove original text column\n",
    "                desc=\"Running tokenizer on dataset\"\n",
    "            )\n",
    "            # Data collator handles dynamic padding\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            logging.info(\"Tokenization complete.\")\n",
    "\n",
    "            # ===========================================\n",
    "            # === 4.A Feature Extraction + Classifier ===\n",
    "            # ===========================================\n",
    "            if is_feature_extractor:\n",
    "                logging.info(\"Running in Feature Extraction mode.\")\n",
    "                # Load base model (no classification head)\n",
    "                model = AutoModel.from_pretrained(model_id).to(DEVICE)\n",
    "                model.eval() # Set to evaluation mode\n",
    "\n",
    "                # --- Extract Features ---\n",
    "                logging.info(\"Extracting features from datasets...\")\n",
    "                start_extract_time = time.time()\n",
    "\n",
    "                # Need dataloaders for batching feature extraction\n",
    "                tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "                train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=FEATURE_EXTRACTOR_BATCH_SIZE, collate_fn=data_collator)\n",
    "                val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=FEATURE_EXTRACTOR_BATCH_SIZE, collate_fn=data_collator)\n",
    "                test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=FEATURE_EXTRACTOR_BATCH_SIZE, collate_fn=data_collator)\n",
    "\n",
    "                X_train_features = []\n",
    "                y_train_labels = []\n",
    "                for batch in train_dataloader:\n",
    "                    y_train_labels.extend(batch['labels'].tolist())\n",
    "                    batch_features = extract_hidden_states(batch, model, tokenizer, DEVICE)\n",
    "                    X_train_features.append(batch_features)\n",
    "\n",
    "                X_val_features = []\n",
    "                y_val_labels = []\n",
    "                for batch in val_dataloader:\n",
    "                    y_val_labels.extend(batch['labels'].tolist())\n",
    "                    batch_features = extract_hidden_states(batch, model, tokenizer, DEVICE)\n",
    "                    X_val_features.append(batch_features)\n",
    "\n",
    "                X_test_features = []\n",
    "                y_test_labels = []\n",
    "                for batch in test_dataloader:\n",
    "                    y_test_labels.extend(batch['labels'].tolist())\n",
    "                    batch_features = extract_hidden_states(batch, model, tokenizer, DEVICE)\n",
    "                    X_test_features.append(batch_features)\n",
    "\n",
    "                X_train_features = np.concatenate(X_train_features)\n",
    "                X_val_features = np.concatenate(X_val_features)\n",
    "                X_test_features = np.concatenate(X_test_features)\n",
    "                end_extract_time = time.time()\n",
    "                logging.info(f\"Feature extraction took {end_extract_time - start_extract_time:.2f}s\")\n",
    "                logging.info(f\"Train features shape: {X_train_features.shape}\")\n",
    "\n",
    "                # --- Train Classifier ---\n",
    "                logging.info(\"Training Logistic Regression classifier...\")\n",
    "                classifier = LogisticRegression(max_iter=LOGREG_MAX_ITER, random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "                start_train_time = time.time()\n",
    "                # Combine train + val features for final classifier training? Or tune on val? Simpler: train on train, eval on test.\n",
    "                classifier.fit(X_train_features, y_train_labels)\n",
    "                end_train_time = time.time()\n",
    "                train_time = end_train_time - start_train_time\n",
    "                logging.info(f\"Classifier training took {train_time:.2f}s\")\n",
    "\n",
    "                # --- Evaluate Classifier ---\n",
    "                start_eval_time = time.time()\n",
    "                y_pred_test = classifier.predict(X_test_features)\n",
    "                end_eval_time = time.time()\n",
    "                eval_time = end_eval_time - start_eval_time\n",
    "\n",
    "                test_metrics = calculate_metrics_from_preds(y_test_labels, y_pred_test)\n",
    "                run_results.update(test_metrics)\n",
    "\n",
    "                logging.info(\"Feature Extractor + LR - Test Set Performance:\")\n",
    "                report_str = classification_report(y_test_labels, y_pred_test, target_names=LABEL_LIST, zero_division=0)\n",
    "                print(report_str)\n",
    "\n",
    "                cm = confusion_matrix(y_test_labels, y_pred_test, labels=list(range(NUM_CLASSES))) # Ensure labels are ordered\n",
    "                cm_df = pd.DataFrame(cm, index=LABEL_LIST, columns=LABEL_LIST)\n",
    "                print(\"Confusion Matrix (Test Set):\")\n",
    "                print(cm_df)\n",
    "\n",
    "                cm_filename = f\"{dataset_name.replace(' ', '_')}_{model_label.replace(' ', '_').replace('+','')}_confusion_matrix.csv\"\n",
    "                cm_save_path = os.path.join(config['result_dir'], cm_filename)\n",
    "                try:\n",
    "                    cm_df.to_csv(cm_save_path)\n",
    "                    logging.info(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "                except Exception as cm_e:\n",
    "                    logging.error(f\"Failed to save confusion matrix to {cm_save_path}: {cm_e}\")\n",
    "\n",
    "\n",
    "                # Save the classifier\n",
    "                clf_save_path = os.path.join(run_model_dir, f\"{dataset_name.replace(' ', '_')}_{model_label.replace(' ', '_')}_LR_classifier.joblib\")\n",
    "                joblib.dump(classifier, clf_save_path)\n",
    "                logging.info(f\"Logistic Regression classifier saved to {clf_save_path}\")\n",
    "\n",
    "                # Cleanup GPU memory used by the base model\n",
    "                del model\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "            # ===========================================\n",
    "            # === 4.B Fine-tuning (Full or LoRA)     ====\n",
    "            # ===========================================\n",
    "            else:\n",
    "                logging.info(f\"Running in Fine-tuning mode (LoRA: {use_lora}).\")\n",
    "                # Load model with sequence classification head\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    model_id,\n",
    "                    num_labels=NUM_CLASSES,\n",
    "                    id2label=ID2LABEL,\n",
    "                    label2id=LABEL2ID\n",
    "                )\n",
    "\n",
    "                # --- Apply LoRA if specified ---\n",
    "                if use_lora:\n",
    "                    logging.info(\"Applying LoRA configuration...\")\n",
    "                    peft_config = LoraConfig(\n",
    "                        task_type=TaskType.SEQ_CLS, # Specify task type\n",
    "                        r=LORA_R,\n",
    "                        lora_alpha=LORA_ALPHA,\n",
    "                        lora_dropout=LORA_DROPOUT,\n",
    "                        target_modules=LORA_TARGET_MODULES,\n",
    "                        bias=\"none\" # Usually set bias to 'none' or 'all'\n",
    "                    )\n",
    "                    model = get_peft_model(model, peft_config)\n",
    "                    model.print_trainable_parameters() # Verify LoRA application\n",
    "\n",
    "                model.to(DEVICE) # Move model to GPU before Trainer\n",
    "\n",
    "                # --- Define Training Arguments ---\n",
    "                training_args = TrainingArguments(\n",
    "                    output_dir=os.path.join(run_model_dir, \"checkpoints\"),\n",
    "                    logging_dir=os.path.join(run_model_dir, \"logs\"),\n",
    "                    report_to=\"none\", # Disable wandb/tensorboard reporting unless configured\n",
    "                    num_train_epochs=NUM_EPOCHS,\n",
    "                    learning_rate=LEARNING_RATE,\n",
    "                    weight_decay=WEIGHT_DECAY,\n",
    "                    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "                    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "\n",
    "                    # --- Strategies ---\n",
    "                    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "                    save_strategy=\"epoch\",        # Save a checkpoint at the end of each epoch\n",
    "                    logging_strategy=\"epoch\",     # Log metrics at the end of each epoch (consistent)\n",
    "\n",
    "                    # --- Explicitly disable step-based intervals when using epoch strategy ---\n",
    "                    eval_steps=None,              # Do not evaluate every N steps\n",
    "                    save_steps=None,              # Do not save every N steps\n",
    "                    logging_steps=None,           # Do not log every N steps (use logging_strategy=\"epoch\")\n",
    "                    # Note: If you WANT step-based logging while using epoch eval/save, you can set\n",
    "                    # logging_strategy=\"steps\" and provide a value for logging_steps, but keep\n",
    "                    # eval_steps=None and save_steps=None.\n",
    "\n",
    "                    # --- Best model loading ---\n",
    "                    load_best_model_at_end=True, # Load the best model based on metric_for_best_model\n",
    "                    metric_for_best_model=METRIC_FOR_BEST_MODEL, # e.g., \"f1_macro\"\n",
    "                    greater_is_better=True,      # F1 score is better when higher\n",
    "                    save_total_limit=2,          # Only keep the best and the latest checkpoint\n",
    "\n",
    "                    # --- Other settings ---\n",
    "                    fp16=FP16,                   # Enable mixed precision training if GPU supports it\n",
    "                    # logging_steps=50,          # Remove or comment out if using logging_strategy=\"epoch\"\n",
    "                    # dataloader_num_workers=2,  # Optional\n",
    "                    gradient_accumulation_steps=1,\n",
    "                    seed=RANDOM_STATE,\n",
    "                    remove_unused_columns=True, # Default is True, good practice\n",
    "                )\n",
    "\n",
    "                print(f\"Using evaluation_strategy: {training_args.eval_strategy}\") # Add this print statement\n",
    "                print(f\"Using save_strategy: {training_args.save_strategy}\")\n",
    "                print(f\"Using load_best_model_at_end: {training_args.load_best_model_at_end}\")\n",
    "\n",
    "                # --- Define Trainer ---\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=tokenized_datasets[\"train\"],\n",
    "                    eval_dataset=tokenized_datasets[\"validation\"], # Use validation set for evaluation during training\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=data_collator,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)] # Stop if metric doesn't improve enough\n",
    "                )\n",
    "\n",
    "                # --- Train the Model ---\n",
    "                logging.info(\"Starting fine-tuning...\")\n",
    "                start_train_time = time.time()\n",
    "                train_result = trainer.train()\n",
    "                end_train_time = time.time()\n",
    "                train_time = end_train_time - start_train_time\n",
    "                logging.info(f\"Fine-tuning completed in {train_time:.2f}s\")\n",
    "\n",
    "                # --- Evaluate on Test Set ---\n",
    "                logging.info(\"Evaluating model on the test set...\")\n",
    "                start_eval_time = time.time()\n",
    "                # Evaluate first to get metrics like loss\n",
    "                test_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
    "                # Then predict to get raw predictions for confusion matrix\n",
    "                predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "                end_eval_time = time.time()\n",
    "                eval_time = end_eval_time - start_eval_time\n",
    "\n",
    "                # Extract predictions and labels\n",
    "                y_pred_test = np.argmax(predictions.predictions, axis=1)\n",
    "                y_true_test = predictions.label_ids\n",
    "\n",
    "                # Map trainer metric names (e.g., 'eval_f1_macro') to our standard names\n",
    "                run_results[\"Accuracy\"] = test_results.get('eval_accuracy', np.nan)\n",
    "                run_results[\"F1 (Macro)\"] = test_results.get('eval_f1_macro', np.nan)\n",
    "                run_results[\"Precision (Macro)\"] = test_results.get('eval_precision_macro', np.nan)\n",
    "                run_results[\"Recall (Macro)\"] = test_results.get('eval_recall_macro', np.nan)\n",
    "                run_results[\"F1 (Weighted)\"] = test_results.get('eval_f1_weighted', np.nan)\n",
    "                run_results[\"Precision (Weighted)\"] = test_results.get('eval_precision_weighted', np.nan)\n",
    "                run_results[\"Recall (Weighted)\"] = test_results.get('eval_recall_weighted', np.nan)\n",
    "\n",
    "                logging.info(\"Fine-tuning - Test Set Performance:\")\n",
    "                logging.info(test_results) # Log the full results dict from trainer\n",
    "\n",
    "                # Generate, print, and save the confusion matrix using predictions\n",
    "                logging.info(\"Fine-tuning - Test Set Confusion Matrix:\")\n",
    "                cm = confusion_matrix(y_true_test, y_pred_test, labels=list(range(NUM_CLASSES))) # Ensure labels are ordered\n",
    "                cm_df = pd.DataFrame(cm, index=LABEL_LIST, columns=LABEL_LIST)\n",
    "                print(cm_df)\n",
    "\n",
    "                # Save the confusion matrix\n",
    "                cm_filename = f\"{dataset_name.replace(' ', '_')}_{model_label.replace(' ', '_').replace('+','')}_confusion_matrix.csv\"\n",
    "                cm_save_path = os.path.join(config['result_dir'], cm_filename)\n",
    "                try:\n",
    "                    cm_df.to_csv(cm_save_path, mode='w+')\n",
    "                    logging.info(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "                except Exception as cm_e:\n",
    "                    logging.error(f\"Failed to save confusion matrix to {cm_save_path}: {cm_e}\")\n",
    "\n",
    "                # --- Save the Final Model & Tokenizer ---\n",
    "                # Trainer already saved the best checkpoint based on validation set.\n",
    "                # For LoRA, the main model is saved by Trainer, adapters need separate save\n",
    "                final_model_save_path = os.path.join(run_model_dir, \"final_model\")\n",
    "                if use_lora:\n",
    "                    logging.info(f\"Saving LoRA adapter model to {final_model_save_path}\")\n",
    "                    model.save_pretrained(final_model_save_path) # Saves only the adapter\n",
    "                else:\n",
    "                    # If not LoRA, trainer saved the full best model, we can optionally save it again here\n",
    "                    # under a consistent name if needed, but load_best_model_at_end handles loading it.\n",
    "                    # Saving explicitly:\n",
    "                    # trainer.save_model(final_model_save_path)\n",
    "                    logging.info(f\"Best model loaded by Trainer. Checkpoint saved in {training_args.output_dir}\")\n",
    "\n",
    "\n",
    "                tokenizer.save_pretrained(final_model_save_path) # Save tokenizer with the model/adapter\n",
    "                logging.info(f\"Tokenizer saved to {final_model_save_path}\")\n",
    "\n",
    "            # --- Store Timings and Finalize Results ---\n",
    "            run_results[\"Train Time (s)\"] = round(train_time, 3)\n",
    "            run_results[\"Eval Time (s)\"] = round(eval_time, 3)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"!!! An error occurred while processing {model_label} for {dataset_name}: {e}\", exc_info=True)\n",
    "            # Record partial results if possible\n",
    "            run_results[\"Accuracy\"] = np.nan\n",
    "            run_results[\"F1 (Macro)\"] = np.nan\n",
    "            # Fill other metrics with NaN or error messages\n",
    "            for metric in METRICS_TO_CALCULATE:\n",
    "                if metric not in run_results:\n",
    "                    run_results[metric] = np.nan if metric not in [\"Train Time (s)\", \"Eval Time (s)\"] else 0.0\n",
    "        finally:\n",
    "            all_results.append(run_results)\n",
    "            # Clean up memory aggressively after each run\n",
    "            del tokenizer\n",
    "            if 'model' in locals(): del model\n",
    "            if 'trainer' in locals(): del trainer\n",
    "            if 'classifier' in locals(): del classifier\n",
    "            if 'tokenized_datasets' in locals(): del tokenized_datasets\n",
    "            # if 'raw_datasets' in locals(): del raw_datasets\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- Combine results into a DataFrame ---\n",
    "results_df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f17493",
   "metadata": {},
   "source": [
    "# 4. Results Summary and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870708ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n===== Overall LLM Results Summary =====\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1400) # Wider display\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Ensure all expected columns exist, fill with NaN if necessary\n",
    "for col in METRICS_TO_CALCULATE:\n",
    "    if col not in results_df.columns:\n",
    "        results_df[col] = np.nan\n",
    "\n",
    "# Reorder columns for clarity\n",
    "column_order = [\"Dataset\", \"Model\"] + METRICS_TO_CALCULATE\n",
    "# Filter out columns not present if something went wrong during creation\n",
    "column_order = [col for col in column_order if col in results_df.columns]\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# --- Save results to CSV for each dataset ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    dataset_results_df = results_df[results_df['Dataset'] == dataset_name]\n",
    "    if not dataset_results_df.empty:\n",
    "        results_filename = f\"{dataset_name.replace(' ', '_')}_llm_transformers_results.csv\"\n",
    "        results_save_path = os.path.join(config['result_dir'], results_filename)\n",
    "        try:\n",
    "            dataset_results_df.to_csv(results_save_path, index=False, mode='w+')\n",
    "            print(f\"\\nResults for {dataset_name} saved to {results_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving results for {dataset_name} to {results_save_path}: {e}\")\n",
    "\n",
    "# --- Save combined results ---\n",
    "combined_results_path = os.path.join(RESULT_DIR, \"combined_llm_transformers_results.csv\")\n",
    "try:\n",
    "    results_df.to_csv(combined_results_path, index=False, mode='w+')\n",
    "    print(f\"\\nCombined results saved to {combined_results_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving combined results to {combined_results_path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
