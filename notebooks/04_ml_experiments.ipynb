{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a962e71",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving/loading sklearn models\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC # Generally preferred for text over SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import lightgbm as lgb # LightGBM\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40e077",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe219ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "DATA_DIR = \"../data/processed\"\n",
    "MODEL_SAVE_DIR = \"../models/ml\"\n",
    "RESULTS_SAVE_DIR = \"../results\"\n",
    "RESULTS_CSV_FILE = os.path.join(RESULTS_SAVE_DIR, \"ml_results_summary.csv\") # Specific filename\n",
    "\n",
    "# --- Experiment Setup ---\n",
    "DOMAINS = [\"book_reviews\", \"financial_news\"] # Add your domain folder names\n",
    "\n",
    "# --- ML Models ---\n",
    "# Use LinearSVC for SVM with text data - often faster and performs well\n",
    "# Increased max_iter for Logistic Regression and LinearSVC for convergence\n",
    "ML_MODELS = {\n",
    "    \"NaiveBayes\": MultinomialNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"SVM\": LinearSVC(random_state=42, max_iter=2000, dual=\"auto\"), # dual=\"auto\" handles sparse/dense cases\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1), # n_jobs=-1 uses all cores\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# --- Feature Extractor Settings ---\n",
    "# Primary: TF-IDF with Uni+Bigrams\n",
    "TFIDF_CONFIG = {\n",
    "    \"vectorizer\": TfidfVectorizer(\n",
    "        max_features=20000, # Adjust based on memory/performance\n",
    "        ngram_range=(1, 2), # Unigrams and Bigrams\n",
    "        sublinear_tf=True # Apply sublinear tf scaling (often helps)\n",
    "    ),\n",
    "    \"name\": \"TFIDF_UniBiGram\"\n",
    "}\n",
    "\n",
    "# Secondary: BoW (Counts) with Unigrams (Primarily for Naive Bayes comparison)\n",
    "BOW_CONFIG = {\n",
    "    \"vectorizer\": CountVectorizer(\n",
    "        max_features=20000, # Match TFIDF for comparison if desired, or adjust\n",
    "        ngram_range=(1, 1) # Unigrams only for standard BoW/NB\n",
    "    ),\n",
    "    \"name\": \"BoW_UniGram\"\n",
    "}\n",
    "\n",
    "# --- Reproducibility ---\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n",
    "for domain in DOMAINS:\n",
    "    os.makedirs(os.path.join(MODEL_SAVE_DIR, domain), exist_ok=True)\n",
    "    os.makedirs(os.path.join(RESULTS_SAVE_DIR, domain), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee368c3",
   "metadata": {},
   "source": [
    "# 3. Helper Functions (Load Data, Extract Features, Train/Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(domain_name):\n",
    "    print(f\"\\nLoading data for domain: {domain_name}...\")\n",
    "    try:\n",
    "        # We primarily need train and test for this ML pipeline\n",
    "        train_path = os.path.join(DATA_DIR, domain_name, \"train.csv\")\n",
    "        test_path = os.path.join(DATA_DIR, domain_name, \"test.csv\")\n",
    "\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        # Basic validation\n",
    "        if 'text' not in train_df.columns or 'label' not in train_df.columns:\n",
    "            raise ValueError(\"Missing 'text' or 'label' column in train data\")\n",
    "        if 'text' not in test_df.columns or 'label' not in test_df.columns:\n",
    "            raise ValueError(\"Missing 'text' or 'label' column in test data\")\n",
    "\n",
    "        # Handle potential NaN values in text (important!)\n",
    "        train_df['text'].fillna('', inplace=True)\n",
    "        test_df['text'].fillna('', inplace=True)\n",
    "\n",
    "        print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "        return train_df['text'], train_df['label'], test_df['text'], test_df['label']\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data for {domain_name}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def extract_features(vectorizer_instance, train_texts, test_texts):\n",
    "    \"\"\"Fits vectorizer on training data and transforms train/test data.\"\"\"\n",
    "    print(f\"Extracting features using {vectorizer_instance.__class__.__name__}...\")\n",
    "    t0 = time()\n",
    "    X_train = vectorizer_instance.fit_transform(train_texts)\n",
    "    X_test = vectorizer_instance.transform(test_texts)\n",
    "    print(f\"Feature extraction done in {time() - t0:.2f}s\")\n",
    "    print(f\"Train features shape: {X_train.shape}, Test features shape: {X_test.shape}\")\n",
    "    return X_train, X_test, vectorizer_instance # Return fitted vectorizer\n",
    "\n",
    "def train_evaluate_model(model_name, model_instance, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains a model and evaluates it on the test set.\"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    t0 = time()\n",
    "    model_instance.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(f\"Training done in {train_time:.2f}s\")\n",
    "\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    t0 = time()\n",
    "    y_pred = model_instance.predict(X_test)\n",
    "    eval_time = time() - t0\n",
    "    print(f\"Evaluation done in {eval_time:.2f}s\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Use classification_report for detailed metrics\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (Macro)\": report_dict['macro avg']['precision'],\n",
    "        \"Recall (Macro)\": report_dict['macro avg']['recall'],\n",
    "        \"F1 (Macro)\": report_dict['macro avg']['f1-score'],\n",
    "        \"Precision (Weighted)\": report_dict['weighted avg']['precision'],\n",
    "        \"Recall (Weighted)\": report_dict['weighted avg']['recall'],\n",
    "        \"F1 (Weighted)\": report_dict['weighted avg']['f1-score'],\n",
    "        \"Train Time (s)\": train_time,\n",
    "        \"Eval Time (s)\": eval_time\n",
    "    }\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0)) # Print text report too\n",
    "\n",
    "    return results, model_instance # Return results dict and trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216b304",
   "metadata": {},
   "source": [
    "# 4. Main Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7124ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_list = [] # Store results dictionaries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in DOMAINS:\n",
    "    X_train_text, y_train, X_test_text, y_test = load_data(domain)\n",
    "    if X_train_text is None:\n",
    "        print(f\"Skipping domain {domain} due to data loading error.\")\n",
    "        continue\n",
    "\n",
    "    # --- Experiment with TF-IDF (Uni+Bigram) ---\n",
    "    print(f\"\\n--- Domain: {domain} | Feature Extractor: {TFIDF_CONFIG['name']} ---\")\n",
    "    tfidf_vectorizer_instance = TFIDF_CONFIG[\"vectorizer\"]\n",
    "    X_train_tfidf, X_test_tfidf, fitted_tfidf_vectorizer = extract_features(\n",
    "        tfidf_vectorizer_instance, X_train_text, X_test_text\n",
    "    )\n",
    "\n",
    "    # Save the fitted TF-IDF vectorizer\n",
    "    vec_save_path = os.path.join(MODEL_SAVE_DIR, domain, f\"vectorizer_{TFIDF_CONFIG['name']}.joblib\")\n",
    "    joblib.dump(fitted_tfidf_vectorizer, vec_save_path)\n",
    "    print(f\"Saved TF-IDF vectorizer to {vec_save_path}\")\n",
    "\n",
    "    # Run models compatible with TF-IDF\n",
    "    for model_name, model_instance in ML_MODELS.items():\n",
    "        # Naive Bayes typically prefers counts, but we run it on TF-IDF for comparison as requested\n",
    "        # if model_name == \"NaiveBayes\":\n",
    "        #     print(f\"\\n--- Running Model: {model_name} (on TF-IDF for comparison) ---\")\n",
    "        # elif model_name != \"NaiveBayes\": # Run other models on TF-IDF\n",
    "        print(f\"\\n--- Running Model: {model_name} (on TF-IDF) ---\")\n",
    "        # else:\n",
    "        #     continue # Skip NB here, will run with BoW later\n",
    "\n",
    "        # Get a fresh model instance for each run\n",
    "        current_model_instance = joblib.load(joblib.dump(model_instance, 'temp_model.joblib')) # Quick way to clone\n",
    "\n",
    "        model_results, trained_model = train_evaluate_model(\n",
    "            model_name, current_model_instance, X_train_tfidf, y_train, X_test_tfidf, y_test\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        model_results[\"Domain\"] = domain\n",
    "        model_results[\"Feature Extractor\"] = TFIDF_CONFIG['name']\n",
    "        all_results_list.append(model_results)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_save_path = os.path.join(MODEL_SAVE_DIR, domain, f\"{model_name.lower()}_{TFIDF_CONFIG['name']}.joblib\")\n",
    "        joblib.dump(trained_model, model_save_path)\n",
    "        print(f\"Saved model to {model_save_path}\")\n",
    "\n",
    "    # --- Experiment with BoW (UniGram) - Primarily for Naive Bayes ---\n",
    "    print(f\"\\n--- Domain: {domain} | Feature Extractor: {BOW_CONFIG['name']} ---\")\n",
    "    bow_vectorizer_instance = BOW_CONFIG[\"vectorizer\"]\n",
    "    X_train_bow, X_test_bow, fitted_bow_vectorizer = extract_features(\n",
    "        bow_vectorizer_instance, X_train_text, X_test_text\n",
    "    )\n",
    "\n",
    "    # Save the fitted BoW vectorizer\n",
    "    vec_save_path = os.path.join(MODEL_SAVE_DIR, domain, f\"vectorizer_{BOW_CONFIG['name']}.joblib\")\n",
    "    joblib.dump(fitted_bow_vectorizer, vec_save_path)\n",
    "    print(f\"Saved BoW vectorizer to {vec_save_path}\")\n",
    "\n",
    "    # Run Naive Bayes on BoW\n",
    "    model_name = \"NaiveBayes\"\n",
    "    if model_name in ML_MODELS:\n",
    "        print(f\"\\n--- Running Model: {model_name} (on BoW) ---\")\n",
    "        model_instance = ML_MODELS[model_name]\n",
    "        current_model_instance = joblib.load(joblib.dump(model_instance, 'temp_model.joblib')) # Clone\n",
    "\n",
    "        model_results, trained_model = train_evaluate_model(\n",
    "            model_name, current_model_instance, X_train_bow, y_train, X_test_bow, y_test\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        model_results[\"Domain\"] = domain\n",
    "        model_results[\"Feature Extractor\"] = BOW_CONFIG['name']\n",
    "        all_results_list.append(model_results)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_save_path = os.path.join(MODEL_SAVE_DIR, domain, f\"{model_name.lower()}_{BOW_CONFIG['name']}.joblib\")\n",
    "        joblib.dump(trained_model, model_save_path)\n",
    "        print(f\"Saved model to {model_save_path}\")\n",
    "    else:\n",
    "        print(f\"{model_name} not found in ML_MODELS dictionary.\")\n",
    "\n",
    "# Clean up temporary file used for cloning\n",
    "if os.path.exists('temp_model.joblib'):\n",
    "    os.remove('temp_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51dbaa",
   "metadata": {},
   "source": [
    "# 5. Aggregate and Save mResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Experiment Finished ---\")\n",
    "if all_results_list:\n",
    "    results_df = pd.DataFrame(all_results_list)\n",
    "    # Define desired column order\n",
    "    cols_order = [\"Domain\", \"Feature Extractor\", \"Model\", \"Accuracy\",\n",
    "                  \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "                  \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "                  \"Train Time (s)\", \"Eval Time (s)\"]\n",
    "    # Ensure all columns exist, add missing ones with NaN if necessary\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = np.nan\n",
    "    results_df = results_df[cols_order] # Reorder\n",
    "\n",
    "    print(\"\\nAggregated Results:\")\n",
    "    print(results_df.to_string()) # Print full dataframe\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df.to_csv(RESULTS_CSV_FILE, index=False)\n",
    "    print(f\"\\nResults saved to {RESULTS_CSV_FILE}\")\n",
    "else:\n",
    "    print(\"No results were generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
