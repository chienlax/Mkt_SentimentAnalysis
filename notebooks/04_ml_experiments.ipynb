{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94405be3",
   "metadata": {},
   "source": [
    "# 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3951a",
   "metadata": {},
   "source": [
    "## 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aacfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import joblib \n",
    "import optuna\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "import logging\n",
    "import psutil\n",
    "\n",
    "# --- Optuna Logging Configuration ---\n",
    "# Reduce Optuna's default logging verbosity\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# --- Configure other warnings ---\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# --- Limit CPU Usage ---\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94af111",
   "metadata": {},
   "source": [
    "## 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7444e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Constants ---\n",
    "\n",
    "BASE_DIR = \"..\" # Assuming the notebook is in a 'notebooks' or similar folder\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, \"models\", \"ml\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"result\")\n",
    "\n",
    "# --- Specific Dataset Paths ---\n",
    "BOOK_REVIEW_DATA_DIR = os.path.join(DATA_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_DATA_DIR = os.path.join(DATA_DIR, \"financial_news\")\n",
    "\n",
    "BOOK_REVIEW_MODEL_DIR = os.path.join(MODEL_OUTPUT_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_MODEL_DIR = os.path.join(MODEL_OUTPUT_DIR, \"financial_news\")\n",
    "\n",
    "BOOK_REVIEW_RESULT_DIR = os.path.join(RESULT_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_RESULT_DIR = os.path.join(RESULT_DIR, \"financial_news\")\n",
    "\n",
    "# --- Create directories if they don't exist ---\n",
    "os.makedirs(BOOK_REVIEW_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(BOOK_REVIEW_RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Names ---\n",
    "TRAIN_FN = \"train.csv\"\n",
    "VAL_FN = \"val.csv\"\n",
    "TEST_FN = \"test.csv\"\n",
    "\n",
    "# --- Column Names ---\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"score\"\n",
    "\n",
    "# --- TF-IDF Parameters ---\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MAX_FEATURES = 20000 # Keep consistent, could also be tuned\n",
    "\n",
    "# --- Model & Tuning Parameters ---\n",
    "RANDOM_STATE = 42\n",
    "N_TRIALS_OPTUNA = 25 # Number of trials for Optuna search\n",
    "CV_FOLDS = 3 # Number of folds for cross-validation within Optuna\n",
    "MAX_ITER_LOGREG_SVM = 1500 # Allow more iterations\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "METRICS_TO_CALCULATE = [\n",
    "    \"Accuracy\",\n",
    "    \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "    \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "    \"Train Time (s)\", \"Eval Time (s)\", \"Best Params\"\n",
    "]\n",
    "OPTIMIZATION_METRIC = 'F1 (Macro)' # Metric to optimize in Optuna (good for multiclass)\n",
    "\n",
    "# --- Datasets Configuration ---\n",
    "# Updated paths and added output directories\n",
    "DATASETS_TO_PROCESS = {\n",
    "    \"Book Review\": {\n",
    "        \"train_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TEST_FN}'),\n",
    "        \"model_dir\": BOOK_REVIEW_MODEL_DIR,\n",
    "        \"result_dir\": BOOK_REVIEW_RESULT_DIR,\n",
    "    },\n",
    "    \"Financial News\": {\n",
    "        \"train_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TEST_FN}'),\n",
    "        \"model_dir\": FINANCIAL_NEWS_MODEL_DIR,\n",
    "        \"result_dir\": FINANCIAL_NEWS_RESULT_DIR,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d10c8",
   "metadata": {},
   "source": [
    "# 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5118c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, val_path, test_path):\n",
    "    \"\"\"Loads train, validation, and test datasets from CSV files.\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        val_df = pd.read_csv(val_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"Loaded Train: {train_path}, Shape: {train_df.shape}\")\n",
    "        print(f\"Loaded Val:   {val_path}, Shape: {val_df.shape}\")\n",
    "        print(f\"Loaded Test:  {test_path}, Shape: {test_df.shape}\")\n",
    "\n",
    "        # Basic validation\n",
    "        if TEXT_COLUMN not in train_df.columns or TARGET_COLUMN not in train_df.columns:\n",
    "            raise ValueError(f\"Required columns '{TEXT_COLUMN}' or '{TARGET_COLUMN}' not found.\")\n",
    "\n",
    "        # Handle potential NaN values in text - replace with empty string\n",
    "        for df in [train_df, val_df, test_df]:\n",
    "            df[TEXT_COLUMN] = df[TEXT_COLUMN].fillna('')\n",
    "            # Ensure target column is treated consistently (e.g., as strings if labels are like 'positive')\n",
    "            # df[TARGET_COLUMN] = df[TARGET_COLUMN].astype(str) # Uncomment if labels need casting\n",
    "\n",
    "        print(f\"Train labels: {train_df[TARGET_COLUMN].unique()}, Val labels: {val_df[TARGET_COLUMN].unique()}, Test labels: {test_df[TARGET_COLUMN].unique()}\")\n",
    "        return train_df, val_df, test_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please check file paths.\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data loading: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, prefix=\"\"):\n",
    "    \"\"\"Calculates standard classification metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    metrics[f\"{prefix}Accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[f\"{prefix}F1 (Macro)\"] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics[f\"{prefix}Precision (Macro)\"] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics[f\"{prefix}Recall (Macro)\"] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    metrics[f\"{prefix}F1 (Weighted)\"] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics[f\"{prefix}Precision (Weighted)\"] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    metrics[f\"{prefix}Recall (Weighted)\"] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def objective_logreg(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Objective function for Logistic Regression tuning.\"\"\"\n",
    "    # Define hyperparameters to tune\n",
    "    logreg_c = trial.suggest_float(\"C\", 1e-4, 1e2, log=True) # Regularization strength (inverse)\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\"]) # Solvers good for L1/L2\n",
    "    penalty = \"l2\"\n",
    "    if solver == \"liblinear\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "    elif solver == \"saga\":\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", \"elasticnet\"])\n",
    "        if penalty == \"elasticnet\":\n",
    "            l1_ratio = trial.suggest_float(\"l1_ratio\", 0, 1) # Needs setting for elasticnet\n",
    "        else:\n",
    "            l1_ratio = None # Not used otherwise\n",
    "\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=logreg_c,\n",
    "        solver=solver,\n",
    "        penalty=penalty,\n",
    "        l1_ratio=l1_ratio if penalty == 'elasticnet' else None,\n",
    "        max_iter=MAX_ITER_LOGREG_SVM,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_macro = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "    return f1_macro # Optuna maximizes this\n",
    "\n",
    "def objective_linearsvc(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Objective function for LinearSVC tuning.\"\"\"\n",
    "    svc_c = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"hinge\", \"squared_hinge\"])\n",
    "    # dual='auto' is generally safe\n",
    "    model = LinearSVC(\n",
    "        C=svc_c,\n",
    "        loss=loss,\n",
    "        max_iter=MAX_ITER_LOGREG_SVM * 2,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        dual='auto'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_macro = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "    return f1_macro\n",
    "\n",
    "def objective_randomforest(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Objective function for RandomForest tuning.\"\"\"\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50, log=True) # Depth of trees\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20) # Min samples to split node\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)  # Min samples at leaf node\n",
    "    # max_features = trial.suggest_categorical(\"max_features\", ['sqrt', 'log2', None]) # Features to consider\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        # max_features=max_features,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_macro = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "    return f1_macro\n",
    "\n",
    "def objective_lgbm(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Objective function for LightGBM tuning.\"\"\"\n",
    "    # Determine objective based on number of unique classes\n",
    "    num_classes = len(np.unique(np.concatenate((y_train, y_val))))\n",
    "    objective = 'multiclass' if num_classes > 2 else 'binary'\n",
    "\n",
    "    params = {\n",
    "        'objective': objective,\n",
    "        'metric': 'multi_logloss' if objective == 'multiclass' else 'binary_logloss', # Logloss often used for training metric\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True), # L2 regularization\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0), # Feature fraction\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0), # Data fraction (bagging)\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "    if objective == 'multiclass':\n",
    "        params['num_class'] = num_classes # Required for multiclass\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              # Use early stopping to prevent overfitting and speed up trials\n",
    "              callbacks=[optuna.integration.LightGBMPruningCallback(trial, 'multi_logloss' if objective == 'multiclass' else 'binary_logloss')] # Use same metric as 'metric'\n",
    "             )\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    f1_macro = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "    return f1_macro\n",
    "\n",
    "\n",
    "# --- Map model names to their objective functions ---\n",
    "# Naive Bayes is usually not tuned extensively with Optuna, handled separately.\n",
    "objective_map = {\n",
    "    \"Logistic Regression\": objective_logreg,\n",
    "    \"Linear SVM\": objective_linearsvc,\n",
    "    \"Random Forest\": objective_randomforest,\n",
    "    \"LightGBM\": objective_lgbm,\n",
    "}\n",
    "\n",
    "# --- Define base model instances (needed for Naive Bayes and as template) ---\n",
    "base_models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "    \"Linear SVM\": LinearSVC(random_state=RANDOM_STATE, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bd23a",
   "metadata": {},
   "source": [
    "# 3. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_run = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=MAX_ITER_LOGREG_SVM,\n",
    "        class_weight='balanced',\n",
    "        solver='liblinear'\n",
    "    ),\n",
    "    \"Linear SVM\": LinearSVC(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=MAX_ITER_LOGREG_SVM * 2, \n",
    "        class_weight='balanced',\n",
    "        dual=False \n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        objective='multiclass',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78efb050",
   "metadata": {},
   "source": [
    "# 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78372421",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "vectorizers = {} # Store vectorizers per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loop through each dataset defined in the configuration ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    print(f\"\\n{'='*20} Processing Dataset: {dataset_name} {'='*20}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    train_df, val_df, test_df = load_dataset(config['train_path'], config['val_path'], config['test_path'])\n",
    "    if train_df is None:\n",
    "        print(f\"Skipping dataset {dataset_name} due to loading error.\")\n",
    "        continue\n",
    "\n",
    "    # Prepare data splits\n",
    "    X_train_raw = train_df[TEXT_COLUMN]\n",
    "    y_train = train_df[TARGET_COLUMN]\n",
    "    X_val_raw = val_df[TEXT_COLUMN]\n",
    "    y_val = val_df[TARGET_COLUMN]\n",
    "    X_test_raw = test_df[TEXT_COLUMN]\n",
    "    y_test = test_df[TARGET_COLUMN]\n",
    "\n",
    "    # Combine Train and Validation for final model training after tuning\n",
    "    X_train_val_raw = pd.concat([X_train_raw, X_val_raw], ignore_index=True)\n",
    "    y_train_val = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nTraining data shape: {X_train_raw.shape}, Validation data shape: {X_val_raw.shape}, Test data shape: {X_test_raw.shape}\")\n",
    "    print(f\"Combined Train+Val shape: {X_train_val_raw.shape}\")\n",
    "\n",
    "    # 2. Feature Extraction (TF-IDF)\n",
    "    print(\"\\nFitting TF-IDF Vectorizer on Training data...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        ngram_range=NGRAM_RANGE,\n",
    "        max_features=MAX_FEATURES\n",
    "    )\n",
    "    # Fit only on original training data and transform on the rest\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_raw)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val_raw)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test_raw)\n",
    "    X_train_val_tfidf = tfidf_vectorizer.transform(X_train_val_raw)\n",
    "\n",
    "    vectorizers[dataset_name] = tfidf_vectorizer # Store vectorizer\n",
    "    vectorizer_path = os.path.join(config['model_dir'], f\"{dataset_name.replace(' ', '_')}_tfidf_vectorizer.joblib\")\n",
    "    joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "    print(f\"TF-IDF Vectorizer saved to {vectorizer_path}\")\n",
    "    print(f\"TF-IDF Matrix Shape (Train): {X_train_tfidf.shape}\")\n",
    "    print(f\"TF-IDF Matrix Shape (Val):   {X_val_tfidf.shape}\")\n",
    "    print(f\"TF-IDF Matrix Shape (Test):  {X_test_tfidf.shape}\")\n",
    "    print(f\"TF-IDF Matrix Shape (Train+Val): {X_train_val_tfidf.shape}\")\n",
    "\n",
    "    # --- Loop through each model defined ---\n",
    "    for model_name, base_model_instance in base_models.items():\n",
    "        print(f\"\\n--- Processing Model: {model_name} ---\")\n",
    "        results = {\"Dataset\": dataset_name, \"Model\": model_name}\n",
    "        best_params = None\n",
    "\n",
    "        try:\n",
    "            # --- Hyperparameter Tuning (if applicable) ---\n",
    "            if model_name in objective_map:\n",
    "                print(f\"Starting Optuna tuning for {model_name} ({N_TRIALS_OPTUNA} trials)...\")\n",
    "                objective_func = objective_map[model_name]\n",
    "\n",
    "                # Wrap objective to pass fixed data arguments\n",
    "                wrapped_objective = lambda trial: objective_func(trial, X_train_tfidf, y_train, X_val_tfidf, y_val)\n",
    "\n",
    "                study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner()) # Use pruning\n",
    "                study.optimize(wrapped_objective, n_trials=N_TRIALS_OPTUNA)\n",
    "\n",
    "                best_params = study.best_params\n",
    "                best_value = study.best_value\n",
    "                results[\"Best Params\"] = str(best_params) # Store as string for CSV\n",
    "                print(f\"Optuna tuning finished. Best Validation {OPTIMIZATION_METRIC}: {best_value:.4f}\")\n",
    "                print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "                # Instantiate the final model with best parameters\n",
    "                final_model = base_model_instance.set_params(**best_params)\n",
    "                # Special handling for LGBM objective/num_class if needed (usually handled by fit)\n",
    "                if model_name == 'LightGBM':\n",
    "                    num_classes = len(y_train_val.unique())\n",
    "                    if num_classes > 2:\n",
    "                        final_model.set_params(objective='multiclass', num_class=num_classes)\n",
    "                    else:\n",
    "                        final_model.set_params(objective='binary')\n",
    "\n",
    "\n",
    "            else: # Handle Naive Bayes (no Optuna tuning here)\n",
    "                print(\"Using default parameters for Naive Bayes.\")\n",
    "                final_model = base_model_instance # Use the default instance\n",
    "                results[\"Best Params\"] = \"Default\"\n",
    "\n",
    "\n",
    "            # --- Train Final Model ---\n",
    "            print(f\"Training final {model_name} model on combined Train+Val data...\")\n",
    "            start_train_time = time.time()\n",
    "            # Train on combined Train + Validation data\n",
    "            final_model.fit(X_train_val_tfidf, y_train_val)\n",
    "            end_train_time = time.time()\n",
    "            results[\"Train Time (s)\"] = round(end_train_time - start_train_time, 3)\n",
    "            print(f\"Final model training completed in {results['Train Time (s)']:.3f} seconds.\")\n",
    "\n",
    "            # --- Evaluate on Test Set ---\n",
    "            print(f\"Evaluating final {model_name} model on Test data...\")\n",
    "            start_eval_time = time.time()\n",
    "            y_pred_test = final_model.predict(X_test_tfidf)\n",
    "            end_eval_time = time.time()\n",
    "            results[\"Eval Time (s)\"] = round(end_eval_time - start_eval_time, 3)\n",
    "\n",
    "            # Calculate test metrics\n",
    "            test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "            results.update(test_metrics) # Add test metrics to results dict\n",
    "\n",
    "            print(\"\\nTest Set Performance:\")\n",
    "            labels_order = sorted(y_test.unique()) \n",
    "            print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "            print(f\"Test Accuracy: {results['Accuracy']:.4f}\")\n",
    "            print(f\"Test F1 (Macro): {results['F1 (Macro)']:.4f}\")\n",
    "\n",
    "            # --- Calculate and Save Confusion Matrix CSV ---\n",
    "            cm = confusion_matrix(y_test, y_pred_test, labels=labels_order)\n",
    "            cm_df = pd.DataFrame(cm, index=labels_order, columns=labels_order)\n",
    "            cm_df.index.name = 'True Label'\n",
    "            cm_df.columns.name = 'Predicted Label'\n",
    "            \n",
    "            cm_filename = f\"{dataset_name.replace(' ', '_')}_{model_name.replace(' ', '_')}_confusion_matrix.csv\"\n",
    "            cm_save_path = os.path.join(config['result_dir'], cm_filename)\n",
    "            try:\n",
    "                cm_df.to_csv(cm_save_path, index=True, mode='w+')\n",
    "                print(f\"\\nConfusion matrix saved to {cm_save_path}\")\n",
    "            except Exception as cm_save_e:\n",
    "                print(f\"\\nError saving confusion matrix CSV for {model_name}: {cm_save_e}\")\n",
    "            # --- End Save Confusion Matrix CSV ---\n",
    "\n",
    "            # --- Save Final Model ---\n",
    "            model_filename = f\"{dataset_name.replace(' ', '_')}_{model_name.replace(' ', '_')}_best_model.joblib\"\n",
    "            model_save_path = os.path.join(config['model_dir'], model_filename)\n",
    "            joblib.dump(final_model, model_save_path)\n",
    "            print(f\"Final tuned model saved to {model_save_path}\")\n",
    "\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! An error occurred while processing {model_name} for {dataset_name}: {e}\")\n",
    "            # Record partial results if possible\n",
    "            results[\"Accuracy\"] = np.nan\n",
    "            results[\"F1 (Macro)\"] = np.nan\n",
    "            results[\"Best Params\"] = f\"Error: {e}\"\n",
    "            # Fill other metrics with NaN or error messages\n",
    "            for metric in METRICS_TO_CALCULATE:\n",
    "                if metric not in results:\n",
    "                    results[metric] = np.nan if metric not in [\"Train Time (s)\", \"Eval Time (s)\", \"Best Params\"] else 0.0\n",
    "\n",
    "        all_results.append(results)\n",
    "\n",
    "\n",
    "# --- Combine results into a DataFrame ---\n",
    "results_df = pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50075484",
   "metadata": {},
   "source": [
    "# 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ae866",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n===== Overall Tuned ML Results Summary =====\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1200) # Wider display\n",
    "pd.set_option('display.max_colwidth', 200) # Show more of Best Params column\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Reorder columns for clarity\n",
    "column_order = [\"Dataset\", \"Model\"] + [m for m in METRICS_TO_CALCULATE if m != \"Best Params\"] + [\"Best Params\"]\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# --- Save results to CSV for each dataset ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    dataset_results_df = results_df[results_df['Dataset'] == dataset_name]\n",
    "    if not dataset_results_df.empty:\n",
    "        results_filename = f\"{dataset_name.replace(' ', '_')}_ml_tfidf_tuned_results.csv\"\n",
    "        results_save_path = os.path.join(config['result_dir'], results_filename)\n",
    "        try:\n",
    "            dataset_results_df.to_csv(results_save_path, index=False, mode='w+')\n",
    "            print(f\"\\nResults for {dataset_name} saved to {results_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving results for {dataset_name} to {results_save_path}: {e}\")\n",
    "\n",
    "# --- Save combined results ---\n",
    "combined_results_path = os.path.join(RESULT_DIR, \"combined_ml_tfidf_tuned_results.csv\")\n",
    "try:\n",
    "    results_df.to_csv(combined_results_path, index=False, mode='w+')\n",
    "    print(f\"\\nCombined results saved to {combined_results_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving combined results to {combined_results_path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
