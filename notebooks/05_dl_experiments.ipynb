{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7965e536",
   "metadata": {},
   "source": [
    "# 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddddf146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    "    \n",
    ")\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# --- Basic Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- Limit CPU Usage ---\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac006e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:01:14,499 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Project Directory Structure ---\n",
    "BASE_DIR = \"..\" \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "\n",
    "MODEL_OUTPUT_BASE_DIR = os.path.join(BASE_DIR, \"models\", \"dl\")\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"result\")\n",
    "\n",
    "# --- Specific Dataset Paths ---\n",
    "BOOK_REVIEW_DATA_DIR = os.path.join(DATA_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_DATA_DIR = os.path.join(DATA_DIR, \"financial_news\")\n",
    "\n",
    "# --- Model/Result Output Dirs (Ensure they exist) ---\n",
    "BOOK_REVIEW_MODEL_DIR = os.path.join(MODEL_OUTPUT_BASE_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_MODEL_DIR = os.path.join(MODEL_OUTPUT_BASE_DIR, \"financial_news\")\n",
    "BOOK_REVIEW_RESULT_DIR = os.path.join(RESULT_DIR, \"book_reviews\")\n",
    "FINANCIAL_NEWS_RESULT_DIR = os.path.join(RESULT_DIR, \"financial_news\")\n",
    "\n",
    "os.makedirs(BOOK_REVIEW_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(BOOK_REVIEW_RESULT_DIR, exist_ok=True)\n",
    "os.makedirs(FINANCIAL_NEWS_RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# --- GloVe Path ---\n",
    "GLOVE_PATH = os.path.join(BASE_DIR, \"data\", \"embeddings\", \"glove.6B.100d.txt\")\n",
    "\n",
    "# --- File Names ---\n",
    "TRAIN_FN = \"train.csv\"\n",
    "VAL_FN = \"val.csv\"\n",
    "TEST_FN = \"test.csv\"\n",
    "\n",
    "# --- Column Names ---\n",
    "TEXT_COLUMN = \"text\"\n",
    "TARGET_COLUMN = \"score\"\n",
    "\n",
    "# Activate GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27800f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:01:14,512 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Model & Training Hyperparameters ---\n",
    "RANDOM_STATE = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Vocabulary params\n",
    "MIN_WORD_FREQ = 3 # Minimum frequency for a word to be included in the vocabulary\n",
    "\n",
    "# Embedding params\n",
    "EMBEDDING_DIM = 100 # Must match GloVe dimension if using pre-trained GloVe\n",
    "LEARNED_EMBEDDING_DIM = 100 # Dimension for embeddings learned from scratch\n",
    "\n",
    "# Model Arch params (can be tuned)\n",
    "HIDDEN_DIM_RNN_LSTM = 64\n",
    "N_LAYERS_RNN_LSTM = 3\n",
    "DROPOUT = 0.5\n",
    "N_FILTERS_CNN = 100\n",
    "FILTER_SIZES_CNN = [3, 4, 5] # Kernel sizes for CNN\n",
    "\n",
    "# Training params\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 30 # Increase for better performance, but takes longer\n",
    "GRADIENT_CLIP = 1.0 # Helps prevent exploding gradients in RNNs/LSTMs\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "METRICS_TO_CALCULATE = [\n",
    "    \"Accuracy\",\n",
    "    \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "    \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "    \"Train Time (Epoch, s)\", \"Eval Time (s)\" # Train time per epoch is more practical for DL\n",
    "]\n",
    "\n",
    "# --- Label Mapping (For PyTorch CrossEntropyLoss) ---\n",
    "LABEL_MAP = {'negative': 0, 'neutral': 1, 'positive': 2} # Example mapping\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# --- Datasets Configuration ---\n",
    "DATASETS_TO_PROCESS = {\n",
    "    \"Book Review\": {\n",
    "        \"train_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(BOOK_REVIEW_DATA_DIR, f'book_reviews_{TEST_FN}'),\n",
    "        \"model_dir\": BOOK_REVIEW_MODEL_DIR,\n",
    "        \"result_dir\": BOOK_REVIEW_RESULT_DIR,\n",
    "        \"vocab_path\": os.path.join(BOOK_REVIEW_MODEL_DIR, \"vocab.pt\"), # Save vocab per dataset\n",
    "    },\n",
    "    \"Financial News\": {\n",
    "        \"train_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TRAIN_FN}'),\n",
    "        \"val_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{VAL_FN}'),\n",
    "        \"test_path\": os.path.join(FINANCIAL_NEWS_DATA_DIR, f'financial_news_{TEST_FN}'),\n",
    "        \"model_dir\": FINANCIAL_NEWS_MODEL_DIR,\n",
    "        \"result_dir\": FINANCIAL_NEWS_RESULT_DIR,\n",
    "         \"vocab_path\": os.path.join(FINANCIAL_NEWS_MODEL_DIR, \"vocab.pt\"),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1ae5b",
   "metadata": {},
   "source": [
    "# 2. Utility Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508fdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Loads data from CSV and handles basic cleaning.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.dropna(subset=[TEXT_COLUMN, TARGET_COLUMN]) # Drop rows with NaNs in critical columns\n",
    "        df[TEXT_COLUMN] = df[TEXT_COLUMN].astype(str) # Ensure text is string\n",
    "        df[TARGET_COLUMN] = df[TARGET_COLUMN].astype(str) # Ensure labels are string before mapping\n",
    "        # Map labels to integers\n",
    "        df[TARGET_COLUMN] = df[TARGET_COLUMN].map(LABEL_MAP)\n",
    "        # Verify mapping worked - check for NaNs introduced if a label wasn't in LABEL_MAP\n",
    "        if df[TARGET_COLUMN].isnull().any():\n",
    "            logging.warning(f\"NaNs found in target column after mapping for {path}. Check LABEL_MAP and data labels.\")\n",
    "            # Option: Drop rows with unmapped labels\n",
    "            original_count = len(df)\n",
    "            df = df.dropna(subset=[TARGET_COLUMN])\n",
    "            logging.warning(f\"Dropped {original_count - len(df)} rows with unmappable labels.\")\n",
    "        df[TARGET_COLUMN] = df[TARGET_COLUMN].astype(int) # Convert to int after mapping\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Simple whitespace tokenizer.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=MIN_WORD_FREQ):\n",
    "    \"\"\"Builds a vocabulary from a list of texts.\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        word_counts.update(tokenize(text))\n",
    "\n",
    "    # Create vocab mapping: word -> index\n",
    "    # Add special tokens: <pad> for padding, <unk> for unknown words\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    idx = 2\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    logging.info(f\"Built vocabulary with {len(vocab)} words (min freq: {min_freq}).\")\n",
    "    return vocab\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for sentiment analysis.\"\"\"\n",
    "    def __init__(self, texts, labels, vocab, max_len=None): # max_len can be added for truncation\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.vocab_stoi = vocab # word -> index\n",
    "        self.vocab_itos = {i: w for w, i in vocab.items()} # index -> word\n",
    "        self.unk_idx = vocab.get(\"<unk>\", 1)\n",
    "        # self.max_len = max_len # Optional: truncate sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = tokenize(text)\n",
    "        # Convert tokens to indices\n",
    "        token_ids = [self.vocab_stoi.get(token, self.unk_idx) for token in tokens]\n",
    "\n",
    "        # Optional Truncation:\n",
    "        # if self.max_len:\n",
    "        #     token_ids = token_ids[:self.max_len]\n",
    "\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Collates data samples into batches with padding.\"\"\"\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(_text, dtype=torch.long)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text)) # Store original lengths\n",
    "\n",
    "    # Pad sequences to the max length in this batch\n",
    "    # batch_first=True means output shape is (batch_size, seq_len)\n",
    "    text_list_padded = pad_sequence(text_list, batch_first=True, padding_value=0) # Use PAD index 0\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long) # Useful for packed sequences later if needed\n",
    "\n",
    "    return text_list_padded, label_list, lengths\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_path, vocab_stoi, embedding_dim):\n",
    "    \"\"\"Loads GloVe embeddings for words in the vocabulary.\"\"\"\n",
    "    if not os.path.exists(glove_path):\n",
    "        logging.error(f\"GloVe file not found at: {glove_path}\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Loading GloVe embeddings from {glove_path}\")\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                try:\n",
    "                    vector = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = vector\n",
    "                except ValueError:\n",
    "                    logging.debug(f\"Skipping line in GloVe file (could not parse vector): {line[:50]}...\")\n",
    "                    continue # Skip lines that might not parse correctly\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading GloVe file: {e}\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Found {len(embeddings_index)} word vectors in GloVe file.\")\n",
    "\n",
    "    vocab_size = len(vocab_stoi)\n",
    "    # Initialize embedding matrix with zeros or small random values\n",
    "    # np.random.seed(RANDOM_STATE)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    # Or random init: embedding_matrix = np.random.rand(vocab_size, embedding_dim) * 0.02 - 0.01\n",
    "\n",
    "    found_count = 0\n",
    "    for word, i in vocab_stoi.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will remain zeros (or random).\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found_count += 1\n",
    "        else:\n",
    "            # Handle <unk> and <pad> specifically\n",
    "            if word == \"<unk>\": # Initialize <unk> token vector (e.g., average or random)\n",
    "                embedding_matrix[i] = np.random.rand(embedding_dim) * 0.02 - 0.01 # Small random\n",
    "                # pass\n",
    "            elif word == \"<pad>\":\n",
    "                embedding_matrix[i] = np.zeros(embedding_dim) # Ensure PAD is zeros\n",
    "\n",
    "    logging.info(f\"Initialized embedding matrix. Shape: {embedding_matrix.shape}\")\n",
    "    logging.info(f\"Found pre-trained vectors for {found_count}/{vocab_size} words in vocabulary.\")\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculates evaluation metrics.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 (Macro)\": f1_macro,\n",
    "        \"Precision (Macro)\": precision_macro,\n",
    "        \"Recall (Macro)\": recall_macro,\n",
    "        \"F1 (Weighted)\": f1_weighted,\n",
    "        \"Precision (Weighted)\": precision_weighted,\n",
    "        \"Recall (Weighted)\": recall_weighted,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd3dc4",
   "metadata": {},
   "source": [
    "# 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65319ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Base Model with Embedding Handling ---\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                pretrained_embeddings,\n",
    "                freeze=freeze_embeddings,\n",
    "                padding_idx=pad_idx\n",
    "            )\n",
    "            logging.info(f\"Using pre-trained embeddings. Freeze: {freeze_embeddings}\")\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "            logging.info(\"Using learned embeddings.\")\n",
    "        self.output_dim = output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58f4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. MLP on Averaged Embeddings ---\n",
    "# Note: This averages embeddings before passing to MLP, simpler than sequence processing.\n",
    "class MLPAveraged(BaseModel):\n",
    "     def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx, hidden_dim1=64, hidden_dim2=32, dropout=DROPOUT, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "         # embedding_dim is input_dim for MLP part\n",
    "         super().__init__(vocab_size, embedding_dim, output_dim, pad_idx, pretrained_embeddings, freeze_embeddings)\n",
    "         self.fc1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "         self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "         self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "         self.dropout = nn.Dropout(dropout)\n",
    "         self.relu = nn.ReLU()\n",
    "\n",
    "     def forward(self, text, text_lengths=None): # text_lengths unused here but kept for consistency\n",
    "         # text shape: (batch_size, seq_len)\n",
    "         embedded = self.embedding(text)\n",
    "         # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "         # Average embeddings across sequence length dimension\n",
    "         # Need to handle padding: Mask out pad tokens before averaging\n",
    "         pad_mask = (text != self.embedding.padding_idx).float().unsqueeze(-1) # (batch_size, seq_len, 1)\n",
    "         embedded = embedded * pad_mask # Zero out embeddings for pad tokens\n",
    "         # Sum embeddings and divide by actual lengths (excluding pad tokens)\n",
    "         # Calculate actual lengths (sum of non-pad tokens)\n",
    "         actual_lengths = pad_mask.sum(dim=1)\n",
    "         actual_lengths = torch.max(actual_lengths, torch.ones_like(actual_lengths)) # Avoid division by zero for empty sequences\n",
    "\n",
    "         pooled = embedded.sum(dim=1) / actual_lengths # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "         x = self.dropout(self.relu(self.fc1(pooled)))\n",
    "         x = self.dropout(self.relu(self.fc2(x)))\n",
    "         output = self.fc3(x) # Shape: (batch_size, output_dim)\n",
    "         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53c8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Basic RNN ---\n",
    "class RNNModel(BaseModel):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx, bidirectional=False, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__(vocab_size, embedding_dim, output_dim, pad_idx, pretrained_embeddings, freeze_embeddings)\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers=n_layers,\n",
    "                          bidirectional=bidirectional,\n",
    "                          batch_first=True, # Input shape: (batch_size, seq_len, embed_dim)\n",
    "                          dropout=dropout if n_layers > 1 else 0) # Dropout only between layers\n",
    "        # Adjust linear layer input size for bidirectional\n",
    "        fc_in_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_in_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths): # text_lengths useful for PackedSequence but not used here\n",
    "        # text shape: (batch_size, seq_len)\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # No packing used here for simplicity, RNN processes padded sequences\n",
    "        # Output shape: (batch_size, seq_len, num_directions * hidden_dim)\n",
    "        # Hidden shape: (n_layers * num_directions, batch_size, hidden_dim)\n",
    "        rnn_output, hidden = self.rnn(embedded)\n",
    "\n",
    "        # Get output from the last time step (or concatenate final forward/backward hidden states)\n",
    "        # hidden[-1] is the hidden state of the last layer (forward)\n",
    "        # hidden[-2] would be the last backward state if bidirectional\n",
    "        if self.rnn.bidirectional:\n",
    "            # Concatenate the final hidden states of the last layer from both directions\n",
    "            # hidden shape: (n_layers * 2, batch, hidden_dim)\n",
    "            # hidden[-2,:,:] is last layer's forward, hidden[-1,:,:] is last layer's backward\n",
    "            hidden_fwd = hidden[-2,:,:]\n",
    "            hidden_bwd = hidden[-1,:,:]\n",
    "            hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)\n",
    "        else:\n",
    "            # hidden shape: (n_layers * 1, batch, hidden_dim)\n",
    "            hidden_cat = hidden[-1,:,:]\n",
    "\n",
    "        # Apply dropout and final linear layer\n",
    "        output = self.fc(self.dropout(hidden_cat)) # Shape: (batch_size, output_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LSTM Model ---\n",
    "class LSTMModel(BaseModel):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx, bidirectional=True, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__(vocab_size, embedding_dim, output_dim, pad_idx, pretrained_embeddings, freeze_embeddings)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if n_layers > 1 else 0)\n",
    "        fc_in_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_in_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths): # text_lengths can be used with pack_padded_sequence\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # Optional: Use packed sequences for efficiency (handles padding)\n",
    "        # packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Using padded sequence directly (simpler for this example):\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # hidden shape: (n_layers * num_directions, batch_size, hidden_dim)\n",
    "        # cell shape: (n_layers * num_directions, batch_size, hidden_dim)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden_fwd = hidden[-2,:,:]\n",
    "            hidden_bwd = hidden[-1,:,:]\n",
    "            hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1,:,:]\n",
    "\n",
    "        output = self.fc(self.dropout(hidden_cat))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05c2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. CNN Model (1D Convolution) ---\n",
    "class CNNModel(BaseModel):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__(vocab_size, embedding_dim, output_dim, pad_idx, pretrained_embeddings, freeze_embeddings)\n",
    "        # Create multiple convolutional layers with different kernel sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=n_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        # The output dimension after concatenating pooled features from all kernel sizes\n",
    "        fc_in_dim = len(filter_sizes) * n_filters\n",
    "        self.fc = nn.Linear(fc_in_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        # text: [batch size, seq len]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded: [batch size, seq len, emb dim]\n",
    "\n",
    "        # Conv1d expects input shape: (batch_size, channels, seq_len)\n",
    "        # So, permute dimensions: (batch_size, emb dim, seq len)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "\n",
    "        # Apply convolutions and pooling\n",
    "        conved = [self.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conved[n]: [batch size, n filters, seq len - filter_sizes[n] + 1]\n",
    "\n",
    "        # Apply max pooling over time (sequence length dimension)\n",
    "        # Pool size should cover the entire sequence length dimension after convolution\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled[n]: [batch size, n filters]\n",
    "\n",
    "        # Concatenate the pooled features from different filter sizes\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat: [batch size, n filters * len(filter_sizes)]\n",
    "\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4f5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. CNN-LSTM Hybrid Model ---\n",
    "class CNNLSTMModel(BaseModel):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_size_cnn, # Single filter size for simplicity here\n",
    "                 hidden_dim_lstm, output_dim, n_layers_lstm, dropout, pad_idx,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__(vocab_size, embedding_dim, output_dim, pad_idx, pretrained_embeddings, freeze_embeddings)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=filter_size_cnn)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Input to LSTM is the output channels of CNN\n",
    "        self.lstm = nn.LSTM(n_filters, # Input features = CNN output channels\n",
    "                            hidden_dim_lstm,\n",
    "                            num_layers=n_layers_lstm,\n",
    "                            bidirectional=True, # Often good to use bidirectional\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if n_layers_lstm > 1 else 0)\n",
    "        fc_in_dim = hidden_dim_lstm * 2 # Bidirectional LSTM\n",
    "        self.fc = nn.Linear(fc_in_dim, output_dim)\n",
    "        self.dropout_embed = nn.Dropout(dropout)\n",
    "        self.dropout_final = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths=None):\n",
    "        # text: [batch size, seq len]\n",
    "        embedded = self.dropout_embed(self.embedding(text))\n",
    "        # embedded: [batch size, seq len, emb dim]\n",
    "\n",
    "        # --- CNN Part ---\n",
    "        # Permute for Conv1d: [batch size, emb dim, seq len]\n",
    "        embedded_permuted = embedded.permute(0, 2, 1)\n",
    "        conved = self.relu(self.conv(embedded_permuted))\n",
    "        # conved: [batch size, n filters, new seq len]\n",
    "        # Permute back for LSTM: [batch size, new seq len, n filters]\n",
    "        conved_permuted = conved.permute(0, 2, 1)\n",
    "\n",
    "        # --- LSTM Part ---\n",
    "        lstm_output, (hidden, cell) = self.lstm(conved_permuted)\n",
    "        # lstm_output: [batch size, seq len, num directions * hidden dim]\n",
    "        # hidden: [n layers * num directions, batch size, hidden dim]\n",
    "\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        hidden_fwd = hidden[-2,:,:]\n",
    "        hidden_bwd = hidden[-1,:,:]\n",
    "        hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)\n",
    "\n",
    "        # --- Final Output ---\n",
    "        output = self.fc(self.dropout_final(hidden_cat))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d3c29",
   "metadata": {},
   "source": [
    "# 4. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e9cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, device, grad_clip=None):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (text, labels, lengths) in enumerate(iterator):\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        lengths = lengths.to('cpu') # lengths for pack_padded_sequence must be on CPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(text, lengths) # Pass lengths if model uses them\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        if grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Optional: Print batch progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            logging.debug(f\"Batch {batch_idx}/{len(iterator)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    train_time_epoch = end_time - start_time\n",
    "    return epoch_loss / len(iterator), train_time_epoch\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \"\"\"Evaluates the model on a given dataset iterator.\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (text, labels, lengths) in enumerate(iterator):\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            lengths = lengths.to('cpu')\n",
    "\n",
    "            predictions = model(text, lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Get predicted labels\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    end_time = time.time()\n",
    "    eval_time = end_time - start_time\n",
    "    metrics = calculate_metrics(all_labels, all_preds)\n",
    "    avg_loss = epoch_loss / len(iterator)\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, metrics, eval_time, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5381328",
   "metadata": {},
   "source": [
    "# 5. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f18912",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba71c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:01:14,636 - INFO - Processing Dataset: Book Review\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Processing Dataset: Book Review ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:01:16,627 - INFO - Loaded existing vocabulary from ..\\models\\dl\\book_reviews\\vocab.pt\n",
      "2025-05-01 10:01:16,651 - INFO - Loading GloVe embeddings from ..\\data\\embeddings\\glove.6B.100d.txt\n",
      "2025-05-01 10:01:22,456 - INFO - Found 400000 word vectors in GloVe file.\n",
      "2025-05-01 10:01:22,552 - INFO - Initialized embedding matrix. Shape: (86590, 100)\n",
      "2025-05-01 10:01:22,553 - INFO - Found pre-trained vectors for 72856/86590 words in vocabulary.\n",
      "2025-05-01 10:01:22,625 - INFO - Starting training for MLP (Avg Learned Emb) on Book Review\n",
      "2025-05-01 10:01:22,659 - INFO - Using learned embeddings.\n",
      "2025-05-01 10:01:22,737 - INFO - Model: MLP (Avg Learned Emb), Trainable Parameters: 8,667,643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: MLP (Avg Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:01:57,224 - INFO - Epoch: 01 | Time: 0m 34s\n",
      "2025-05-01 10:01:57,225 - INFO - \tTrain Loss: 0.496\n",
      "2025-05-01 10:01:57,225 - INFO - \t Val. Loss: 0.421 | Val. F1 (Macro): 0.5111\n",
      "2025-05-01 10:01:57,304 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 10:02:31,496 - INFO - Epoch: 02 | Time: 0m 34s\n",
      "2025-05-01 10:02:31,497 - INFO - \tTrain Loss: 0.404\n",
      "2025-05-01 10:02:31,498 - INFO - \t Val. Loss: 0.404 | Val. F1 (Macro): 0.5195\n",
      "2025-05-01 10:02:31,561 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 10:03:04,637 - INFO - Epoch: 03 | Time: 0m 33s\n",
      "2025-05-01 10:03:04,638 - INFO - \tTrain Loss: 0.372\n",
      "2025-05-01 10:03:04,638 - INFO - \t Val. Loss: 0.397 | Val. F1 (Macro): 0.5560\n",
      "2025-05-01 10:03:04,693 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 10:03:38,201 - INFO - Epoch: 04 | Time: 0m 34s\n",
      "2025-05-01 10:03:38,202 - INFO - \tTrain Loss: 0.346\n",
      "2025-05-01 10:03:38,202 - INFO - \t Val. Loss: 0.398 | Val. F1 (Macro): 0.5868\n",
      "2025-05-01 10:04:10,810 - INFO - Epoch: 05 | Time: 0m 33s\n",
      "2025-05-01 10:04:10,811 - INFO - \tTrain Loss: 0.326\n",
      "2025-05-01 10:04:10,812 - INFO - \t Val. Loss: 0.410 | Val. F1 (Macro): 0.6040\n",
      "2025-05-01 10:04:43,675 - INFO - Epoch: 06 | Time: 0m 33s\n",
      "2025-05-01 10:04:43,677 - INFO - \tTrain Loss: 0.309\n",
      "2025-05-01 10:04:43,679 - INFO - \t Val. Loss: 0.423 | Val. F1 (Macro): 0.6173\n",
      "2025-05-01 10:05:16,990 - INFO - Epoch: 07 | Time: 0m 33s\n",
      "2025-05-01 10:05:16,991 - INFO - \tTrain Loss: 0.294\n",
      "2025-05-01 10:05:16,992 - INFO - \t Val. Loss: 0.435 | Val. F1 (Macro): 0.6242\n",
      "2025-05-01 10:05:50,168 - INFO - Epoch: 08 | Time: 0m 33s\n",
      "2025-05-01 10:05:50,168 - INFO - \tTrain Loss: 0.279\n",
      "2025-05-01 10:05:50,169 - INFO - \t Val. Loss: 0.449 | Val. F1 (Macro): 0.6222\n",
      "2025-05-01 10:06:25,413 - INFO - Epoch: 09 | Time: 0m 35s\n",
      "2025-05-01 10:06:25,415 - INFO - \tTrain Loss: 0.268\n",
      "2025-05-01 10:06:25,415 - INFO - \t Val. Loss: 0.468 | Val. F1 (Macro): 0.6330\n",
      "2025-05-01 10:06:59,917 - INFO - Epoch: 10 | Time: 0m 35s\n",
      "2025-05-01 10:06:59,919 - INFO - \tTrain Loss: 0.256\n",
      "2025-05-01 10:06:59,920 - INFO - \t Val. Loss: 0.496 | Val. F1 (Macro): 0.6328\n",
      "2025-05-01 10:07:32,671 - INFO - Epoch: 11 | Time: 0m 33s\n",
      "2025-05-01 10:07:32,672 - INFO - \tTrain Loss: 0.244\n",
      "2025-05-01 10:07:32,672 - INFO - \t Val. Loss: 0.520 | Val. F1 (Macro): 0.6216\n",
      "2025-05-01 10:08:04,908 - INFO - Epoch: 12 | Time: 0m 32s\n",
      "2025-05-01 10:08:04,909 - INFO - \tTrain Loss: 0.235\n",
      "2025-05-01 10:08:04,910 - INFO - \t Val. Loss: 0.541 | Val. F1 (Macro): 0.6272\n",
      "2025-05-01 10:08:37,361 - INFO - Epoch: 13 | Time: 0m 32s\n",
      "2025-05-01 10:08:37,362 - INFO - \tTrain Loss: 0.224\n",
      "2025-05-01 10:08:37,362 - INFO - \t Val. Loss: 0.552 | Val. F1 (Macro): 0.6259\n",
      "2025-05-01 10:09:10,008 - INFO - Epoch: 14 | Time: 0m 33s\n",
      "2025-05-01 10:09:10,009 - INFO - \tTrain Loss: 0.214\n",
      "2025-05-01 10:09:10,009 - INFO - \t Val. Loss: 0.578 | Val. F1 (Macro): 0.6286\n",
      "2025-05-01 10:09:42,381 - INFO - Epoch: 15 | Time: 0m 32s\n",
      "2025-05-01 10:09:42,382 - INFO - \tTrain Loss: 0.205\n",
      "2025-05-01 10:09:42,382 - INFO - \t Val. Loss: 0.618 | Val. F1 (Macro): 0.6241\n",
      "2025-05-01 10:10:15,494 - INFO - Epoch: 16 | Time: 0m 33s\n",
      "2025-05-01 10:10:15,495 - INFO - \tTrain Loss: 0.196\n",
      "2025-05-01 10:10:15,496 - INFO - \t Val. Loss: 0.638 | Val. F1 (Macro): 0.6186\n",
      "2025-05-01 10:10:48,069 - INFO - Epoch: 17 | Time: 0m 33s\n",
      "2025-05-01 10:10:48,070 - INFO - \tTrain Loss: 0.189\n",
      "2025-05-01 10:10:48,071 - INFO - \t Val. Loss: 0.663 | Val. F1 (Macro): 0.6193\n",
      "2025-05-01 10:11:21,011 - INFO - Epoch: 18 | Time: 0m 33s\n",
      "2025-05-01 10:11:21,013 - INFO - \tTrain Loss: 0.180\n",
      "2025-05-01 10:11:21,013 - INFO - \t Val. Loss: 0.706 | Val. F1 (Macro): 0.6184\n",
      "2025-05-01 10:11:54,039 - INFO - Epoch: 19 | Time: 0m 33s\n",
      "2025-05-01 10:11:54,040 - INFO - \tTrain Loss: 0.172\n",
      "2025-05-01 10:11:54,040 - INFO - \t Val. Loss: 0.773 | Val. F1 (Macro): 0.6149\n",
      "2025-05-01 10:12:26,660 - INFO - Epoch: 20 | Time: 0m 33s\n",
      "2025-05-01 10:12:26,662 - INFO - \tTrain Loss: 0.166\n",
      "2025-05-01 10:12:26,662 - INFO - \t Val. Loss: 0.786 | Val. F1 (Macro): 0.6112\n",
      "2025-05-01 10:12:59,049 - INFO - Epoch: 21 | Time: 0m 32s\n",
      "2025-05-01 10:12:59,050 - INFO - \tTrain Loss: 0.159\n",
      "2025-05-01 10:12:59,051 - INFO - \t Val. Loss: 0.876 | Val. F1 (Macro): 0.6133\n",
      "2025-05-01 10:13:31,048 - INFO - Epoch: 22 | Time: 0m 32s\n",
      "2025-05-01 10:13:31,049 - INFO - \tTrain Loss: 0.154\n",
      "2025-05-01 10:13:31,049 - INFO - \t Val. Loss: 0.896 | Val. F1 (Macro): 0.6085\n",
      "2025-05-01 10:14:03,172 - INFO - Epoch: 23 | Time: 0m 32s\n",
      "2025-05-01 10:14:03,173 - INFO - \tTrain Loss: 0.146\n",
      "2025-05-01 10:14:03,173 - INFO - \t Val. Loss: 0.983 | Val. F1 (Macro): 0.6096\n",
      "2025-05-01 10:14:35,890 - INFO - Epoch: 24 | Time: 0m 33s\n",
      "2025-05-01 10:14:35,890 - INFO - \tTrain Loss: 0.142\n",
      "2025-05-01 10:14:35,891 - INFO - \t Val. Loss: 0.992 | Val. F1 (Macro): 0.6092\n",
      "2025-05-01 10:15:08,871 - INFO - Epoch: 25 | Time: 0m 33s\n",
      "2025-05-01 10:15:08,872 - INFO - \tTrain Loss: 0.135\n",
      "2025-05-01 10:15:08,873 - INFO - \t Val. Loss: 1.036 | Val. F1 (Macro): 0.6092\n",
      "2025-05-01 10:15:41,766 - INFO - Epoch: 26 | Time: 0m 33s\n",
      "2025-05-01 10:15:41,767 - INFO - \tTrain Loss: 0.131\n",
      "2025-05-01 10:15:41,767 - INFO - \t Val. Loss: 1.086 | Val. F1 (Macro): 0.6087\n",
      "2025-05-01 10:16:14,398 - INFO - Epoch: 27 | Time: 0m 33s\n",
      "2025-05-01 10:16:14,399 - INFO - \tTrain Loss: 0.127\n",
      "2025-05-01 10:16:14,400 - INFO - \t Val. Loss: 1.151 | Val. F1 (Macro): 0.6085\n",
      "2025-05-01 10:16:47,157 - INFO - Epoch: 28 | Time: 0m 33s\n",
      "2025-05-01 10:16:47,159 - INFO - \tTrain Loss: 0.122\n",
      "2025-05-01 10:16:47,160 - INFO - \t Val. Loss: 1.222 | Val. F1 (Macro): 0.6030\n",
      "2025-05-01 10:17:21,023 - INFO - Epoch: 29 | Time: 0m 34s\n",
      "2025-05-01 10:17:21,024 - INFO - \tTrain Loss: 0.118\n",
      "2025-05-01 10:17:21,025 - INFO - \t Val. Loss: 1.320 | Val. F1 (Macro): 0.6049\n",
      "2025-05-01 10:17:52,962 - INFO - Epoch: 30 | Time: 0m 32s\n",
      "2025-05-01 10:17:52,962 - INFO - \tTrain Loss: 0.114\n",
      "2025-05-01 10:17:52,963 - INFO - \t Val. Loss: 1.376 | Val. F1 (Macro): 0.6067\n",
      "2025-05-01 10:17:52,999 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 10:17:56,339 - INFO - Test Set Performance:\n",
      "2025-05-01 10:17:56,340 - INFO - \tAccuracy: 0.8567\n",
      "2025-05-01 10:17:56,340 - INFO - \tF1 (Macro): 0.5505\n",
      "2025-05-01 10:17:56,341 - INFO - \tPrecision (Macro): 0.6616\n",
      "2025-05-01 10:17:56,343 - INFO - \tRecall (Macro): 0.5465\n",
      "2025-05-01 10:17:56,343 - INFO - \tF1 (Weighted): 0.8226\n",
      "2025-05-01 10:17:56,344 - INFO - \tPrecision (Weighted): 0.8193\n",
      "2025-05-01 10:17:56,345 - INFO - \tRecall (Weighted): 0.8567\n",
      "2025-05-01 10:17:56,345 - INFO - \tTest Loss: 0.399\n",
      "2025-05-01 10:17:56,346 - INFO - \tEval Time: 3.172s\n",
      "2025-05-01 10:17:56,350 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_MLP_(Avg_Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 10:17:56,495 - INFO - Starting training for RNN (Learned Emb) on Book Review\n",
      "2025-05-01 10:17:56,530 - INFO - Using learned embeddings.\n",
      "2025-05-01 10:17:56,551 - INFO - Model: RNN (Learned Emb), Trainable Parameters: 8,686,459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: RNN (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:18:53,157 - INFO - Epoch: 01 | Time: 0m 57s\n",
      "2025-05-01 10:18:53,158 - INFO - \tTrain Loss: 0.649\n",
      "2025-05-01 10:18:53,158 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:18:53,213 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_RNN_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 10:19:48,200 - INFO - Epoch: 02 | Time: 0m 55s\n",
      "2025-05-01 10:19:48,201 - INFO - \tTrain Loss: 0.647\n",
      "2025-05-01 10:19:48,202 - INFO - \t Val. Loss: 0.644 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:20:42,546 - INFO - Epoch: 03 | Time: 0m 54s\n",
      "2025-05-01 10:20:42,547 - INFO - \tTrain Loss: 0.645\n",
      "2025-05-01 10:20:42,548 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:20:42,599 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_RNN_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 10:21:36,912 - INFO - Epoch: 04 | Time: 0m 54s\n",
      "2025-05-01 10:21:36,913 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:21:36,913 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:22:34,464 - INFO - Epoch: 05 | Time: 0m 58s\n",
      "2025-05-01 10:22:34,464 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:22:34,465 - INFO - \t Val. Loss: 0.644 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:23:29,598 - INFO - Epoch: 06 | Time: 0m 55s\n",
      "2025-05-01 10:23:29,599 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:23:29,600 - INFO - \t Val. Loss: 0.645 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:24:24,281 - INFO - Epoch: 07 | Time: 0m 55s\n",
      "2025-05-01 10:24:24,282 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:24:24,282 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:25:19,148 - INFO - Epoch: 08 | Time: 0m 55s\n",
      "2025-05-01 10:25:19,149 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:25:19,149 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:26:13,297 - INFO - Epoch: 09 | Time: 0m 54s\n",
      "2025-05-01 10:26:13,298 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:26:13,298 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:26:13,354 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_RNN_(Learned_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 10:27:07,685 - INFO - Epoch: 10 | Time: 0m 54s\n",
      "2025-05-01 10:27:07,686 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:27:07,687 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:28:01,907 - INFO - Epoch: 11 | Time: 0m 54s\n",
      "2025-05-01 10:28:01,908 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:28:01,909 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:28:56,579 - INFO - Epoch: 12 | Time: 0m 55s\n",
      "2025-05-01 10:28:56,580 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:28:56,581 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:29:51,059 - INFO - Epoch: 13 | Time: 0m 54s\n",
      "2025-05-01 10:29:51,060 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:29:51,061 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:30:45,611 - INFO - Epoch: 14 | Time: 0m 55s\n",
      "2025-05-01 10:30:45,611 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:30:45,612 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:31:39,730 - INFO - Epoch: 15 | Time: 0m 54s\n",
      "2025-05-01 10:31:39,731 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:31:39,731 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:32:33,878 - INFO - Epoch: 16 | Time: 0m 54s\n",
      "2025-05-01 10:32:33,879 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:32:33,880 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:33:28,114 - INFO - Epoch: 17 | Time: 0m 54s\n",
      "2025-05-01 10:33:28,114 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:33:28,115 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:34:22,245 - INFO - Epoch: 18 | Time: 0m 54s\n",
      "2025-05-01 10:34:22,246 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:34:22,247 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:35:16,807 - INFO - Epoch: 19 | Time: 0m 55s\n",
      "2025-05-01 10:35:16,808 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:35:16,808 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:36:11,491 - INFO - Epoch: 20 | Time: 0m 55s\n",
      "2025-05-01 10:36:11,492 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:36:11,493 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:37:06,322 - INFO - Epoch: 21 | Time: 0m 55s\n",
      "2025-05-01 10:37:06,323 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:37:06,324 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:38:00,579 - INFO - Epoch: 22 | Time: 0m 54s\n",
      "2025-05-01 10:38:00,579 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:38:00,580 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:38:54,815 - INFO - Epoch: 23 | Time: 0m 54s\n",
      "2025-05-01 10:38:54,816 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:38:54,817 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:39:48,954 - INFO - Epoch: 24 | Time: 0m 54s\n",
      "2025-05-01 10:39:48,955 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:39:48,955 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:40:43,416 - INFO - Epoch: 25 | Time: 0m 54s\n",
      "2025-05-01 10:40:43,418 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:40:43,419 - INFO - \t Val. Loss: 0.645 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:41:38,250 - INFO - Epoch: 26 | Time: 0m 55s\n",
      "2025-05-01 10:41:38,251 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:41:38,251 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:42:32,581 - INFO - Epoch: 27 | Time: 0m 54s\n",
      "2025-05-01 10:42:32,581 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:42:32,581 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:43:26,838 - INFO - Epoch: 28 | Time: 0m 54s\n",
      "2025-05-01 10:43:26,839 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:43:26,840 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:44:21,120 - INFO - Epoch: 29 | Time: 0m 54s\n",
      "2025-05-01 10:44:21,120 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:44:21,121 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:45:15,820 - INFO - Epoch: 30 | Time: 0m 55s\n",
      "2025-05-01 10:45:15,820 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 10:45:15,821 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:45:15,857 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_RNN_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 10:45:19,805 - INFO - Test Set Performance:\n",
      "2025-05-01 10:45:19,806 - INFO - \tAccuracy: 0.7976\n",
      "2025-05-01 10:45:19,806 - INFO - \tF1 (Macro): 0.2958\n",
      "2025-05-01 10:45:19,806 - INFO - \tPrecision (Macro): 0.2659\n",
      "2025-05-01 10:45:19,807 - INFO - \tRecall (Macro): 0.3333\n",
      "2025-05-01 10:45:19,808 - INFO - \tF1 (Weighted): 0.7079\n",
      "2025-05-01 10:45:19,808 - INFO - \tPrecision (Weighted): 0.6362\n",
      "2025-05-01 10:45:19,808 - INFO - \tRecall (Weighted): 0.7976\n",
      "2025-05-01 10:45:19,809 - INFO - \tTest Loss: 0.641\n",
      "2025-05-01 10:45:19,810 - INFO - \tEval Time: 3.821s\n",
      "2025-05-01 10:45:19,813 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_RNN_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 10:45:19,962 - INFO - Starting training for LSTM (Learned Emb) on Book Review\n",
      "2025-05-01 10:45:19,998 - INFO - Using learned embeddings.\n",
      "2025-05-01 10:45:20,008 - INFO - Model: LSTM (Learned Emb), Trainable Parameters: 8,768,251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: LSTM (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 10:46:40,318 - INFO - Epoch: 01 | Time: 1m 20s\n",
      "2025-05-01 10:46:40,319 - INFO - \tTrain Loss: 0.650\n",
      "2025-05-01 10:46:40,320 - INFO - \t Val. Loss: 0.644 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:46:40,380 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 10:48:00,132 - INFO - Epoch: 02 | Time: 1m 20s\n",
      "2025-05-01 10:48:00,133 - INFO - \tTrain Loss: 0.645\n",
      "2025-05-01 10:48:00,133 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:48:00,191 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 10:49:20,146 - INFO - Epoch: 03 | Time: 1m 20s\n",
      "2025-05-01 10:49:20,147 - INFO - \tTrain Loss: 0.643\n",
      "2025-05-01 10:49:20,148 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:49:20,203 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 10:50:39,824 - INFO - Epoch: 04 | Time: 1m 20s\n",
      "2025-05-01 10:50:39,825 - INFO - \tTrain Loss: 0.642\n",
      "2025-05-01 10:50:39,825 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:51:58,939 - INFO - Epoch: 05 | Time: 1m 19s\n",
      "2025-05-01 10:51:58,940 - INFO - \tTrain Loss: 0.642\n",
      "2025-05-01 10:51:58,940 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:51:58,993 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 10:53:20,057 - INFO - Epoch: 06 | Time: 1m 21s\n",
      "2025-05-01 10:53:20,058 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 10:53:20,058 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:53:20,127 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 10:54:40,559 - INFO - Epoch: 07 | Time: 1m 20s\n",
      "2025-05-01 10:54:40,560 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 10:54:40,561 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2962\n",
      "2025-05-01 10:54:40,612 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 10:56:01,602 - INFO - Epoch: 08 | Time: 1m 21s\n",
      "2025-05-01 10:56:01,603 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 10:56:01,604 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 10:57:22,419 - INFO - Epoch: 09 | Time: 1m 21s\n",
      "2025-05-01 10:57:22,420 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 10:57:22,420 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2964\n",
      "2025-05-01 10:58:43,239 - INFO - Epoch: 10 | Time: 1m 21s\n",
      "2025-05-01 10:58:43,239 - INFO - \tTrain Loss: 0.640\n",
      "2025-05-01 10:58:43,240 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2964\n",
      "2025-05-01 10:58:43,301 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 10)\n",
      "2025-05-01 11:00:04,635 - INFO - Epoch: 11 | Time: 1m 21s\n",
      "2025-05-01 11:00:04,636 - INFO - \tTrain Loss: 0.640\n",
      "2025-05-01 11:00:04,636 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2964\n",
      "2025-05-01 11:01:25,870 - INFO - Epoch: 12 | Time: 1m 21s\n",
      "2025-05-01 11:01:25,871 - INFO - \tTrain Loss: 0.640\n",
      "2025-05-01 11:01:25,871 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2968\n",
      "2025-05-01 11:01:25,927 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 11:02:46,256 - INFO - Epoch: 13 | Time: 1m 20s\n",
      "2025-05-01 11:02:46,257 - INFO - \tTrain Loss: 0.640\n",
      "2025-05-01 11:02:46,258 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2973\n",
      "2025-05-01 11:04:06,708 - INFO - Epoch: 14 | Time: 1m 20s\n",
      "2025-05-01 11:04:06,710 - INFO - \tTrain Loss: 0.639\n",
      "2025-05-01 11:04:06,712 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2971\n",
      "2025-05-01 11:05:27,478 - INFO - Epoch: 15 | Time: 1m 21s\n",
      "2025-05-01 11:05:27,479 - INFO - \tTrain Loss: 0.635\n",
      "2025-05-01 11:05:27,479 - INFO - \t Val. Loss: 0.614 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 11:05:27,542 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 11:06:49,121 - INFO - Epoch: 16 | Time: 1m 22s\n",
      "2025-05-01 11:06:49,122 - INFO - \tTrain Loss: 0.616\n",
      "2025-05-01 11:06:49,124 - INFO - \t Val. Loss: 0.616 | Val. F1 (Macro): 0.2974\n",
      "2025-05-01 11:08:24,571 - INFO - Epoch: 17 | Time: 1m 35s\n",
      "2025-05-01 11:08:24,572 - INFO - \tTrain Loss: 0.621\n",
      "2025-05-01 11:08:24,572 - INFO - \t Val. Loss: 0.624 | Val. F1 (Macro): 0.2965\n",
      "2025-05-01 11:10:07,273 - INFO - Epoch: 18 | Time: 1m 43s\n",
      "2025-05-01 11:10:07,274 - INFO - \tTrain Loss: 0.615\n",
      "2025-05-01 11:10:07,275 - INFO - \t Val. Loss: 0.623 | Val. F1 (Macro): 0.2980\n",
      "2025-05-01 11:11:49,553 - INFO - Epoch: 19 | Time: 1m 42s\n",
      "2025-05-01 11:11:49,554 - INFO - \tTrain Loss: 0.583\n",
      "2025-05-01 11:11:49,554 - INFO - \t Val. Loss: 0.596 | Val. F1 (Macro): 0.3457\n",
      "2025-05-01 11:11:49,612 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 11:13:32,317 - INFO - Epoch: 20 | Time: 1m 43s\n",
      "2025-05-01 11:13:32,318 - INFO - \tTrain Loss: 0.538\n",
      "2025-05-01 11:13:32,319 - INFO - \t Val. Loss: 0.571 | Val. F1 (Macro): 0.3984\n",
      "2025-05-01 11:13:32,374 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 20)\n",
      "2025-05-01 11:15:17,033 - INFO - Epoch: 21 | Time: 1m 45s\n",
      "2025-05-01 11:15:17,034 - INFO - \tTrain Loss: 0.504\n",
      "2025-05-01 11:15:17,034 - INFO - \t Val. Loss: 0.533 | Val. F1 (Macro): 0.4456\n",
      "2025-05-01 11:15:17,131 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 21)\n",
      "2025-05-01 11:17:09,564 - INFO - Epoch: 22 | Time: 1m 52s\n",
      "2025-05-01 11:17:09,565 - INFO - \tTrain Loss: 0.474\n",
      "2025-05-01 11:17:09,566 - INFO - \t Val. Loss: 0.516 | Val. F1 (Macro): 0.4807\n",
      "2025-05-01 11:17:09,624 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 22)\n",
      "2025-05-01 11:18:52,901 - INFO - Epoch: 23 | Time: 1m 43s\n",
      "2025-05-01 11:18:52,902 - INFO - \tTrain Loss: 0.453\n",
      "2025-05-01 11:18:52,902 - INFO - \t Val. Loss: 0.519 | Val. F1 (Macro): 0.4743\n",
      "2025-05-01 11:20:37,459 - INFO - Epoch: 24 | Time: 1m 45s\n",
      "2025-05-01 11:20:37,460 - INFO - \tTrain Loss: 0.433\n",
      "2025-05-01 11:20:37,460 - INFO - \t Val. Loss: 0.482 | Val. F1 (Macro): 0.4975\n",
      "2025-05-01 11:20:37,514 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 24)\n",
      "2025-05-01 11:22:22,063 - INFO - Epoch: 25 | Time: 1m 45s\n",
      "2025-05-01 11:22:22,064 - INFO - \tTrain Loss: 0.418\n",
      "2025-05-01 11:22:22,064 - INFO - \t Val. Loss: 0.479 | Val. F1 (Macro): 0.5055\n",
      "2025-05-01 11:22:22,122 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 25)\n",
      "2025-05-01 11:24:03,250 - INFO - Epoch: 26 | Time: 1m 41s\n",
      "2025-05-01 11:24:03,251 - INFO - \tTrain Loss: 0.405\n",
      "2025-05-01 11:24:03,251 - INFO - \t Val. Loss: 0.539 | Val. F1 (Macro): 0.5017\n",
      "2025-05-01 11:25:50,925 - INFO - Epoch: 27 | Time: 1m 48s\n",
      "2025-05-01 11:25:50,926 - INFO - \tTrain Loss: 0.393\n",
      "2025-05-01 11:25:50,927 - INFO - \t Val. Loss: 0.506 | Val. F1 (Macro): 0.5011\n",
      "2025-05-01 11:27:39,118 - INFO - Epoch: 28 | Time: 1m 48s\n",
      "2025-05-01 11:27:39,119 - INFO - \tTrain Loss: 0.384\n",
      "2025-05-01 11:27:39,120 - INFO - \t Val. Loss: 0.480 | Val. F1 (Macro): 0.5143\n",
      "2025-05-01 11:29:28,039 - INFO - Epoch: 29 | Time: 1m 49s\n",
      "2025-05-01 11:29:28,040 - INFO - \tTrain Loss: 0.373\n",
      "2025-05-01 11:29:28,041 - INFO - \t Val. Loss: 0.452 | Val. F1 (Macro): 0.5222\n",
      "2025-05-01 11:29:28,108 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 29)\n",
      "2025-05-01 11:31:15,041 - INFO - Epoch: 30 | Time: 1m 47s\n",
      "2025-05-01 11:31:15,042 - INFO - \tTrain Loss: 0.367\n",
      "2025-05-01 11:31:15,043 - INFO - \t Val. Loss: 0.432 | Val. F1 (Macro): 0.5320\n",
      "2025-05-01 11:31:15,109 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt (Epoch 30)\n",
      "2025-05-01 11:31:15,157 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 11:31:26,901 - INFO - Test Set Performance:\n",
      "2025-05-01 11:31:26,901 - INFO - \tAccuracy: 0.8615\n",
      "2025-05-01 11:31:26,901 - INFO - \tF1 (Macro): 0.5323\n",
      "2025-05-01 11:31:26,902 - INFO - \tPrecision (Macro): 0.6266\n",
      "2025-05-01 11:31:26,902 - INFO - \tRecall (Macro): 0.5367\n",
      "2025-05-01 11:31:26,902 - INFO - \tF1 (Weighted): 0.8205\n",
      "2025-05-01 11:31:26,903 - INFO - \tPrecision (Weighted): 0.8094\n",
      "2025-05-01 11:31:26,903 - INFO - \tRecall (Weighted): 0.8615\n",
      "2025-05-01 11:31:26,904 - INFO - \tTest Loss: 0.432\n",
      "2025-05-01 11:31:26,904 - INFO - \tEval Time: 11.599s\n",
      "2025-05-01 11:31:26,907 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_LSTM_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 11:31:35,517 - INFO - Starting training for BiLSTM (Learned Emb) on Book Review\n",
      "2025-05-01 11:31:35,582 - INFO - Using learned embeddings.\n",
      "2025-05-01 11:31:35,629 - INFO - Model: BiLSTM (Learned Emb), Trainable Parameters: 8,943,035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: BiLSTM (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 11:34:17,090 - INFO - Epoch: 01 | Time: 2m 41s\n",
      "2025-05-01 11:34:17,091 - INFO - \tTrain Loss: 0.575\n",
      "2025-05-01 11:34:17,091 - INFO - \t Val. Loss: 0.580 | Val. F1 (Macro): 0.4282\n",
      "2025-05-01 11:34:17,148 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 11:36:51,097 - INFO - Epoch: 02 | Time: 2m 34s\n",
      "2025-05-01 11:36:51,098 - INFO - \tTrain Loss: 0.466\n",
      "2025-05-01 11:36:51,098 - INFO - \t Val. Loss: 0.491 | Val. F1 (Macro): 0.4796\n",
      "2025-05-01 11:36:51,155 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 11:39:25,776 - INFO - Epoch: 03 | Time: 2m 35s\n",
      "2025-05-01 11:39:25,777 - INFO - \tTrain Loss: 0.415\n",
      "2025-05-01 11:39:25,778 - INFO - \t Val. Loss: 0.461 | Val. F1 (Macro): 0.4998\n",
      "2025-05-01 11:39:25,836 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 11:41:59,678 - INFO - Epoch: 04 | Time: 2m 34s\n",
      "2025-05-01 11:41:59,680 - INFO - \tTrain Loss: 0.384\n",
      "2025-05-01 11:41:59,680 - INFO - \t Val. Loss: 0.492 | Val. F1 (Macro): 0.5349\n",
      "2025-05-01 11:44:32,397 - INFO - Epoch: 05 | Time: 2m 33s\n",
      "2025-05-01 11:44:32,398 - INFO - \tTrain Loss: 0.363\n",
      "2025-05-01 11:44:32,399 - INFO - \t Val. Loss: 0.397 | Val. F1 (Macro): 0.5931\n",
      "2025-05-01 11:44:32,454 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(Learned_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 11:47:06,603 - INFO - Epoch: 06 | Time: 2m 34s\n",
      "2025-05-01 11:47:06,604 - INFO - \tTrain Loss: 0.344\n",
      "2025-05-01 11:47:06,604 - INFO - \t Val. Loss: 0.417 | Val. F1 (Macro): 0.5693\n",
      "2025-05-01 11:49:40,863 - INFO - Epoch: 07 | Time: 2m 34s\n",
      "2025-05-01 11:49:40,864 - INFO - \tTrain Loss: 0.330\n",
      "2025-05-01 11:49:40,864 - INFO - \t Val. Loss: 0.444 | Val. F1 (Macro): 0.6097\n",
      "2025-05-01 11:52:11,987 - INFO - Epoch: 08 | Time: 2m 31s\n",
      "2025-05-01 11:52:11,988 - INFO - \tTrain Loss: 0.318\n",
      "2025-05-01 11:52:11,989 - INFO - \t Val. Loss: 0.442 | Val. F1 (Macro): 0.6047\n",
      "2025-05-01 11:54:47,251 - INFO - Epoch: 09 | Time: 2m 35s\n",
      "2025-05-01 11:54:47,252 - INFO - \tTrain Loss: 0.308\n",
      "2025-05-01 11:54:47,252 - INFO - \t Val. Loss: 0.447 | Val. F1 (Macro): 0.6081\n",
      "2025-05-01 11:57:21,118 - INFO - Epoch: 10 | Time: 2m 34s\n",
      "2025-05-01 11:57:21,119 - INFO - \tTrain Loss: 0.297\n",
      "2025-05-01 11:57:21,119 - INFO - \t Val. Loss: 0.433 | Val. F1 (Macro): 0.6261\n",
      "2025-05-01 12:00:00,300 - INFO - Epoch: 11 | Time: 2m 39s\n",
      "2025-05-01 12:00:00,301 - INFO - \tTrain Loss: 0.288\n",
      "2025-05-01 12:00:00,302 - INFO - \t Val. Loss: 0.472 | Val. F1 (Macro): 0.6196\n",
      "2025-05-01 12:02:40,509 - INFO - Epoch: 12 | Time: 2m 40s\n",
      "2025-05-01 12:02:40,510 - INFO - \tTrain Loss: 0.283\n",
      "2025-05-01 12:02:40,511 - INFO - \t Val. Loss: 0.444 | Val. F1 (Macro): 0.6415\n",
      "2025-05-01 12:05:20,970 - INFO - Epoch: 13 | Time: 2m 40s\n",
      "2025-05-01 12:05:20,972 - INFO - \tTrain Loss: 0.274\n",
      "2025-05-01 12:05:20,973 - INFO - \t Val. Loss: 0.470 | Val. F1 (Macro): 0.6343\n",
      "2025-05-01 12:08:01,556 - INFO - Epoch: 14 | Time: 2m 41s\n",
      "2025-05-01 12:08:01,558 - INFO - \tTrain Loss: 0.268\n",
      "2025-05-01 12:08:01,559 - INFO - \t Val. Loss: 0.447 | Val. F1 (Macro): 0.6398\n",
      "2025-05-01 12:10:42,210 - INFO - Epoch: 15 | Time: 2m 41s\n",
      "2025-05-01 12:10:42,212 - INFO - \tTrain Loss: 0.260\n",
      "2025-05-01 12:10:42,212 - INFO - \t Val. Loss: 0.522 | Val. F1 (Macro): 0.6175\n",
      "2025-05-01 12:13:20,059 - INFO - Epoch: 16 | Time: 2m 38s\n",
      "2025-05-01 12:13:20,060 - INFO - \tTrain Loss: 0.254\n",
      "2025-05-01 12:13:20,060 - INFO - \t Val. Loss: 0.471 | Val. F1 (Macro): 0.6400\n",
      "2025-05-01 12:15:54,227 - INFO - Epoch: 17 | Time: 2m 34s\n",
      "2025-05-01 12:15:54,228 - INFO - \tTrain Loss: 0.249\n",
      "2025-05-01 12:15:54,229 - INFO - \t Val. Loss: 0.525 | Val. F1 (Macro): 0.6351\n",
      "2025-05-01 12:18:29,266 - INFO - Epoch: 18 | Time: 2m 35s\n",
      "2025-05-01 12:18:29,267 - INFO - \tTrain Loss: 0.243\n",
      "2025-05-01 12:18:29,268 - INFO - \t Val. Loss: 0.517 | Val. F1 (Macro): 0.6329\n",
      "2025-05-01 12:21:05,026 - INFO - Epoch: 19 | Time: 2m 36s\n",
      "2025-05-01 12:21:05,028 - INFO - \tTrain Loss: 0.239\n",
      "2025-05-01 12:21:05,028 - INFO - \t Val. Loss: 0.559 | Val. F1 (Macro): 0.6185\n",
      "2025-05-01 12:23:40,294 - INFO - Epoch: 20 | Time: 2m 35s\n",
      "2025-05-01 12:23:40,294 - INFO - \tTrain Loss: 0.231\n",
      "2025-05-01 12:23:40,295 - INFO - \t Val. Loss: 0.515 | Val. F1 (Macro): 0.6292\n",
      "2025-05-01 12:26:12,982 - INFO - Epoch: 21 | Time: 2m 33s\n",
      "2025-05-01 12:26:12,983 - INFO - \tTrain Loss: 0.226\n",
      "2025-05-01 12:26:12,983 - INFO - \t Val. Loss: 0.526 | Val. F1 (Macro): 0.6328\n",
      "2025-05-01 12:28:54,411 - INFO - Epoch: 22 | Time: 2m 41s\n",
      "2025-05-01 12:28:54,413 - INFO - \tTrain Loss: 0.222\n",
      "2025-05-01 12:28:54,413 - INFO - \t Val. Loss: 0.577 | Val. F1 (Macro): 0.6367\n",
      "2025-05-01 12:31:36,925 - INFO - Epoch: 23 | Time: 2m 43s\n",
      "2025-05-01 12:31:36,926 - INFO - \tTrain Loss: 0.217\n",
      "2025-05-01 12:31:36,927 - INFO - \t Val. Loss: 0.583 | Val. F1 (Macro): 0.6152\n",
      "2025-05-01 12:34:19,697 - INFO - Epoch: 24 | Time: 2m 43s\n",
      "2025-05-01 12:34:19,697 - INFO - \tTrain Loss: 0.213\n",
      "2025-05-01 12:34:19,698 - INFO - \t Val. Loss: 0.526 | Val. F1 (Macro): 0.6253\n",
      "2025-05-01 12:37:02,996 - INFO - Epoch: 25 | Time: 2m 43s\n",
      "2025-05-01 12:37:02,997 - INFO - \tTrain Loss: 0.209\n",
      "2025-05-01 12:37:02,998 - INFO - \t Val. Loss: 0.557 | Val. F1 (Macro): 0.6434\n",
      "2025-05-01 12:39:45,334 - INFO - Epoch: 26 | Time: 2m 42s\n",
      "2025-05-01 12:39:45,336 - INFO - \tTrain Loss: 0.204\n",
      "2025-05-01 12:39:45,337 - INFO - \t Val. Loss: 0.625 | Val. F1 (Macro): 0.6232\n",
      "2025-05-01 12:42:27,076 - INFO - Epoch: 27 | Time: 2m 42s\n",
      "2025-05-01 12:42:27,077 - INFO - \tTrain Loss: 0.200\n",
      "2025-05-01 12:42:27,078 - INFO - \t Val. Loss: 0.623 | Val. F1 (Macro): 0.6355\n",
      "2025-05-01 12:45:07,691 - INFO - Epoch: 28 | Time: 2m 41s\n",
      "2025-05-01 12:45:07,693 - INFO - \tTrain Loss: 0.196\n",
      "2025-05-01 12:45:07,693 - INFO - \t Val. Loss: 0.626 | Val. F1 (Macro): 0.6246\n",
      "2025-05-01 12:47:46,992 - INFO - Epoch: 29 | Time: 2m 39s\n",
      "2025-05-01 12:47:46,992 - INFO - \tTrain Loss: 0.192\n",
      "2025-05-01 12:47:46,993 - INFO - \t Val. Loss: 0.584 | Val. F1 (Macro): 0.6342\n",
      "2025-05-01 12:50:24,491 - INFO - Epoch: 30 | Time: 2m 37s\n",
      "2025-05-01 12:50:24,493 - INFO - \tTrain Loss: 0.189\n",
      "2025-05-01 12:50:24,493 - INFO - \t Val. Loss: 0.615 | Val. F1 (Macro): 0.6326\n",
      "2025-05-01 12:50:24,559 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 12:50:37,370 - INFO - Test Set Performance:\n",
      "2025-05-01 12:50:37,371 - INFO - \tAccuracy: 0.8660\n",
      "2025-05-01 12:50:37,372 - INFO - \tF1 (Macro): 0.5945\n",
      "2025-05-01 12:50:37,372 - INFO - \tPrecision (Macro): 0.6987\n",
      "2025-05-01 12:50:37,372 - INFO - \tRecall (Macro): 0.5644\n",
      "2025-05-01 12:50:37,373 - INFO - \tF1 (Weighted): 0.8389\n",
      "2025-05-01 12:50:37,373 - INFO - \tPrecision (Weighted): 0.8350\n",
      "2025-05-01 12:50:37,373 - INFO - \tRecall (Weighted): 0.8660\n",
      "2025-05-01 12:50:37,374 - INFO - \tTest Loss: 0.400\n",
      "2025-05-01 12:50:37,374 - INFO - \tEval Time: 12.679s\n",
      "2025-05-01 12:50:37,377 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_BiLSTM_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 12:50:39,178 - INFO - Starting training for CNN (Learned Emb) on Book Review\n",
      "2025-05-01 12:50:39,218 - INFO - Using learned embeddings.\n",
      "2025-05-01 12:50:39,235 - INFO - Model: CNN (Learned Emb), Trainable Parameters: 8,780,203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: CNN (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 12:52:45,713 - INFO - Epoch: 01 | Time: 2m 6s\n",
      "2025-05-01 12:52:45,714 - INFO - \tTrain Loss: 0.609\n",
      "2025-05-01 12:52:45,714 - INFO - \t Val. Loss: 0.464 | Val. F1 (Macro): 0.4674\n",
      "2025-05-01 12:52:45,775 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 12:54:30,697 - INFO - Epoch: 02 | Time: 1m 45s\n",
      "2025-05-01 12:54:30,698 - INFO - \tTrain Loss: 0.516\n",
      "2025-05-01 12:54:30,698 - INFO - \t Val. Loss: 0.430 | Val. F1 (Macro): 0.5146\n",
      "2025-05-01 12:54:30,774 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 12:56:10,898 - INFO - Epoch: 03 | Time: 1m 40s\n",
      "2025-05-01 12:56:10,900 - INFO - \tTrain Loss: 0.475\n",
      "2025-05-01 12:56:10,900 - INFO - \t Val. Loss: 0.408 | Val. F1 (Macro): 0.5400\n",
      "2025-05-01 12:56:10,993 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 12:57:48,828 - INFO - Epoch: 04 | Time: 1m 38s\n",
      "2025-05-01 12:57:48,829 - INFO - \tTrain Loss: 0.449\n",
      "2025-05-01 12:57:48,830 - INFO - \t Val. Loss: 0.395 | Val. F1 (Macro): 0.5757\n",
      "2025-05-01 12:57:48,902 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 12:59:26,115 - INFO - Epoch: 05 | Time: 1m 37s\n",
      "2025-05-01 12:59:26,118 - INFO - \tTrain Loss: 0.432\n",
      "2025-05-01 12:59:26,118 - INFO - \t Val. Loss: 0.396 | Val. F1 (Macro): 0.5664\n",
      "2025-05-01 13:01:03,109 - INFO - Epoch: 06 | Time: 1m 37s\n",
      "2025-05-01 13:01:03,109 - INFO - \tTrain Loss: 0.419\n",
      "2025-05-01 13:01:03,110 - INFO - \t Val. Loss: 0.388 | Val. F1 (Macro): 0.5816\n",
      "2025-05-01 13:01:03,201 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 13:02:37,915 - INFO - Epoch: 07 | Time: 1m 35s\n",
      "2025-05-01 13:02:37,916 - INFO - \tTrain Loss: 0.407\n",
      "2025-05-01 13:02:37,917 - INFO - \t Val. Loss: 0.385 | Val. F1 (Macro): 0.5919\n",
      "2025-05-01 13:02:37,982 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 13:04:11,346 - INFO - Epoch: 08 | Time: 1m 33s\n",
      "2025-05-01 13:04:11,348 - INFO - \tTrain Loss: 0.396\n",
      "2025-05-01 13:04:11,348 - INFO - \t Val. Loss: 0.388 | Val. F1 (Macro): 0.5849\n",
      "2025-05-01 13:05:43,000 - INFO - Epoch: 09 | Time: 1m 32s\n",
      "2025-05-01 13:05:43,001 - INFO - \tTrain Loss: 0.389\n",
      "2025-05-01 13:05:43,001 - INFO - \t Val. Loss: 0.384 | Val. F1 (Macro): 0.5941\n",
      "2025-05-01 13:05:43,064 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 13:07:15,357 - INFO - Epoch: 10 | Time: 1m 32s\n",
      "2025-05-01 13:07:15,358 - INFO - \tTrain Loss: 0.382\n",
      "2025-05-01 13:07:15,359 - INFO - \t Val. Loss: 0.402 | Val. F1 (Macro): 0.5537\n",
      "2025-05-01 13:08:47,330 - INFO - Epoch: 11 | Time: 1m 32s\n",
      "2025-05-01 13:08:47,332 - INFO - \tTrain Loss: 0.375\n",
      "2025-05-01 13:08:47,332 - INFO - \t Val. Loss: 0.385 | Val. F1 (Macro): 0.5825\n",
      "2025-05-01 13:10:19,153 - INFO - Epoch: 12 | Time: 1m 32s\n",
      "2025-05-01 13:10:19,155 - INFO - \tTrain Loss: 0.369\n",
      "2025-05-01 13:10:19,156 - INFO - \t Val. Loss: 0.396 | Val. F1 (Macro): 0.5651\n",
      "2025-05-01 13:11:51,225 - INFO - Epoch: 13 | Time: 1m 32s\n",
      "2025-05-01 13:11:51,227 - INFO - \tTrain Loss: 0.362\n",
      "2025-05-01 13:11:51,228 - INFO - \t Val. Loss: 0.395 | Val. F1 (Macro): 0.5732\n",
      "2025-05-01 13:13:23,815 - INFO - Epoch: 14 | Time: 1m 33s\n",
      "2025-05-01 13:13:23,816 - INFO - \tTrain Loss: 0.357\n",
      "2025-05-01 13:13:23,816 - INFO - \t Val. Loss: 0.381 | Val. F1 (Macro): 0.6003\n",
      "2025-05-01 13:13:23,871 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 13:14:55,838 - INFO - Epoch: 15 | Time: 1m 32s\n",
      "2025-05-01 13:14:55,838 - INFO - \tTrain Loss: 0.351\n",
      "2025-05-01 13:14:55,839 - INFO - \t Val. Loss: 0.404 | Val. F1 (Macro): 0.5602\n",
      "2025-05-01 13:16:27,924 - INFO - Epoch: 16 | Time: 1m 32s\n",
      "2025-05-01 13:16:27,924 - INFO - \tTrain Loss: 0.346\n",
      "2025-05-01 13:16:27,925 - INFO - \t Val. Loss: 0.399 | Val. F1 (Macro): 0.5765\n",
      "2025-05-01 13:18:00,194 - INFO - Epoch: 17 | Time: 1m 32s\n",
      "2025-05-01 13:18:00,195 - INFO - \tTrain Loss: 0.342\n",
      "2025-05-01 13:18:00,196 - INFO - \t Val. Loss: 0.403 | Val. F1 (Macro): 0.5827\n",
      "2025-05-01 13:19:32,951 - INFO - Epoch: 18 | Time: 1m 33s\n",
      "2025-05-01 13:19:32,952 - INFO - \tTrain Loss: 0.335\n",
      "2025-05-01 13:19:32,953 - INFO - \t Val. Loss: 0.417 | Val. F1 (Macro): 0.5740\n",
      "2025-05-01 13:21:04,718 - INFO - Epoch: 19 | Time: 1m 32s\n",
      "2025-05-01 13:21:04,718 - INFO - \tTrain Loss: 0.332\n",
      "2025-05-01 13:21:04,719 - INFO - \t Val. Loss: 0.396 | Val. F1 (Macro): 0.5961\n",
      "2025-05-01 13:22:36,937 - INFO - Epoch: 20 | Time: 1m 32s\n",
      "2025-05-01 13:22:36,938 - INFO - \tTrain Loss: 0.327\n",
      "2025-05-01 13:22:36,938 - INFO - \t Val. Loss: 0.408 | Val. F1 (Macro): 0.5779\n",
      "2025-05-01 13:24:09,819 - INFO - Epoch: 21 | Time: 1m 33s\n",
      "2025-05-01 13:24:09,820 - INFO - \tTrain Loss: 0.322\n",
      "2025-05-01 13:24:09,821 - INFO - \t Val. Loss: 0.421 | Val. F1 (Macro): 0.5714\n",
      "2025-05-01 13:25:40,115 - INFO - Epoch: 22 | Time: 1m 30s\n",
      "2025-05-01 13:25:40,117 - INFO - \tTrain Loss: 0.316\n",
      "2025-05-01 13:25:40,118 - INFO - \t Val. Loss: 0.417 | Val. F1 (Macro): 0.5787\n",
      "2025-05-01 13:27:08,452 - INFO - Epoch: 23 | Time: 1m 28s\n",
      "2025-05-01 13:27:08,454 - INFO - \tTrain Loss: 0.314\n",
      "2025-05-01 13:27:08,455 - INFO - \t Val. Loss: 0.411 | Val. F1 (Macro): 0.5944\n",
      "2025-05-01 13:28:36,251 - INFO - Epoch: 24 | Time: 1m 28s\n",
      "2025-05-01 13:28:36,253 - INFO - \tTrain Loss: 0.308\n",
      "2025-05-01 13:28:36,253 - INFO - \t Val. Loss: 0.416 | Val. F1 (Macro): 0.5795\n",
      "2025-05-01 13:30:04,006 - INFO - Epoch: 25 | Time: 1m 28s\n",
      "2025-05-01 13:30:04,006 - INFO - \tTrain Loss: 0.304\n",
      "2025-05-01 13:30:04,007 - INFO - \t Val. Loss: 0.411 | Val. F1 (Macro): 0.5991\n",
      "2025-05-01 13:31:31,773 - INFO - Epoch: 26 | Time: 1m 28s\n",
      "2025-05-01 13:31:31,774 - INFO - \tTrain Loss: 0.300\n",
      "2025-05-01 13:31:31,775 - INFO - \t Val. Loss: 0.423 | Val. F1 (Macro): 0.5956\n",
      "2025-05-01 13:32:59,549 - INFO - Epoch: 27 | Time: 1m 28s\n",
      "2025-05-01 13:32:59,550 - INFO - \tTrain Loss: 0.297\n",
      "2025-05-01 13:32:59,551 - INFO - \t Val. Loss: 0.427 | Val. F1 (Macro): 0.5729\n",
      "2025-05-01 13:34:27,167 - INFO - Epoch: 28 | Time: 1m 28s\n",
      "2025-05-01 13:34:27,169 - INFO - \tTrain Loss: 0.293\n",
      "2025-05-01 13:34:27,169 - INFO - \t Val. Loss: 0.435 | Val. F1 (Macro): 0.5867\n",
      "2025-05-01 13:35:54,605 - INFO - Epoch: 29 | Time: 1m 27s\n",
      "2025-05-01 13:35:54,606 - INFO - \tTrain Loss: 0.291\n",
      "2025-05-01 13:35:54,606 - INFO - \t Val. Loss: 0.435 | Val. F1 (Macro): 0.5826\n",
      "2025-05-01 13:37:21,739 - INFO - Epoch: 30 | Time: 1m 27s\n",
      "2025-05-01 13:37:21,741 - INFO - \tTrain Loss: 0.285\n",
      "2025-05-01 13:37:21,742 - INFO - \t Val. Loss: 0.439 | Val. F1 (Macro): 0.5817\n",
      "2025-05-01 13:37:21,817 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_CNN_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 13:37:30,127 - INFO - Test Set Performance:\n",
      "2025-05-01 13:37:30,128 - INFO - \tAccuracy: 0.8649\n",
      "2025-05-01 13:37:30,128 - INFO - \tF1 (Macro): 0.5969\n",
      "2025-05-01 13:37:30,129 - INFO - \tPrecision (Macro): 0.7385\n",
      "2025-05-01 13:37:30,129 - INFO - \tRecall (Macro): 0.5712\n",
      "2025-05-01 13:37:30,130 - INFO - \tF1 (Weighted): 0.8376\n",
      "2025-05-01 13:37:30,131 - INFO - \tPrecision (Weighted): 0.8434\n",
      "2025-05-01 13:37:30,131 - INFO - \tRecall (Weighted): 0.8649\n",
      "2025-05-01 13:37:30,131 - INFO - \tTest Loss: 0.382\n",
      "2025-05-01 13:37:30,132 - INFO - \tEval Time: 8.136s\n",
      "2025-05-01 13:37:30,135 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_CNN_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 13:37:30,455 - INFO - Starting training for MLP (Avg GloVe Emb) on Book Review\n",
      "2025-05-01 13:37:30,456 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 13:37:30,466 - INFO - Model: MLP (Avg GloVe Emb), Trainable Parameters: 8,643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: MLP (Avg GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 13:38:01,045 - INFO - Epoch: 01 | Time: 0m 31s\n",
      "2025-05-01 13:38:01,046 - INFO - \tTrain Loss: 0.580\n",
      "2025-05-01 13:38:01,047 - INFO - \t Val. Loss: 0.539 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 13:38:01,103 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 13:38:32,093 - INFO - Epoch: 02 | Time: 0m 31s\n",
      "2025-05-01 13:38:32,094 - INFO - \tTrain Loss: 0.544\n",
      "2025-05-01 13:38:32,095 - INFO - \t Val. Loss: 0.528 | Val. F1 (Macro): 0.3482\n",
      "2025-05-01 13:38:32,181 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 13:39:03,556 - INFO - Epoch: 03 | Time: 0m 31s\n",
      "2025-05-01 13:39:03,558 - INFO - \tTrain Loss: 0.539\n",
      "2025-05-01 13:39:03,559 - INFO - \t Val. Loss: 0.522 | Val. F1 (Macro): 0.3857\n",
      "2025-05-01 13:39:03,635 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 13:39:35,686 - INFO - Epoch: 04 | Time: 0m 32s\n",
      "2025-05-01 13:39:35,687 - INFO - \tTrain Loss: 0.537\n",
      "2025-05-01 13:39:35,687 - INFO - \t Val. Loss: 0.527 | Val. F1 (Macro): 0.3705\n",
      "2025-05-01 13:40:06,615 - INFO - Epoch: 05 | Time: 0m 31s\n",
      "2025-05-01 13:40:06,616 - INFO - \tTrain Loss: 0.536\n",
      "2025-05-01 13:40:06,617 - INFO - \t Val. Loss: 0.517 | Val. F1 (Macro): 0.4256\n",
      "2025-05-01 13:40:06,706 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 13:40:38,995 - INFO - Epoch: 06 | Time: 0m 32s\n",
      "2025-05-01 13:40:38,996 - INFO - \tTrain Loss: 0.534\n",
      "2025-05-01 13:40:38,996 - INFO - \t Val. Loss: 0.517 | Val. F1 (Macro): 0.4081\n",
      "2025-05-01 13:40:39,055 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 13:41:10,105 - INFO - Epoch: 07 | Time: 0m 31s\n",
      "2025-05-01 13:41:10,106 - INFO - \tTrain Loss: 0.533\n",
      "2025-05-01 13:41:10,107 - INFO - \t Val. Loss: 0.520 | Val. F1 (Macro): 0.3797\n",
      "2025-05-01 13:41:41,041 - INFO - Epoch: 08 | Time: 0m 31s\n",
      "2025-05-01 13:41:41,042 - INFO - \tTrain Loss: 0.531\n",
      "2025-05-01 13:41:41,043 - INFO - \t Val. Loss: 0.514 | Val. F1 (Macro): 0.4150\n",
      "2025-05-01 13:41:41,093 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 13:42:17,275 - INFO - Epoch: 09 | Time: 0m 36s\n",
      "2025-05-01 13:42:17,277 - INFO - \tTrain Loss: 0.530\n",
      "2025-05-01 13:42:17,277 - INFO - \t Val. Loss: 0.513 | Val. F1 (Macro): 0.4093\n",
      "2025-05-01 13:42:17,341 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 13:42:51,237 - INFO - Epoch: 10 | Time: 0m 34s\n",
      "2025-05-01 13:42:51,238 - INFO - \tTrain Loss: 0.531\n",
      "2025-05-01 13:42:51,238 - INFO - \t Val. Loss: 0.511 | Val. F1 (Macro): 0.4233\n",
      "2025-05-01 13:42:51,306 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 10)\n",
      "2025-05-01 13:43:25,611 - INFO - Epoch: 11 | Time: 0m 34s\n",
      "2025-05-01 13:43:25,613 - INFO - \tTrain Loss: 0.530\n",
      "2025-05-01 13:43:25,613 - INFO - \t Val. Loss: 0.516 | Val. F1 (Macro): 0.3909\n",
      "2025-05-01 13:43:57,697 - INFO - Epoch: 12 | Time: 0m 32s\n",
      "2025-05-01 13:43:57,699 - INFO - \tTrain Loss: 0.529\n",
      "2025-05-01 13:43:57,699 - INFO - \t Val. Loss: 0.514 | Val. F1 (Macro): 0.4050\n",
      "2025-05-01 13:44:30,913 - INFO - Epoch: 13 | Time: 0m 33s\n",
      "2025-05-01 13:44:30,914 - INFO - \tTrain Loss: 0.529\n",
      "2025-05-01 13:44:30,914 - INFO - \t Val. Loss: 0.509 | Val. F1 (Macro): 0.4220\n",
      "2025-05-01 13:44:30,964 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 13:45:03,446 - INFO - Epoch: 14 | Time: 0m 32s\n",
      "2025-05-01 13:45:03,447 - INFO - \tTrain Loss: 0.528\n",
      "2025-05-01 13:45:03,447 - INFO - \t Val. Loss: 0.511 | Val. F1 (Macro): 0.4324\n",
      "2025-05-01 13:45:35,652 - INFO - Epoch: 15 | Time: 0m 32s\n",
      "2025-05-01 13:45:35,653 - INFO - \tTrain Loss: 0.527\n",
      "2025-05-01 13:45:35,653 - INFO - \t Val. Loss: 0.512 | Val. F1 (Macro): 0.4025\n",
      "2025-05-01 13:46:07,783 - INFO - Epoch: 16 | Time: 0m 32s\n",
      "2025-05-01 13:46:07,784 - INFO - \tTrain Loss: 0.527\n",
      "2025-05-01 13:46:07,785 - INFO - \t Val. Loss: 0.512 | Val. F1 (Macro): 0.4423\n",
      "2025-05-01 13:46:39,203 - INFO - Epoch: 17 | Time: 0m 31s\n",
      "2025-05-01 13:46:39,203 - INFO - \tTrain Loss: 0.527\n",
      "2025-05-01 13:46:39,204 - INFO - \t Val. Loss: 0.508 | Val. F1 (Macro): 0.4189\n",
      "2025-05-01 13:46:39,254 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 13:47:10,638 - INFO - Epoch: 18 | Time: 0m 31s\n",
      "2025-05-01 13:47:10,639 - INFO - \tTrain Loss: 0.525\n",
      "2025-05-01 13:47:10,639 - INFO - \t Val. Loss: 0.509 | Val. F1 (Macro): 0.4368\n",
      "2025-05-01 13:47:41,697 - INFO - Epoch: 19 | Time: 0m 31s\n",
      "2025-05-01 13:47:41,698 - INFO - \tTrain Loss: 0.525\n",
      "2025-05-01 13:47:41,699 - INFO - \t Val. Loss: 0.508 | Val. F1 (Macro): 0.4336\n",
      "2025-05-01 13:48:12,934 - INFO - Epoch: 20 | Time: 0m 31s\n",
      "2025-05-01 13:48:12,935 - INFO - \tTrain Loss: 0.525\n",
      "2025-05-01 13:48:12,935 - INFO - \t Val. Loss: 0.511 | Val. F1 (Macro): 0.4052\n",
      "2025-05-01 13:48:43,549 - INFO - Epoch: 21 | Time: 0m 31s\n",
      "2025-05-01 13:48:43,550 - INFO - \tTrain Loss: 0.524\n",
      "2025-05-01 13:48:43,562 - INFO - \t Val. Loss: 0.511 | Val. F1 (Macro): 0.4438\n",
      "2025-05-01 13:49:14,992 - INFO - Epoch: 22 | Time: 0m 31s\n",
      "2025-05-01 13:49:14,993 - INFO - \tTrain Loss: 0.524\n",
      "2025-05-01 13:49:14,994 - INFO - \t Val. Loss: 0.506 | Val. F1 (Macro): 0.4073\n",
      "2025-05-01 13:49:15,061 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 22)\n",
      "2025-05-01 13:49:46,784 - INFO - Epoch: 23 | Time: 0m 32s\n",
      "2025-05-01 13:49:46,785 - INFO - \tTrain Loss: 0.523\n",
      "2025-05-01 13:49:46,786 - INFO - \t Val. Loss: 0.506 | Val. F1 (Macro): 0.4343\n",
      "2025-05-01 13:50:17,937 - INFO - Epoch: 24 | Time: 0m 31s\n",
      "2025-05-01 13:50:17,938 - INFO - \tTrain Loss: 0.523\n",
      "2025-05-01 13:50:17,938 - INFO - \t Val. Loss: 0.512 | Val. F1 (Macro): 0.3949\n",
      "2025-05-01 13:50:48,986 - INFO - Epoch: 25 | Time: 0m 31s\n",
      "2025-05-01 13:50:48,987 - INFO - \tTrain Loss: 0.522\n",
      "2025-05-01 13:50:48,987 - INFO - \t Val. Loss: 0.506 | Val. F1 (Macro): 0.4465\n",
      "2025-05-01 13:50:49,073 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 25)\n",
      "2025-05-01 13:51:19,879 - INFO - Epoch: 26 | Time: 0m 31s\n",
      "2025-05-01 13:51:19,881 - INFO - \tTrain Loss: 0.523\n",
      "2025-05-01 13:51:19,881 - INFO - \t Val. Loss: 0.508 | Val. F1 (Macro): 0.4077\n",
      "2025-05-01 13:51:51,007 - INFO - Epoch: 27 | Time: 0m 31s\n",
      "2025-05-01 13:51:51,008 - INFO - \tTrain Loss: 0.523\n",
      "2025-05-01 13:51:51,008 - INFO - \t Val. Loss: 0.505 | Val. F1 (Macro): 0.4172\n",
      "2025-05-01 13:51:51,101 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 27)\n",
      "2025-05-01 13:52:22,369 - INFO - Epoch: 28 | Time: 0m 31s\n",
      "2025-05-01 13:52:22,371 - INFO - \tTrain Loss: 0.521\n",
      "2025-05-01 13:52:22,371 - INFO - \t Val. Loss: 0.506 | Val. F1 (Macro): 0.4131\n",
      "2025-05-01 13:52:53,156 - INFO - Epoch: 29 | Time: 0m 31s\n",
      "2025-05-01 13:52:53,156 - INFO - \tTrain Loss: 0.522\n",
      "2025-05-01 13:52:53,157 - INFO - \t Val. Loss: 0.509 | Val. F1 (Macro): 0.4035\n",
      "2025-05-01 13:53:24,735 - INFO - Epoch: 30 | Time: 0m 32s\n",
      "2025-05-01 13:53:24,737 - INFO - \tTrain Loss: 0.520\n",
      "2025-05-01 13:53:24,737 - INFO - \t Val. Loss: 0.506 | Val. F1 (Macro): 0.4173\n",
      "2025-05-01 13:53:24,839 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 13:53:29,753 - INFO - Test Set Performance:\n",
      "2025-05-01 13:53:29,753 - INFO - \tAccuracy: 0.8142\n",
      "2025-05-01 13:53:29,754 - INFO - \tF1 (Macro): 0.4110\n",
      "2025-05-01 13:53:29,754 - INFO - \tPrecision (Macro): 0.4839\n",
      "2025-05-01 13:53:29,754 - INFO - \tRecall (Macro): 0.4053\n",
      "2025-05-01 13:53:29,755 - INFO - \tF1 (Weighted): 0.7552\n",
      "2025-05-01 13:53:29,755 - INFO - \tPrecision (Weighted): 0.7300\n",
      "2025-05-01 13:53:29,755 - INFO - \tRecall (Weighted): 0.8142\n",
      "2025-05-01 13:53:29,756 - INFO - \tTest Loss: 0.507\n",
      "2025-05-01 13:53:29,756 - INFO - \tEval Time: 4.741s\n",
      "2025-05-01 13:53:29,760 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_MLP_(Avg_GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 13:53:29,951 - INFO - Starting training for CNN (GloVe Emb) on Book Review\n",
      "2025-05-01 13:53:29,952 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 13:53:29,965 - INFO - Model: CNN (GloVe Emb), Trainable Parameters: 121,203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: CNN (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 13:54:33,974 - INFO - Epoch: 01 | Time: 1m 4s\n",
      "2025-05-01 13:54:33,976 - INFO - \tTrain Loss: 0.566\n",
      "2025-05-01 13:54:33,976 - INFO - \t Val. Loss: 0.486 | Val. F1 (Macro): 0.4719\n",
      "2025-05-01 13:54:34,028 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 13:55:41,246 - INFO - Epoch: 02 | Time: 1m 7s\n",
      "2025-05-01 13:55:41,247 - INFO - \tTrain Loss: 0.529\n",
      "2025-05-01 13:55:41,247 - INFO - \t Val. Loss: 0.473 | Val. F1 (Macro): 0.4747\n",
      "2025-05-01 13:55:41,300 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 13:56:51,758 - INFO - Epoch: 03 | Time: 1m 10s\n",
      "2025-05-01 13:56:51,759 - INFO - \tTrain Loss: 0.518\n",
      "2025-05-01 13:56:51,760 - INFO - \t Val. Loss: 0.461 | Val. F1 (Macro): 0.4793\n",
      "2025-05-01 13:56:51,834 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 13:58:01,584 - INFO - Epoch: 04 | Time: 1m 10s\n",
      "2025-05-01 13:58:01,586 - INFO - \tTrain Loss: 0.513\n",
      "2025-05-01 13:58:01,586 - INFO - \t Val. Loss: 0.458 | Val. F1 (Macro): 0.4895\n",
      "2025-05-01 13:58:01,658 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 13:59:12,684 - INFO - Epoch: 05 | Time: 1m 11s\n",
      "2025-05-01 13:59:12,687 - INFO - \tTrain Loss: 0.507\n",
      "2025-05-01 13:59:12,688 - INFO - \t Val. Loss: 0.454 | Val. F1 (Macro): 0.4762\n",
      "2025-05-01 13:59:12,775 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 14:00:18,539 - INFO - Epoch: 06 | Time: 1m 6s\n",
      "2025-05-01 14:00:18,540 - INFO - \tTrain Loss: 0.504\n",
      "2025-05-01 14:00:18,540 - INFO - \t Val. Loss: 0.452 | Val. F1 (Macro): 0.4765\n",
      "2025-05-01 14:00:18,592 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 14:01:22,414 - INFO - Epoch: 07 | Time: 1m 4s\n",
      "2025-05-01 14:01:22,414 - INFO - \tTrain Loss: 0.502\n",
      "2025-05-01 14:01:22,415 - INFO - \t Val. Loss: 0.450 | Val. F1 (Macro): 0.4916\n",
      "2025-05-01 14:01:22,478 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 14:02:24,482 - INFO - Epoch: 08 | Time: 1m 2s\n",
      "2025-05-01 14:02:24,483 - INFO - \tTrain Loss: 0.498\n",
      "2025-05-01 14:02:24,484 - INFO - \t Val. Loss: 0.449 | Val. F1 (Macro): 0.4781\n",
      "2025-05-01 14:02:24,541 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 14:03:26,329 - INFO - Epoch: 09 | Time: 1m 2s\n",
      "2025-05-01 14:03:26,330 - INFO - \tTrain Loss: 0.498\n",
      "2025-05-01 14:03:26,331 - INFO - \t Val. Loss: 0.446 | Val. F1 (Macro): 0.4903\n",
      "2025-05-01 14:03:26,397 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 14:04:33,292 - INFO - Epoch: 10 | Time: 1m 7s\n",
      "2025-05-01 14:04:33,293 - INFO - \tTrain Loss: 0.494\n",
      "2025-05-01 14:04:33,294 - INFO - \t Val. Loss: 0.451 | Val. F1 (Macro): 0.4666\n",
      "2025-05-01 14:05:42,705 - INFO - Epoch: 11 | Time: 1m 9s\n",
      "2025-05-01 14:05:42,705 - INFO - \tTrain Loss: 0.493\n",
      "2025-05-01 14:05:42,706 - INFO - \t Val. Loss: 0.452 | Val. F1 (Macro): 0.4604\n",
      "2025-05-01 14:06:50,962 - INFO - Epoch: 12 | Time: 1m 8s\n",
      "2025-05-01 14:06:50,963 - INFO - \tTrain Loss: 0.491\n",
      "2025-05-01 14:06:50,963 - INFO - \t Val. Loss: 0.444 | Val. F1 (Macro): 0.4860\n",
      "2025-05-01 14:06:51,045 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 14:07:57,605 - INFO - Epoch: 13 | Time: 1m 7s\n",
      "2025-05-01 14:07:57,605 - INFO - \tTrain Loss: 0.491\n",
      "2025-05-01 14:07:57,606 - INFO - \t Val. Loss: 0.446 | Val. F1 (Macro): 0.4695\n",
      "2025-05-01 14:09:03,330 - INFO - Epoch: 14 | Time: 1m 6s\n",
      "2025-05-01 14:09:03,331 - INFO - \tTrain Loss: 0.490\n",
      "2025-05-01 14:09:03,331 - INFO - \t Val. Loss: 0.444 | Val. F1 (Macro): 0.4736\n",
      "2025-05-01 14:10:07,969 - INFO - Epoch: 15 | Time: 1m 5s\n",
      "2025-05-01 14:10:07,970 - INFO - \tTrain Loss: 0.489\n",
      "2025-05-01 14:10:07,971 - INFO - \t Val. Loss: 0.448 | Val. F1 (Macro): 0.4622\n",
      "2025-05-01 14:11:12,794 - INFO - Epoch: 16 | Time: 1m 5s\n",
      "2025-05-01 14:11:12,796 - INFO - \tTrain Loss: 0.489\n",
      "2025-05-01 14:11:12,797 - INFO - \t Val. Loss: 0.442 | Val. F1 (Macro): 0.4716\n",
      "2025-05-01 14:11:12,879 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 14:12:17,369 - INFO - Epoch: 17 | Time: 1m 4s\n",
      "2025-05-01 14:12:17,370 - INFO - \tTrain Loss: 0.489\n",
      "2025-05-01 14:12:17,371 - INFO - \t Val. Loss: 0.445 | Val. F1 (Macro): 0.4794\n",
      "2025-05-01 14:13:21,675 - INFO - Epoch: 18 | Time: 1m 4s\n",
      "2025-05-01 14:13:21,676 - INFO - \tTrain Loss: 0.486\n",
      "2025-05-01 14:13:21,676 - INFO - \t Val. Loss: 0.446 | Val. F1 (Macro): 0.4577\n",
      "2025-05-01 14:14:25,538 - INFO - Epoch: 19 | Time: 1m 4s\n",
      "2025-05-01 14:14:25,539 - INFO - \tTrain Loss: 0.485\n",
      "2025-05-01 14:14:25,539 - INFO - \t Val. Loss: 0.441 | Val. F1 (Macro): 0.4789\n",
      "2025-05-01 14:14:25,599 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 14:15:30,476 - INFO - Epoch: 20 | Time: 1m 5s\n",
      "2025-05-01 14:15:30,478 - INFO - \tTrain Loss: 0.483\n",
      "2025-05-01 14:15:30,478 - INFO - \t Val. Loss: 0.440 | Val. F1 (Macro): 0.4883\n",
      "2025-05-01 14:15:30,578 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 20)\n",
      "2025-05-01 14:16:34,531 - INFO - Epoch: 21 | Time: 1m 4s\n",
      "2025-05-01 14:16:34,533 - INFO - \tTrain Loss: 0.483\n",
      "2025-05-01 14:16:34,534 - INFO - \t Val. Loss: 0.439 | Val. F1 (Macro): 0.5007\n",
      "2025-05-01 14:16:34,601 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 21)\n",
      "2025-05-01 14:17:38,444 - INFO - Epoch: 22 | Time: 1m 4s\n",
      "2025-05-01 14:17:38,446 - INFO - \tTrain Loss: 0.484\n",
      "2025-05-01 14:17:38,447 - INFO - \t Val. Loss: 0.438 | Val. F1 (Macro): 0.4988\n",
      "2025-05-01 14:17:38,530 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 22)\n",
      "2025-05-01 14:18:42,310 - INFO - Epoch: 23 | Time: 1m 4s\n",
      "2025-05-01 14:18:42,311 - INFO - \tTrain Loss: 0.482\n",
      "2025-05-01 14:18:42,312 - INFO - \t Val. Loss: 0.442 | Val. F1 (Macro): 0.4775\n",
      "2025-05-01 14:19:46,380 - INFO - Epoch: 24 | Time: 1m 4s\n",
      "2025-05-01 14:19:46,381 - INFO - \tTrain Loss: 0.482\n",
      "2025-05-01 14:19:46,382 - INFO - \t Val. Loss: 0.439 | Val. F1 (Macro): 0.4825\n",
      "2025-05-01 14:20:50,567 - INFO - Epoch: 25 | Time: 1m 4s\n",
      "2025-05-01 14:20:50,569 - INFO - \tTrain Loss: 0.481\n",
      "2025-05-01 14:20:50,569 - INFO - \t Val. Loss: 0.439 | Val. F1 (Macro): 0.4809\n",
      "2025-05-01 14:21:54,908 - INFO - Epoch: 26 | Time: 1m 4s\n",
      "2025-05-01 14:21:54,909 - INFO - \tTrain Loss: 0.481\n",
      "2025-05-01 14:21:54,910 - INFO - \t Val. Loss: 0.440 | Val. F1 (Macro): 0.4792\n",
      "2025-05-01 14:23:01,232 - INFO - Epoch: 27 | Time: 1m 6s\n",
      "2025-05-01 14:23:01,234 - INFO - \tTrain Loss: 0.480\n",
      "2025-05-01 14:23:01,234 - INFO - \t Val. Loss: 0.439 | Val. F1 (Macro): 0.4910\n",
      "2025-05-01 14:24:09,498 - INFO - Epoch: 28 | Time: 1m 8s\n",
      "2025-05-01 14:24:09,498 - INFO - \tTrain Loss: 0.481\n",
      "2025-05-01 14:24:09,499 - INFO - \t Val. Loss: 0.446 | Val. F1 (Macro): 0.4586\n",
      "2025-05-01 14:25:15,580 - INFO - Epoch: 29 | Time: 1m 6s\n",
      "2025-05-01 14:25:15,582 - INFO - \tTrain Loss: 0.479\n",
      "2025-05-01 14:25:15,583 - INFO - \t Val. Loss: 0.443 | Val. F1 (Macro): 0.4705\n",
      "2025-05-01 14:26:19,869 - INFO - Epoch: 30 | Time: 1m 4s\n",
      "2025-05-01 14:26:19,871 - INFO - \tTrain Loss: 0.479\n",
      "2025-05-01 14:26:19,872 - INFO - \t Val. Loss: 0.435 | Val. F1 (Macro): 0.4806\n",
      "2025-05-01 14:26:19,935 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt (Epoch 30)\n",
      "2025-05-01 14:26:19,980 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 14:26:27,133 - INFO - Test Set Performance:\n",
      "2025-05-01 14:26:27,134 - INFO - \tAccuracy: 0.8376\n",
      "2025-05-01 14:26:27,134 - INFO - \tF1 (Macro): 0.4767\n",
      "2025-05-01 14:26:27,134 - INFO - \tPrecision (Macro): 0.7359\n",
      "2025-05-01 14:26:27,135 - INFO - \tRecall (Macro): 0.4599\n",
      "2025-05-01 14:26:27,135 - INFO - \tF1 (Weighted): 0.7881\n",
      "2025-05-01 14:26:27,136 - INFO - \tPrecision (Weighted): 0.8130\n",
      "2025-05-01 14:26:27,136 - INFO - \tRecall (Weighted): 0.8376\n",
      "2025-05-01 14:26:27,136 - INFO - \tTest Loss: 0.434\n",
      "2025-05-01 14:26:27,137 - INFO - \tEval Time: 6.952s\n",
      "2025-05-01 14:26:27,139 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_CNN_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 14:26:27,362 - INFO - Starting training for LSTM (GloVe Emb) on Book Review\n",
      "2025-05-01 14:26:27,363 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 14:26:27,373 - INFO - Model: LSTM (GloVe Emb), Trainable Parameters: 109,251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: LSTM (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 14:27:47,930 - INFO - Epoch: 01 | Time: 1m 21s\n",
      "2025-05-01 14:27:47,932 - INFO - \tTrain Loss: 0.650\n",
      "2025-05-01 14:27:47,933 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:27:48,018 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 14:29:07,098 - INFO - Epoch: 02 | Time: 1m 19s\n",
      "2025-05-01 14:29:07,100 - INFO - \tTrain Loss: 0.645\n",
      "2025-05-01 14:29:07,100 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:30:26,867 - INFO - Epoch: 03 | Time: 1m 20s\n",
      "2025-05-01 14:30:26,868 - INFO - \tTrain Loss: 0.643\n",
      "2025-05-01 14:30:26,868 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:31:45,297 - INFO - Epoch: 04 | Time: 1m 18s\n",
      "2025-05-01 14:31:45,300 - INFO - \tTrain Loss: 0.643\n",
      "2025-05-01 14:31:45,301 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:33:04,852 - INFO - Epoch: 05 | Time: 1m 20s\n",
      "2025-05-01 14:33:04,854 - INFO - \tTrain Loss: 0.640\n",
      "2025-05-01 14:33:04,854 - INFO - \t Val. Loss: 0.597 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:33:04,982 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 14:34:24,957 - INFO - Epoch: 06 | Time: 1m 20s\n",
      "2025-05-01 14:34:24,958 - INFO - \tTrain Loss: 0.636\n",
      "2025-05-01 14:34:24,959 - INFO - \t Val. Loss: 0.630 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:35:44,496 - INFO - Epoch: 07 | Time: 1m 20s\n",
      "2025-05-01 14:35:44,498 - INFO - \tTrain Loss: 0.632\n",
      "2025-05-01 14:35:44,499 - INFO - \t Val. Loss: 0.640 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:37:04,979 - INFO - Epoch: 08 | Time: 1m 20s\n",
      "2025-05-01 14:37:04,982 - INFO - \tTrain Loss: 0.631\n",
      "2025-05-01 14:37:04,983 - INFO - \t Val. Loss: 0.640 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:38:25,635 - INFO - Epoch: 09 | Time: 1m 21s\n",
      "2025-05-01 14:38:25,636 - INFO - \tTrain Loss: 0.642\n",
      "2025-05-01 14:38:25,636 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:39:46,321 - INFO - Epoch: 10 | Time: 1m 21s\n",
      "2025-05-01 14:39:46,321 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:39:46,322 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:41:07,607 - INFO - Epoch: 11 | Time: 1m 21s\n",
      "2025-05-01 14:41:07,608 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:41:07,608 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:42:29,780 - INFO - Epoch: 12 | Time: 1m 22s\n",
      "2025-05-01 14:42:29,781 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:42:29,782 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:43:55,344 - INFO - Epoch: 13 | Time: 1m 26s\n",
      "2025-05-01 14:43:55,347 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:43:55,348 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:45:20,570 - INFO - Epoch: 14 | Time: 1m 25s\n",
      "2025-05-01 14:45:20,571 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:45:20,572 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:46:45,421 - INFO - Epoch: 15 | Time: 1m 25s\n",
      "2025-05-01 14:46:45,423 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:46:45,423 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:48:10,522 - INFO - Epoch: 16 | Time: 1m 25s\n",
      "2025-05-01 14:48:10,524 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:48:10,524 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:49:34,100 - INFO - Epoch: 17 | Time: 1m 24s\n",
      "2025-05-01 14:49:34,100 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:49:34,101 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:50:57,923 - INFO - Epoch: 18 | Time: 1m 24s\n",
      "2025-05-01 14:50:57,925 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:50:57,926 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:52:26,533 - INFO - Epoch: 19 | Time: 1m 29s\n",
      "2025-05-01 14:52:26,533 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:52:26,534 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:53:51,961 - INFO - Epoch: 20 | Time: 1m 25s\n",
      "2025-05-01 14:53:51,963 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:53:51,964 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:55:15,495 - INFO - Epoch: 21 | Time: 1m 24s\n",
      "2025-05-01 14:55:15,496 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:55:15,497 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:56:38,822 - INFO - Epoch: 22 | Time: 1m 23s\n",
      "2025-05-01 14:56:38,823 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:56:38,824 - INFO - \t Val. Loss: 0.641 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:58:02,343 - INFO - Epoch: 23 | Time: 1m 24s\n",
      "2025-05-01 14:58:02,344 - INFO - \tTrain Loss: 0.641\n",
      "2025-05-01 14:58:02,344 - INFO - \t Val. Loss: 0.630 | Val. F1 (Macro): 0.2958\n",
      "2025-05-01 14:59:26,214 - INFO - Epoch: 24 | Time: 1m 24s\n",
      "2025-05-01 14:59:26,215 - INFO - \tTrain Loss: 0.578\n",
      "2025-05-01 14:59:26,216 - INFO - \t Val. Loss: 0.507 | Val. F1 (Macro): 0.4117\n",
      "2025-05-01 14:59:26,286 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 24)\n",
      "2025-05-01 15:00:49,528 - INFO - Epoch: 25 | Time: 1m 23s\n",
      "2025-05-01 15:00:49,529 - INFO - \tTrain Loss: 0.518\n",
      "2025-05-01 15:00:49,529 - INFO - \t Val. Loss: 0.452 | Val. F1 (Macro): 0.4996\n",
      "2025-05-01 15:00:49,594 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 25)\n",
      "2025-05-01 15:02:13,868 - INFO - Epoch: 26 | Time: 1m 24s\n",
      "2025-05-01 15:02:13,869 - INFO - \tTrain Loss: 0.489\n",
      "2025-05-01 15:02:13,870 - INFO - \t Val. Loss: 0.439 | Val. F1 (Macro): 0.5112\n",
      "2025-05-01 15:02:13,964 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 26)\n",
      "2025-05-01 15:03:38,983 - INFO - Epoch: 27 | Time: 1m 25s\n",
      "2025-05-01 15:03:38,984 - INFO - \tTrain Loss: 0.471\n",
      "2025-05-01 15:03:38,984 - INFO - \t Val. Loss: 0.416 | Val. F1 (Macro): 0.5179\n",
      "2025-05-01 15:03:39,063 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 27)\n",
      "2025-05-01 15:05:02,504 - INFO - Epoch: 28 | Time: 1m 23s\n",
      "2025-05-01 15:05:02,506 - INFO - \tTrain Loss: 0.459\n",
      "2025-05-01 15:05:02,507 - INFO - \t Val. Loss: 0.413 | Val. F1 (Macro): 0.5249\n",
      "2025-05-01 15:05:02,599 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 28)\n",
      "2025-05-01 15:06:27,127 - INFO - Epoch: 29 | Time: 1m 25s\n",
      "2025-05-01 15:06:27,128 - INFO - \tTrain Loss: 0.450\n",
      "2025-05-01 15:06:27,129 - INFO - \t Val. Loss: 0.399 | Val. F1 (Macro): 0.5285\n",
      "2025-05-01 15:06:27,192 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt (Epoch 29)\n",
      "2025-05-01 15:07:51,873 - INFO - Epoch: 30 | Time: 1m 25s\n",
      "2025-05-01 15:07:51,874 - INFO - \tTrain Loss: 0.443\n",
      "2025-05-01 15:07:51,874 - INFO - \t Val. Loss: 0.401 | Val. F1 (Macro): 0.5249\n",
      "2025-05-01 15:07:51,915 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 15:07:59,858 - INFO - Test Set Performance:\n",
      "2025-05-01 15:07:59,859 - INFO - \tAccuracy: 0.8564\n",
      "2025-05-01 15:07:59,859 - INFO - \tF1 (Macro): 0.5272\n",
      "2025-05-01 15:07:59,860 - INFO - \tPrecision (Macro): 0.5086\n",
      "2025-05-01 15:07:59,860 - INFO - \tRecall (Macro): 0.5474\n",
      "2025-05-01 15:07:59,861 - INFO - \tF1 (Weighted): 0.8177\n",
      "2025-05-01 15:07:59,862 - INFO - \tPrecision (Weighted): 0.7825\n",
      "2025-05-01 15:07:59,862 - INFO - \tRecall (Weighted): 0.8564\n",
      "2025-05-01 15:07:59,864 - INFO - \tTest Loss: 0.402\n",
      "2025-05-01 15:07:59,865 - INFO - \tEval Time: 7.658s\n",
      "2025-05-01 15:07:59,871 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_LSTM_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 15:08:01,205 - INFO - Starting training for BiLSTM (GloVe Emb) on Book Review\n",
      "2025-05-01 15:08:01,205 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 15:08:01,227 - INFO - Model: BiLSTM (GloVe Emb), Trainable Parameters: 284,035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: BiLSTM (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 15:14:50,687 - INFO - Epoch: 01 | Time: 6m 49s\n",
      "2025-05-01 15:14:50,688 - INFO - \tTrain Loss: 0.565\n",
      "2025-05-01 15:14:50,688 - INFO - \t Val. Loss: 0.471 | Val. F1 (Macro): 0.4500\n",
      "2025-05-01 15:14:50,739 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 15:28:48,214 - INFO - Epoch: 02 | Time: 13m 57s\n",
      "2025-05-01 15:28:48,215 - INFO - \tTrain Loss: 0.478\n",
      "2025-05-01 15:28:48,215 - INFO - \t Val. Loss: 0.445 | Val. F1 (Macro): 0.4956\n",
      "2025-05-01 15:28:48,282 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 15:42:34,959 - INFO - Epoch: 03 | Time: 13m 47s\n",
      "2025-05-01 15:42:34,963 - INFO - \tTrain Loss: 0.448\n",
      "2025-05-01 15:42:34,964 - INFO - \t Val. Loss: 0.420 | Val. F1 (Macro): 0.5093\n",
      "2025-05-01 15:42:35,019 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 15:56:22,739 - INFO - Epoch: 04 | Time: 13m 48s\n",
      "2025-05-01 15:56:22,741 - INFO - \tTrain Loss: 0.429\n",
      "2025-05-01 15:56:22,741 - INFO - \t Val. Loss: 0.377 | Val. F1 (Macro): 0.5658\n",
      "2025-05-01 15:56:22,831 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 16:08:37,461 - INFO - Epoch: 05 | Time: 12m 15s\n",
      "2025-05-01 16:08:37,462 - INFO - \tTrain Loss: 0.417\n",
      "2025-05-01 16:08:37,462 - INFO - \t Val. Loss: 0.369 | Val. F1 (Macro): 0.5838\n",
      "2025-05-01 16:08:37,512 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 16:18:05,247 - INFO - Epoch: 06 | Time: 9m 28s\n",
      "2025-05-01 16:18:05,248 - INFO - \tTrain Loss: 0.406\n",
      "2025-05-01 16:18:05,249 - INFO - \t Val. Loss: 0.376 | Val. F1 (Macro): 0.6013\n",
      "2025-05-01 16:27:14,673 - INFO - Epoch: 07 | Time: 9m 9s\n",
      "2025-05-01 16:27:14,674 - INFO - \tTrain Loss: 0.397\n",
      "2025-05-01 16:27:14,675 - INFO - \t Val. Loss: 0.353 | Val. F1 (Macro): 0.6244\n",
      "2025-05-01 16:27:14,730 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 16:36:21,641 - INFO - Epoch: 08 | Time: 9m 7s\n",
      "2025-05-01 16:36:21,642 - INFO - \tTrain Loss: 0.391\n",
      "2025-05-01 16:36:21,643 - INFO - \t Val. Loss: 0.354 | Val. F1 (Macro): 0.6172\n",
      "2025-05-01 16:45:31,144 - INFO - Epoch: 09 | Time: 9m 9s\n",
      "2025-05-01 16:45:31,145 - INFO - \tTrain Loss: 0.385\n",
      "2025-05-01 16:45:31,147 - INFO - \t Val. Loss: 0.347 | Val. F1 (Macro): 0.6374\n",
      "2025-05-01 16:45:31,201 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 16:54:50,348 - INFO - Epoch: 10 | Time: 9m 19s\n",
      "2025-05-01 16:54:50,350 - INFO - \tTrain Loss: 0.382\n",
      "2025-05-01 16:54:50,351 - INFO - \t Val. Loss: 0.358 | Val. F1 (Macro): 0.6172\n",
      "2025-05-01 17:04:05,550 - INFO - Epoch: 11 | Time: 9m 15s\n",
      "2025-05-01 17:04:05,551 - INFO - \tTrain Loss: 0.376\n",
      "2025-05-01 17:04:05,552 - INFO - \t Val. Loss: 0.341 | Val. F1 (Macro): 0.6538\n",
      "2025-05-01 17:04:05,607 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 11)\n",
      "2025-05-01 17:13:17,693 - INFO - Epoch: 12 | Time: 9m 12s\n",
      "2025-05-01 17:13:17,693 - INFO - \tTrain Loss: 0.373\n",
      "2025-05-01 17:13:17,694 - INFO - \t Val. Loss: 0.340 | Val. F1 (Macro): 0.6421\n",
      "2025-05-01 17:13:17,745 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 17:22:30,050 - INFO - Epoch: 13 | Time: 9m 12s\n",
      "2025-05-01 17:22:30,053 - INFO - \tTrain Loss: 0.370\n",
      "2025-05-01 17:22:30,053 - INFO - \t Val. Loss: 0.337 | Val. F1 (Macro): 0.6695\n",
      "2025-05-01 17:22:30,109 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 17:31:45,141 - INFO - Epoch: 14 | Time: 9m 15s\n",
      "2025-05-01 17:31:45,142 - INFO - \tTrain Loss: 0.368\n",
      "2025-05-01 17:31:45,142 - INFO - \t Val. Loss: 0.336 | Val. F1 (Macro): 0.6670\n",
      "2025-05-01 17:31:45,199 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 17:40:59,359 - INFO - Epoch: 15 | Time: 9m 14s\n",
      "2025-05-01 17:40:59,360 - INFO - \tTrain Loss: 0.366\n",
      "2025-05-01 17:40:59,360 - INFO - \t Val. Loss: 0.330 | Val. F1 (Macro): 0.6546\n",
      "2025-05-01 17:40:59,440 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 17:50:12,070 - INFO - Epoch: 16 | Time: 9m 13s\n",
      "2025-05-01 17:50:12,071 - INFO - \tTrain Loss: 0.362\n",
      "2025-05-01 17:50:12,071 - INFO - \t Val. Loss: 0.327 | Val. F1 (Macro): 0.6627\n",
      "2025-05-01 17:50:12,123 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 17:59:24,930 - INFO - Epoch: 17 | Time: 9m 13s\n",
      "2025-05-01 17:59:24,931 - INFO - \tTrain Loss: 0.362\n",
      "2025-05-01 17:59:24,931 - INFO - \t Val. Loss: 0.334 | Val. F1 (Macro): 0.6635\n",
      "2025-05-01 18:08:37,601 - INFO - Epoch: 18 | Time: 9m 13s\n",
      "2025-05-01 18:08:37,602 - INFO - \tTrain Loss: 0.359\n",
      "2025-05-01 18:08:37,604 - INFO - \t Val. Loss: 0.330 | Val. F1 (Macro): 0.6545\n",
      "2025-05-01 18:17:50,260 - INFO - Epoch: 19 | Time: 9m 13s\n",
      "2025-05-01 18:17:50,261 - INFO - \tTrain Loss: 0.358\n",
      "2025-05-01 18:17:50,262 - INFO - \t Val. Loss: 0.325 | Val. F1 (Macro): 0.6640\n",
      "2025-05-01 18:17:50,316 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 18:27:02,897 - INFO - Epoch: 20 | Time: 9m 13s\n",
      "2025-05-01 18:27:02,897 - INFO - \tTrain Loss: 0.357\n",
      "2025-05-01 18:27:02,898 - INFO - \t Val. Loss: 0.333 | Val. F1 (Macro): 0.6758\n",
      "2025-05-01 18:36:15,599 - INFO - Epoch: 21 | Time: 9m 13s\n",
      "2025-05-01 18:36:15,600 - INFO - \tTrain Loss: 0.354\n",
      "2025-05-01 18:36:15,600 - INFO - \t Val. Loss: 0.330 | Val. F1 (Macro): 0.6751\n",
      "2025-05-01 18:45:28,430 - INFO - Epoch: 22 | Time: 9m 13s\n",
      "2025-05-01 18:45:28,431 - INFO - \tTrain Loss: 0.353\n",
      "2025-05-01 18:45:28,432 - INFO - \t Val. Loss: 0.321 | Val. F1 (Macro): 0.6783\n",
      "2025-05-01 18:45:28,481 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 22)\n",
      "2025-05-01 18:54:40,885 - INFO - Epoch: 23 | Time: 9m 12s\n",
      "2025-05-01 18:54:40,886 - INFO - \tTrain Loss: 0.354\n",
      "2025-05-01 18:54:40,887 - INFO - \t Val. Loss: 0.323 | Val. F1 (Macro): 0.6750\n",
      "2025-05-01 19:03:52,931 - INFO - Epoch: 24 | Time: 9m 12s\n",
      "2025-05-01 19:03:52,932 - INFO - \tTrain Loss: 0.350\n",
      "2025-05-01 19:03:52,932 - INFO - \t Val. Loss: 0.322 | Val. F1 (Macro): 0.6904\n",
      "2025-05-01 19:13:06,307 - INFO - Epoch: 25 | Time: 9m 13s\n",
      "2025-05-01 19:13:06,308 - INFO - \tTrain Loss: 0.349\n",
      "2025-05-01 19:13:06,309 - INFO - \t Val. Loss: 0.324 | Val. F1 (Macro): 0.6881\n",
      "2025-05-01 19:22:20,103 - INFO - Epoch: 26 | Time: 9m 14s\n",
      "2025-05-01 19:22:20,104 - INFO - \tTrain Loss: 0.348\n",
      "2025-05-01 19:22:20,104 - INFO - \t Val. Loss: 0.321 | Val. F1 (Macro): 0.6827\n",
      "2025-05-01 19:31:36,847 - INFO - Epoch: 27 | Time: 9m 17s\n",
      "2025-05-01 19:31:36,847 - INFO - \tTrain Loss: 0.348\n",
      "2025-05-01 19:31:36,848 - INFO - \t Val. Loss: 0.320 | Val. F1 (Macro): 0.6817\n",
      "2025-05-01 19:31:36,909 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt (Epoch 27)\n",
      "2025-05-01 19:40:50,938 - INFO - Epoch: 28 | Time: 9m 14s\n",
      "2025-05-01 19:40:50,939 - INFO - \tTrain Loss: 0.347\n",
      "2025-05-01 19:40:50,940 - INFO - \t Val. Loss: 0.322 | Val. F1 (Macro): 0.6870\n",
      "2025-05-01 19:50:03,023 - INFO - Epoch: 29 | Time: 9m 12s\n",
      "2025-05-01 19:50:03,024 - INFO - \tTrain Loss: 0.345\n",
      "2025-05-01 19:50:03,024 - INFO - \t Val. Loss: 0.322 | Val. F1 (Macro): 0.6694\n",
      "2025-05-01 19:59:13,502 - INFO - Epoch: 30 | Time: 9m 10s\n",
      "2025-05-01 19:59:13,503 - INFO - \tTrain Loss: 0.343\n",
      "2025-05-01 19:59:13,503 - INFO - \t Val. Loss: 0.322 | Val. F1 (Macro): 0.6893\n",
      "2025-05-01 19:59:13,546 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 19:59:40,489 - INFO - Test Set Performance:\n",
      "2025-05-01 19:59:40,490 - INFO - \tAccuracy: 0.8800\n",
      "2025-05-01 19:59:40,490 - INFO - \tF1 (Macro): 0.6789\n",
      "2025-05-01 19:59:40,491 - INFO - \tPrecision (Macro): 0.7189\n",
      "2025-05-01 19:59:40,492 - INFO - \tRecall (Macro): 0.6572\n",
      "2025-05-01 19:59:40,492 - INFO - \tF1 (Weighted): 0.8694\n",
      "2025-05-01 19:59:40,493 - INFO - \tPrecision (Weighted): 0.8642\n",
      "2025-05-01 19:59:40,494 - INFO - \tRecall (Weighted): 0.8800\n",
      "2025-05-01 19:59:40,495 - INFO - \tTest Loss: 0.321\n",
      "2025-05-01 19:59:40,495 - INFO - \tEval Time: 26.758s\n",
      "2025-05-01 19:59:40,498 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_BiLSTM_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 19:59:41,902 - INFO - Starting training for CNN-LSTM (GloVe Emb) on Book Review\n",
      "2025-05-01 19:59:41,903 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 19:59:41,930 - INFO - Model: CNN-LSTM (GloVe Emb), Trainable Parameters: 314,135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: CNN-LSTM (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 20:01:52,810 - INFO - Epoch: 01 | Time: 2m 11s\n",
      "2025-05-01 20:01:52,811 - INFO - \tTrain Loss: 0.565\n",
      "2025-05-01 20:01:52,812 - INFO - \t Val. Loss: 0.465 | Val. F1 (Macro): 0.4815\n",
      "2025-05-01 20:01:52,862 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 20:04:02,955 - INFO - Epoch: 02 | Time: 2m 10s\n",
      "2025-05-01 20:04:02,956 - INFO - \tTrain Loss: 0.482\n",
      "2025-05-01 20:04:02,956 - INFO - \t Val. Loss: 0.441 | Val. F1 (Macro): 0.4977\n",
      "2025-05-01 20:04:03,012 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 20:06:13,510 - INFO - Epoch: 03 | Time: 2m 10s\n",
      "2025-05-01 20:06:13,511 - INFO - \tTrain Loss: 0.456\n",
      "2025-05-01 20:06:13,512 - INFO - \t Val. Loss: 0.427 | Val. F1 (Macro): 0.5161\n",
      "2025-05-01 20:06:13,566 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 20:08:24,448 - INFO - Epoch: 04 | Time: 2m 11s\n",
      "2025-05-01 20:08:24,449 - INFO - \tTrain Loss: 0.436\n",
      "2025-05-01 20:08:24,449 - INFO - \t Val. Loss: 0.389 | Val. F1 (Macro): 0.5410\n",
      "2025-05-01 20:08:24,505 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 20:10:34,411 - INFO - Epoch: 05 | Time: 2m 10s\n",
      "2025-05-01 20:10:34,412 - INFO - \tTrain Loss: 0.422\n",
      "2025-05-01 20:10:34,413 - INFO - \t Val. Loss: 0.374 | Val. F1 (Macro): 0.5781\n",
      "2025-05-01 20:10:34,465 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 20:12:44,725 - INFO - Epoch: 06 | Time: 2m 10s\n",
      "2025-05-01 20:12:44,725 - INFO - \tTrain Loss: 0.410\n",
      "2025-05-01 20:12:44,727 - INFO - \t Val. Loss: 0.385 | Val. F1 (Macro): 0.5727\n",
      "2025-05-01 20:14:55,046 - INFO - Epoch: 07 | Time: 2m 10s\n",
      "2025-05-01 20:14:55,047 - INFO - \tTrain Loss: 0.403\n",
      "2025-05-01 20:14:55,048 - INFO - \t Val. Loss: 0.381 | Val. F1 (Macro): 0.5846\n",
      "2025-05-01 20:17:04,855 - INFO - Epoch: 08 | Time: 2m 10s\n",
      "2025-05-01 20:17:04,856 - INFO - \tTrain Loss: 0.395\n",
      "2025-05-01 20:17:04,856 - INFO - \t Val. Loss: 0.354 | Val. F1 (Macro): 0.6241\n",
      "2025-05-01 20:17:04,914 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 20:19:15,472 - INFO - Epoch: 09 | Time: 2m 11s\n",
      "2025-05-01 20:19:15,472 - INFO - \tTrain Loss: 0.389\n",
      "2025-05-01 20:19:15,473 - INFO - \t Val. Loss: 0.355 | Val. F1 (Macro): 0.6196\n",
      "2025-05-01 20:21:25,568 - INFO - Epoch: 10 | Time: 2m 10s\n",
      "2025-05-01 20:21:25,569 - INFO - \tTrain Loss: 0.385\n",
      "2025-05-01 20:21:25,569 - INFO - \t Val. Loss: 0.360 | Val. F1 (Macro): 0.5978\n",
      "2025-05-01 20:23:35,809 - INFO - Epoch: 11 | Time: 2m 10s\n",
      "2025-05-01 20:23:35,810 - INFO - \tTrain Loss: 0.382\n",
      "2025-05-01 20:23:35,810 - INFO - \t Val. Loss: 0.351 | Val. F1 (Macro): 0.6366\n",
      "2025-05-01 20:23:35,865 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 11)\n",
      "2025-05-01 20:25:48,309 - INFO - Epoch: 12 | Time: 2m 12s\n",
      "2025-05-01 20:25:48,310 - INFO - \tTrain Loss: 0.378\n",
      "2025-05-01 20:25:48,311 - INFO - \t Val. Loss: 0.345 | Val. F1 (Macro): 0.6362\n",
      "2025-05-01 20:25:48,358 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 20:27:57,963 - INFO - Epoch: 13 | Time: 2m 10s\n",
      "2025-05-01 20:27:57,964 - INFO - \tTrain Loss: 0.375\n",
      "2025-05-01 20:27:57,964 - INFO - \t Val. Loss: 0.345 | Val. F1 (Macro): 0.6242\n",
      "2025-05-01 20:27:58,021 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 20:30:08,735 - INFO - Epoch: 14 | Time: 2m 11s\n",
      "2025-05-01 20:30:08,735 - INFO - \tTrain Loss: 0.372\n",
      "2025-05-01 20:30:08,736 - INFO - \t Val. Loss: 0.349 | Val. F1 (Macro): 0.6291\n",
      "2025-05-01 20:32:21,248 - INFO - Epoch: 15 | Time: 2m 13s\n",
      "2025-05-01 20:32:21,248 - INFO - \tTrain Loss: 0.369\n",
      "2025-05-01 20:32:21,249 - INFO - \t Val. Loss: 0.340 | Val. F1 (Macro): 0.6027\n",
      "2025-05-01 20:32:21,306 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 20:34:30,614 - INFO - Epoch: 16 | Time: 2m 9s\n",
      "2025-05-01 20:34:30,616 - INFO - \tTrain Loss: 0.367\n",
      "2025-05-01 20:34:30,616 - INFO - \t Val. Loss: 0.339 | Val. F1 (Macro): 0.6684\n",
      "2025-05-01 20:34:30,693 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 20:36:40,347 - INFO - Epoch: 17 | Time: 2m 10s\n",
      "2025-05-01 20:36:40,348 - INFO - \tTrain Loss: 0.365\n",
      "2025-05-01 20:36:40,348 - INFO - \t Val. Loss: 0.335 | Val. F1 (Macro): 0.6408\n",
      "2025-05-01 20:36:40,395 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 20:38:50,278 - INFO - Epoch: 18 | Time: 2m 10s\n",
      "2025-05-01 20:38:50,279 - INFO - \tTrain Loss: 0.362\n",
      "2025-05-01 20:38:50,279 - INFO - \t Val. Loss: 0.337 | Val. F1 (Macro): 0.6776\n",
      "2025-05-01 20:40:59,458 - INFO - Epoch: 19 | Time: 2m 9s\n",
      "2025-05-01 20:40:59,458 - INFO - \tTrain Loss: 0.361\n",
      "2025-05-01 20:40:59,459 - INFO - \t Val. Loss: 0.336 | Val. F1 (Macro): 0.6500\n",
      "2025-05-01 20:43:09,212 - INFO - Epoch: 20 | Time: 2m 10s\n",
      "2025-05-01 20:43:09,212 - INFO - \tTrain Loss: 0.359\n",
      "2025-05-01 20:43:09,213 - INFO - \t Val. Loss: 0.342 | Val. F1 (Macro): 0.6493\n",
      "2025-05-01 20:45:19,006 - INFO - Epoch: 21 | Time: 2m 10s\n",
      "2025-05-01 20:45:19,006 - INFO - \tTrain Loss: 0.358\n",
      "2025-05-01 20:45:19,006 - INFO - \t Val. Loss: 0.336 | Val. F1 (Macro): 0.6565\n",
      "2025-05-01 20:47:33,127 - INFO - Epoch: 22 | Time: 2m 14s\n",
      "2025-05-01 20:47:33,128 - INFO - \tTrain Loss: 0.356\n",
      "2025-05-01 20:47:33,129 - INFO - \t Val. Loss: 0.337 | Val. F1 (Macro): 0.6520\n",
      "2025-05-01 20:49:45,285 - INFO - Epoch: 23 | Time: 2m 12s\n",
      "2025-05-01 20:49:45,286 - INFO - \tTrain Loss: 0.354\n",
      "2025-05-01 20:49:45,287 - INFO - \t Val. Loss: 0.330 | Val. F1 (Macro): 0.6702\n",
      "2025-05-01 20:49:45,340 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 23)\n",
      "2025-05-01 20:51:56,249 - INFO - Epoch: 24 | Time: 2m 11s\n",
      "2025-05-01 20:51:56,250 - INFO - \tTrain Loss: 0.354\n",
      "2025-05-01 20:51:56,251 - INFO - \t Val. Loss: 0.332 | Val. F1 (Macro): 0.6721\n",
      "2025-05-01 20:54:06,830 - INFO - Epoch: 25 | Time: 2m 11s\n",
      "2025-05-01 20:54:06,830 - INFO - \tTrain Loss: 0.353\n",
      "2025-05-01 20:54:06,831 - INFO - \t Val. Loss: 0.330 | Val. F1 (Macro): 0.6446\n",
      "2025-05-01 20:56:17,635 - INFO - Epoch: 26 | Time: 2m 11s\n",
      "2025-05-01 20:56:17,636 - INFO - \tTrain Loss: 0.352\n",
      "2025-05-01 20:56:17,637 - INFO - \t Val. Loss: 0.339 | Val. F1 (Macro): 0.6654\n",
      "2025-05-01 20:58:28,930 - INFO - Epoch: 27 | Time: 2m 11s\n",
      "2025-05-01 20:58:28,931 - INFO - \tTrain Loss: 0.349\n",
      "2025-05-01 20:58:28,932 - INFO - \t Val. Loss: 0.328 | Val. F1 (Macro): 0.6762\n",
      "2025-05-01 20:58:28,989 - INFO - Saved best model to ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 27)\n",
      "2025-05-01 21:00:40,127 - INFO - Epoch: 28 | Time: 2m 11s\n",
      "2025-05-01 21:00:40,128 - INFO - \tTrain Loss: 0.349\n",
      "2025-05-01 21:00:40,129 - INFO - \t Val. Loss: 0.330 | Val. F1 (Macro): 0.6766\n",
      "2025-05-01 21:02:51,597 - INFO - Epoch: 29 | Time: 2m 11s\n",
      "2025-05-01 21:02:51,598 - INFO - \tTrain Loss: 0.347\n",
      "2025-05-01 21:02:51,599 - INFO - \t Val. Loss: 0.343 | Val. F1 (Macro): 0.6518\n",
      "2025-05-01 21:05:02,060 - INFO - Epoch: 30 | Time: 2m 10s\n",
      "2025-05-01 21:05:02,061 - INFO - \tTrain Loss: 0.346\n",
      "2025-05-01 21:05:02,062 - INFO - \t Val. Loss: 0.337 | Val. F1 (Macro): 0.6677\n",
      "2025-05-01 21:05:02,096 - INFO - Loaded best model from ..\\models\\dl\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:05:11,764 - INFO - Test Set Performance:\n",
      "2025-05-01 21:05:11,764 - INFO - \tAccuracy: 0.8783\n",
      "2025-05-01 21:05:11,765 - INFO - \tF1 (Macro): 0.6723\n",
      "2025-05-01 21:05:11,766 - INFO - \tPrecision (Macro): 0.7255\n",
      "2025-05-01 21:05:11,766 - INFO - \tRecall (Macro): 0.6400\n",
      "2025-05-01 21:05:11,767 - INFO - \tF1 (Weighted): 0.8662\n",
      "2025-05-01 21:05:11,768 - INFO - \tPrecision (Weighted): 0.8610\n",
      "2025-05-01 21:05:11,768 - INFO - \tRecall (Weighted): 0.8783\n",
      "2025-05-01 21:05:11,769 - INFO - \tTest Loss: 0.332\n",
      "2025-05-01 21:05:11,770 - INFO - \tEval Time: 9.488s\n",
      "2025-05-01 21:05:11,773 - INFO - Saved confusion matrix CSV to ..\\result\\book_reviews\\Book_Review_CNN-LSTM_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:05:12,600 - INFO - Processing Dataset: Financial News\n",
      "2025-05-01 21:05:12,652 - INFO - Built vocabulary with 2845 words (min freq: 3).\n",
      "2025-05-01 21:05:12,661 - INFO - Built and saved vocabulary to ..\\models\\dl\\financial_news\\vocab.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Processing Dataset: Financial News ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:05:12,825 - INFO - Loading GloVe embeddings from ..\\data\\embeddings\\glove.6B.100d.txt\n",
      "2025-05-01 21:05:18,364 - INFO - Found 400000 word vectors in GloVe file.\n",
      "2025-05-01 21:05:18,368 - INFO - Initialized embedding matrix. Shape: (2845, 100)\n",
      "2025-05-01 21:05:18,369 - INFO - Found pre-trained vectors for 2672/2845 words in vocabulary.\n",
      "2025-05-01 21:05:18,468 - INFO - Starting training for MLP (Avg Learned Emb) on Financial News\n",
      "2025-05-01 21:05:18,473 - INFO - Using learned embeddings.\n",
      "2025-05-01 21:05:18,476 - INFO - Model: MLP (Avg Learned Emb), Trainable Parameters: 293,143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: MLP (Avg Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:05:18,876 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:05:18,877 - INFO - \tTrain Loss: 0.974\n",
      "2025-05-01 21:05:18,877 - INFO - \t Val. Loss: 0.871 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:18,882 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:05:19,279 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:05:19,279 - INFO - \tTrain Loss: 0.890\n",
      "2025-05-01 21:05:19,280 - INFO - \t Val. Loss: 0.834 | Val. F1 (Macro): 0.3005\n",
      "2025-05-01 21:05:19,284 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:05:19,615 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:05:19,616 - INFO - \tTrain Loss: 0.852\n",
      "2025-05-01 21:05:19,616 - INFO - \t Val. Loss: 0.815 | Val. F1 (Macro): 0.3608\n",
      "2025-05-01 21:05:19,620 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:05:19,962 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:05:19,963 - INFO - \tTrain Loss: 0.818\n",
      "2025-05-01 21:05:19,963 - INFO - \t Val. Loss: 0.793 | Val. F1 (Macro): 0.3978\n",
      "2025-05-01 21:05:19,967 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:05:20,313 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:05:20,314 - INFO - \tTrain Loss: 0.776\n",
      "2025-05-01 21:05:20,315 - INFO - \t Val. Loss: 0.772 | Val. F1 (Macro): 0.4122\n",
      "2025-05-01 21:05:20,319 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:05:20,647 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:05:20,648 - INFO - \tTrain Loss: 0.744\n",
      "2025-05-01 21:05:20,648 - INFO - \t Val. Loss: 0.757 | Val. F1 (Macro): 0.4235\n",
      "2025-05-01 21:05:20,652 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 21:05:20,987 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:05:20,988 - INFO - \tTrain Loss: 0.700\n",
      "2025-05-01 21:05:20,988 - INFO - \t Val. Loss: 0.751 | Val. F1 (Macro): 0.4317\n",
      "2025-05-01 21:05:20,992 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:05:21,314 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:05:21,315 - INFO - \tTrain Loss: 0.657\n",
      "2025-05-01 21:05:21,315 - INFO - \t Val. Loss: 0.748 | Val. F1 (Macro): 0.4486\n",
      "2025-05-01 21:05:21,319 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 21:05:21,624 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:05:21,624 - INFO - \tTrain Loss: 0.617\n",
      "2025-05-01 21:05:21,624 - INFO - \t Val. Loss: 0.753 | Val. F1 (Macro): 0.4489\n",
      "2025-05-01 21:05:21,899 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:05:21,899 - INFO - \tTrain Loss: 0.577\n",
      "2025-05-01 21:05:21,899 - INFO - \t Val. Loss: 0.767 | Val. F1 (Macro): 0.4572\n",
      "2025-05-01 21:05:22,179 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:05:22,179 - INFO - \tTrain Loss: 0.553\n",
      "2025-05-01 21:05:22,180 - INFO - \t Val. Loss: 0.776 | Val. F1 (Macro): 0.4594\n",
      "2025-05-01 21:05:22,451 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:05:22,452 - INFO - \tTrain Loss: 0.502\n",
      "2025-05-01 21:05:22,452 - INFO - \t Val. Loss: 0.818 | Val. F1 (Macro): 0.4551\n",
      "2025-05-01 21:05:22,782 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:05:22,783 - INFO - \tTrain Loss: 0.478\n",
      "2025-05-01 21:05:22,783 - INFO - \t Val. Loss: 0.841 | Val. F1 (Macro): 0.4518\n",
      "2025-05-01 21:05:23,126 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:05:23,127 - INFO - \tTrain Loss: 0.453\n",
      "2025-05-01 21:05:23,127 - INFO - \t Val. Loss: 0.888 | Val. F1 (Macro): 0.4568\n",
      "2025-05-01 21:05:23,439 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:05:23,439 - INFO - \tTrain Loss: 0.424\n",
      "2025-05-01 21:05:23,440 - INFO - \t Val. Loss: 0.924 | Val. F1 (Macro): 0.4542\n",
      "2025-05-01 21:05:23,770 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:05:23,770 - INFO - \tTrain Loss: 0.408\n",
      "2025-05-01 21:05:23,771 - INFO - \t Val. Loss: 0.947 | Val. F1 (Macro): 0.4475\n",
      "2025-05-01 21:05:24,110 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:05:24,111 - INFO - \tTrain Loss: 0.386\n",
      "2025-05-01 21:05:24,112 - INFO - \t Val. Loss: 0.979 | Val. F1 (Macro): 0.5095\n",
      "2025-05-01 21:05:24,428 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:05:24,429 - INFO - \tTrain Loss: 0.356\n",
      "2025-05-01 21:05:24,430 - INFO - \t Val. Loss: 1.031 | Val. F1 (Macro): 0.5621\n",
      "2025-05-01 21:05:24,749 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:05:24,750 - INFO - \tTrain Loss: 0.334\n",
      "2025-05-01 21:05:24,750 - INFO - \t Val. Loss: 1.106 | Val. F1 (Macro): 0.5710\n",
      "2025-05-01 21:05:25,089 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:05:25,090 - INFO - \tTrain Loss: 0.308\n",
      "2025-05-01 21:05:25,090 - INFO - \t Val. Loss: 1.165 | Val. F1 (Macro): 0.5746\n",
      "2025-05-01 21:05:25,398 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:05:25,399 - INFO - \tTrain Loss: 0.286\n",
      "2025-05-01 21:05:25,399 - INFO - \t Val. Loss: 1.184 | Val. F1 (Macro): 0.5863\n",
      "2025-05-01 21:05:25,726 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:05:25,727 - INFO - \tTrain Loss: 0.269\n",
      "2025-05-01 21:05:25,728 - INFO - \t Val. Loss: 1.253 | Val. F1 (Macro): 0.5882\n",
      "2025-05-01 21:05:26,079 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:05:26,080 - INFO - \tTrain Loss: 0.241\n",
      "2025-05-01 21:05:26,080 - INFO - \t Val. Loss: 1.303 | Val. F1 (Macro): 0.5783\n",
      "2025-05-01 21:05:26,380 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:05:26,381 - INFO - \tTrain Loss: 0.217\n",
      "2025-05-01 21:05:26,381 - INFO - \t Val. Loss: 1.363 | Val. F1 (Macro): 0.5816\n",
      "2025-05-01 21:05:26,708 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:05:26,708 - INFO - \tTrain Loss: 0.209\n",
      "2025-05-01 21:05:26,709 - INFO - \t Val. Loss: 1.431 | Val. F1 (Macro): 0.5875\n",
      "2025-05-01 21:05:27,042 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:05:27,043 - INFO - \tTrain Loss: 0.187\n",
      "2025-05-01 21:05:27,043 - INFO - \t Val. Loss: 1.480 | Val. F1 (Macro): 0.5995\n",
      "2025-05-01 21:05:27,375 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:05:27,376 - INFO - \tTrain Loss: 0.168\n",
      "2025-05-01 21:05:27,377 - INFO - \t Val. Loss: 1.599 | Val. F1 (Macro): 0.6087\n",
      "2025-05-01 21:05:27,698 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:05:27,699 - INFO - \tTrain Loss: 0.150\n",
      "2025-05-01 21:05:27,699 - INFO - \t Val. Loss: 1.628 | Val. F1 (Macro): 0.6160\n",
      "2025-05-01 21:05:27,967 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:05:27,968 - INFO - \tTrain Loss: 0.153\n",
      "2025-05-01 21:05:27,968 - INFO - \t Val. Loss: 1.686 | Val. F1 (Macro): 0.6105\n",
      "2025-05-01 21:05:28,260 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:05:28,261 - INFO - \tTrain Loss: 0.135\n",
      "2025-05-01 21:05:28,262 - INFO - \t Val. Loss: 1.739 | Val. F1 (Macro): 0.6261\n",
      "2025-05-01 21:05:28,274 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:05:28,310 - INFO - Test Set Performance:\n",
      "2025-05-01 21:05:28,311 - INFO - \tAccuracy: 0.6492\n",
      "2025-05-01 21:05:28,312 - INFO - \tF1 (Macro): 0.4223\n",
      "2025-05-01 21:05:28,313 - INFO - \tPrecision (Macro): 0.3961\n",
      "2025-05-01 21:05:28,313 - INFO - \tRecall (Macro): 0.4522\n",
      "2025-05-01 21:05:28,314 - INFO - \tF1 (Weighted): 0.6050\n",
      "2025-05-01 21:05:28,314 - INFO - \tPrecision (Weighted): 0.5665\n",
      "2025-05-01 21:05:28,315 - INFO - \tRecall (Weighted): 0.6492\n",
      "2025-05-01 21:05:28,316 - INFO - \tTest Loss: 0.803\n",
      "2025-05-01 21:05:28,316 - INFO - \tEval Time: 0.028s\n",
      "2025-05-01 21:05:28,319 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_MLP_(Avg_Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:05:28,400 - INFO - Starting training for RNN (Learned Emb) on Financial News\n",
      "2025-05-01 21:05:28,402 - INFO - Using learned embeddings.\n",
      "2025-05-01 21:05:28,405 - INFO - Model: RNN (Learned Emb), Trainable Parameters: 311,959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: RNN (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:05:28,842 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:05:28,843 - INFO - \tTrain Loss: 0.953\n",
      "2025-05-01 21:05:28,843 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:28,847 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:05:29,209 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:05:29,210 - INFO - \tTrain Loss: 0.942\n",
      "2025-05-01 21:05:29,210 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:29,214 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:05:29,547 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:05:29,548 - INFO - \tTrain Loss: 0.934\n",
      "2025-05-01 21:05:29,548 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:29,844 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:05:29,845 - INFO - \tTrain Loss: 0.938\n",
      "2025-05-01 21:05:29,846 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:30,183 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:05:30,184 - INFO - \tTrain Loss: 0.932\n",
      "2025-05-01 21:05:30,184 - INFO - \t Val. Loss: 0.917 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:30,511 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:05:30,512 - INFO - \tTrain Loss: 0.933\n",
      "2025-05-01 21:05:30,512 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:30,853 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:05:30,854 - INFO - \tTrain Loss: 0.933\n",
      "2025-05-01 21:05:30,855 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:31,165 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:05:31,166 - INFO - \tTrain Loss: 0.932\n",
      "2025-05-01 21:05:31,166 - INFO - \t Val. Loss: 0.922 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:31,472 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:05:31,472 - INFO - \tTrain Loss: 0.932\n",
      "2025-05-01 21:05:31,473 - INFO - \t Val. Loss: 0.923 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:31,797 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:05:31,798 - INFO - \tTrain Loss: 0.933\n",
      "2025-05-01 21:05:31,798 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:32,087 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:05:32,088 - INFO - \tTrain Loss: 0.930\n",
      "2025-05-01 21:05:32,088 - INFO - \t Val. Loss: 0.917 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:32,414 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:05:32,415 - INFO - \tTrain Loss: 0.929\n",
      "2025-05-01 21:05:32,415 - INFO - \t Val. Loss: 0.935 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:32,736 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:05:32,737 - INFO - \tTrain Loss: 0.930\n",
      "2025-05-01 21:05:32,737 - INFO - \t Val. Loss: 0.919 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:33,085 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:05:33,086 - INFO - \tTrain Loss: 0.932\n",
      "2025-05-01 21:05:33,086 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:33,357 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:05:33,358 - INFO - \tTrain Loss: 0.932\n",
      "2025-05-01 21:05:33,358 - INFO - \t Val. Loss: 0.918 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:33,636 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:05:33,637 - INFO - \tTrain Loss: 0.930\n",
      "2025-05-01 21:05:33,637 - INFO - \t Val. Loss: 0.919 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:33,918 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:05:33,918 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:33,919 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:33,922 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 21:05:34,240 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:05:34,241 - INFO - \tTrain Loss: 0.927\n",
      "2025-05-01 21:05:34,241 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:34,246 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt (Epoch 18)\n",
      "2025-05-01 21:05:34,566 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:05:34,566 - INFO - \tTrain Loss: 0.929\n",
      "2025-05-01 21:05:34,567 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2519\n",
      "2025-05-01 21:05:34,570 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 21:05:34,847 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:05:34,847 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:34,848 - INFO - \t Val. Loss: 0.926 | Val. F1 (Macro): 0.2519\n",
      "2025-05-01 21:05:35,140 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:05:35,141 - INFO - \tTrain Loss: 0.931\n",
      "2025-05-01 21:05:35,141 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2519\n",
      "2025-05-01 21:05:35,461 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:05:35,461 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:35,462 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:35,776 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:05:35,777 - INFO - \tTrain Loss: 0.929\n",
      "2025-05-01 21:05:35,777 - INFO - \t Val. Loss: 0.917 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:36,101 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:05:36,102 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:36,103 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:36,409 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:05:36,410 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:36,410 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:36,730 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:05:36,730 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:36,731 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:37,082 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:05:37,083 - INFO - \tTrain Loss: 0.926\n",
      "2025-05-01 21:05:37,084 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:37,412 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:05:37,413 - INFO - \tTrain Loss: 0.929\n",
      "2025-05-01 21:05:37,413 - INFO - \t Val. Loss: 0.916 | Val. F1 (Macro): 0.2519\n",
      "2025-05-01 21:05:37,737 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:05:37,738 - INFO - \tTrain Loss: 0.926\n",
      "2025-05-01 21:05:37,738 - INFO - \t Val. Loss: 0.915 | Val. F1 (Macro): 0.2519\n",
      "2025-05-01 21:05:37,742 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt (Epoch 29)\n",
      "2025-05-01 21:05:38,096 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:05:38,097 - INFO - \tTrain Loss: 0.928\n",
      "2025-05-01 21:05:38,098 - INFO - \t Val. Loss: 0.917 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:38,111 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_RNN_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:05:38,149 - INFO - Test Set Performance:\n",
      "2025-05-01 21:05:38,149 - INFO - \tAccuracy: 0.5956\n",
      "2025-05-01 21:05:38,150 - INFO - \tF1 (Macro): 0.2550\n",
      "2025-05-01 21:05:38,150 - INFO - \tPrecision (Macro): 0.4207\n",
      "2025-05-01 21:05:38,151 - INFO - \tRecall (Macro): 0.3358\n",
      "2025-05-01 21:05:38,152 - INFO - \tF1 (Weighted): 0.4485\n",
      "2025-05-01 21:05:38,152 - INFO - \tPrecision (Weighted): 0.5408\n",
      "2025-05-01 21:05:38,153 - INFO - \tRecall (Weighted): 0.5956\n",
      "2025-05-01 21:05:38,153 - INFO - \tTest Loss: 0.937\n",
      "2025-05-01 21:05:38,154 - INFO - \tEval Time: 0.029s\n",
      "2025-05-01 21:05:38,157 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_RNN_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:05:38,238 - INFO - Starting training for LSTM (Learned Emb) on Financial News\n",
      "2025-05-01 21:05:38,240 - INFO - Using learned embeddings.\n",
      "2025-05-01 21:05:38,245 - INFO - Model: LSTM (Learned Emb), Trainable Parameters: 393,751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: LSTM (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:05:38,617 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:05:38,617 - INFO - \tTrain Loss: 0.965\n",
      "2025-05-01 21:05:38,618 - INFO - \t Val. Loss: 0.915 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:38,622 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:05:38,961 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:05:38,962 - INFO - \tTrain Loss: 0.926\n",
      "2025-05-01 21:05:38,962 - INFO - \t Val. Loss: 0.902 | Val. F1 (Macro): 0.2954\n",
      "2025-05-01 21:05:38,967 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:05:39,287 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:05:39,288 - INFO - \tTrain Loss: 0.884\n",
      "2025-05-01 21:05:39,288 - INFO - \t Val. Loss: 0.870 | Val. F1 (Macro): 0.3716\n",
      "2025-05-01 21:05:39,292 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:05:39,610 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:05:39,611 - INFO - \tTrain Loss: 0.859\n",
      "2025-05-01 21:05:39,612 - INFO - \t Val. Loss: 0.844 | Val. F1 (Macro): 0.3703\n",
      "2025-05-01 21:05:39,616 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:05:39,948 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:05:39,949 - INFO - \tTrain Loss: 0.851\n",
      "2025-05-01 21:05:39,949 - INFO - \t Val. Loss: 0.858 | Val. F1 (Macro): 0.3989\n",
      "2025-05-01 21:05:40,267 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:05:40,268 - INFO - \tTrain Loss: 0.851\n",
      "2025-05-01 21:05:40,268 - INFO - \t Val. Loss: 0.857 | Val. F1 (Macro): 0.4071\n",
      "2025-05-01 21:05:40,590 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:05:40,591 - INFO - \tTrain Loss: 0.832\n",
      "2025-05-01 21:05:40,592 - INFO - \t Val. Loss: 0.844 | Val. F1 (Macro): 0.3970\n",
      "2025-05-01 21:05:40,596 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:05:40,909 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:05:40,910 - INFO - \tTrain Loss: 0.827\n",
      "2025-05-01 21:05:40,911 - INFO - \t Val. Loss: 0.893 | Val. F1 (Macro): 0.4002\n",
      "2025-05-01 21:05:41,234 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:05:41,234 - INFO - \tTrain Loss: 0.811\n",
      "2025-05-01 21:05:41,235 - INFO - \t Val. Loss: 0.835 | Val. F1 (Macro): 0.4108\n",
      "2025-05-01 21:05:41,239 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 21:05:41,555 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:05:41,556 - INFO - \tTrain Loss: 0.800\n",
      "2025-05-01 21:05:41,556 - INFO - \t Val. Loss: 0.829 | Val. F1 (Macro): 0.4138\n",
      "2025-05-01 21:05:41,560 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 10)\n",
      "2025-05-01 21:05:41,893 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:05:41,894 - INFO - \tTrain Loss: 0.774\n",
      "2025-05-01 21:05:41,894 - INFO - \t Val. Loss: 0.841 | Val. F1 (Macro): 0.4134\n",
      "2025-05-01 21:05:42,208 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:05:42,209 - INFO - \tTrain Loss: 0.778\n",
      "2025-05-01 21:05:42,209 - INFO - \t Val. Loss: 0.820 | Val. F1 (Macro): 0.4162\n",
      "2025-05-01 21:05:42,214 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:05:42,515 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:05:42,516 - INFO - \tTrain Loss: 0.761\n",
      "2025-05-01 21:05:42,517 - INFO - \t Val. Loss: 0.816 | Val. F1 (Macro): 0.4217\n",
      "2025-05-01 21:05:42,521 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 21:05:42,829 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:05:42,830 - INFO - \tTrain Loss: 0.735\n",
      "2025-05-01 21:05:42,831 - INFO - \t Val. Loss: 0.806 | Val. F1 (Macro): 0.4239\n",
      "2025-05-01 21:05:42,834 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 21:05:43,157 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:05:43,158 - INFO - \tTrain Loss: 0.725\n",
      "2025-05-01 21:05:43,159 - INFO - \t Val. Loss: 0.818 | Val. F1 (Macro): 0.4353\n",
      "2025-05-01 21:05:43,464 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:05:43,464 - INFO - \tTrain Loss: 0.722\n",
      "2025-05-01 21:05:43,465 - INFO - \t Val. Loss: 0.851 | Val. F1 (Macro): 0.4390\n",
      "2025-05-01 21:05:43,765 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:05:43,766 - INFO - \tTrain Loss: 0.723\n",
      "2025-05-01 21:05:43,766 - INFO - \t Val. Loss: 0.792 | Val. F1 (Macro): 0.4152\n",
      "2025-05-01 21:05:43,771 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 21:05:44,092 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:05:44,093 - INFO - \tTrain Loss: 0.709\n",
      "2025-05-01 21:05:44,094 - INFO - \t Val. Loss: 0.842 | Val. F1 (Macro): 0.4319\n",
      "2025-05-01 21:05:44,403 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:05:44,403 - INFO - \tTrain Loss: 0.677\n",
      "2025-05-01 21:05:44,404 - INFO - \t Val. Loss: 0.873 | Val. F1 (Macro): 0.4465\n",
      "2025-05-01 21:05:44,707 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:05:44,707 - INFO - \tTrain Loss: 0.674\n",
      "2025-05-01 21:05:44,709 - INFO - \t Val. Loss: 0.885 | Val. F1 (Macro): 0.4423\n",
      "2025-05-01 21:05:45,037 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:05:45,038 - INFO - \tTrain Loss: 0.663\n",
      "2025-05-01 21:05:45,038 - INFO - \t Val. Loss: 0.797 | Val. F1 (Macro): 0.4492\n",
      "2025-05-01 21:05:45,333 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:05:45,334 - INFO - \tTrain Loss: 0.660\n",
      "2025-05-01 21:05:45,335 - INFO - \t Val. Loss: 0.842 | Val. F1 (Macro): 0.4422\n",
      "2025-05-01 21:05:45,614 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:05:45,615 - INFO - \tTrain Loss: 0.647\n",
      "2025-05-01 21:05:45,615 - INFO - \t Val. Loss: 0.801 | Val. F1 (Macro): 0.4447\n",
      "2025-05-01 21:05:45,869 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:05:45,870 - INFO - \tTrain Loss: 0.639\n",
      "2025-05-01 21:05:45,870 - INFO - \t Val. Loss: 0.863 | Val. F1 (Macro): 0.4451\n",
      "2025-05-01 21:05:46,121 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:05:46,121 - INFO - \tTrain Loss: 0.632\n",
      "2025-05-01 21:05:46,122 - INFO - \t Val. Loss: 0.851 | Val. F1 (Macro): 0.4341\n",
      "2025-05-01 21:05:46,385 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:05:46,386 - INFO - \tTrain Loss: 0.611\n",
      "2025-05-01 21:05:46,386 - INFO - \t Val. Loss: 0.852 | Val. F1 (Macro): 0.4445\n",
      "2025-05-01 21:05:46,644 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:05:46,645 - INFO - \tTrain Loss: 0.607\n",
      "2025-05-01 21:05:46,645 - INFO - \t Val. Loss: 0.854 | Val. F1 (Macro): 0.4473\n",
      "2025-05-01 21:05:46,897 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:05:46,898 - INFO - \tTrain Loss: 0.599\n",
      "2025-05-01 21:05:46,899 - INFO - \t Val. Loss: 0.787 | Val. F1 (Macro): 0.4604\n",
      "2025-05-01 21:05:46,905 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt (Epoch 28)\n",
      "2025-05-01 21:05:47,215 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:05:47,216 - INFO - \tTrain Loss: 0.585\n",
      "2025-05-01 21:05:47,217 - INFO - \t Val. Loss: 0.884 | Val. F1 (Macro): 0.4517\n",
      "2025-05-01 21:05:47,533 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:05:47,534 - INFO - \tTrain Loss: 0.590\n",
      "2025-05-01 21:05:47,534 - INFO - \t Val. Loss: 0.824 | Val. F1 (Macro): 0.4599\n",
      "2025-05-01 21:05:47,549 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_LSTM_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:05:47,588 - INFO - Test Set Performance:\n",
      "2025-05-01 21:05:47,588 - INFO - \tAccuracy: 0.6699\n",
      "2025-05-01 21:05:47,589 - INFO - \tF1 (Macro): 0.4513\n",
      "2025-05-01 21:05:47,589 - INFO - \tPrecision (Macro): 0.4225\n",
      "2025-05-01 21:05:47,590 - INFO - \tRecall (Macro): 0.4905\n",
      "2025-05-01 21:05:47,591 - INFO - \tF1 (Weighted): 0.6340\n",
      "2025-05-01 21:05:47,591 - INFO - \tPrecision (Weighted): 0.6069\n",
      "2025-05-01 21:05:47,592 - INFO - \tRecall (Weighted): 0.6699\n",
      "2025-05-01 21:05:47,592 - INFO - \tTest Loss: 0.815\n",
      "2025-05-01 21:05:47,593 - INFO - \tEval Time: 0.030s\n",
      "2025-05-01 21:05:47,597 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_LSTM_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:05:47,676 - INFO - Starting training for BiLSTM (Learned Emb) on Financial News\n",
      "2025-05-01 21:05:47,679 - INFO - Using learned embeddings.\n",
      "2025-05-01 21:05:47,685 - INFO - Model: BiLSTM (Learned Emb), Trainable Parameters: 568,535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: BiLSTM (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:05:48,098 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:05:48,099 - INFO - \tTrain Loss: 0.937\n",
      "2025-05-01 21:05:48,100 - INFO - \t Val. Loss: 0.882 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:05:48,107 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:05:48,486 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:05:48,487 - INFO - \tTrain Loss: 0.875\n",
      "2025-05-01 21:05:48,488 - INFO - \t Val. Loss: 0.809 | Val. F1 (Macro): 0.3985\n",
      "2025-05-01 21:05:48,492 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:05:48,855 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:05:48,856 - INFO - \tTrain Loss: 0.820\n",
      "2025-05-01 21:05:48,856 - INFO - \t Val. Loss: 0.773 | Val. F1 (Macro): 0.4011\n",
      "2025-05-01 21:05:48,861 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:05:49,239 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:05:49,240 - INFO - \tTrain Loss: 0.788\n",
      "2025-05-01 21:05:49,240 - INFO - \t Val. Loss: 0.792 | Val. F1 (Macro): 0.4897\n",
      "2025-05-01 21:05:49,569 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:05:49,570 - INFO - \tTrain Loss: 0.769\n",
      "2025-05-01 21:05:49,570 - INFO - \t Val. Loss: 0.767 | Val. F1 (Macro): 0.4430\n",
      "2025-05-01 21:05:49,574 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:05:49,885 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:05:49,886 - INFO - \tTrain Loss: 0.737\n",
      "2025-05-01 21:05:49,886 - INFO - \t Val. Loss: 0.778 | Val. F1 (Macro): 0.4511\n",
      "2025-05-01 21:05:50,200 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:05:50,200 - INFO - \tTrain Loss: 0.720\n",
      "2025-05-01 21:05:50,201 - INFO - \t Val. Loss: 0.766 | Val. F1 (Macro): 0.5058\n",
      "2025-05-01 21:05:50,205 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:05:50,505 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:05:50,506 - INFO - \tTrain Loss: 0.710\n",
      "2025-05-01 21:05:50,506 - INFO - \t Val. Loss: 0.747 | Val. F1 (Macro): 0.4656\n",
      "2025-05-01 21:05:50,510 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 21:05:50,825 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:05:50,826 - INFO - \tTrain Loss: 0.684\n",
      "2025-05-01 21:05:50,826 - INFO - \t Val. Loss: 0.775 | Val. F1 (Macro): 0.4585\n",
      "2025-05-01 21:05:51,136 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:05:51,137 - INFO - \tTrain Loss: 0.665\n",
      "2025-05-01 21:05:51,137 - INFO - \t Val. Loss: 0.749 | Val. F1 (Macro): 0.5675\n",
      "2025-05-01 21:05:51,452 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:05:51,453 - INFO - \tTrain Loss: 0.647\n",
      "2025-05-01 21:05:51,453 - INFO - \t Val. Loss: 0.787 | Val. F1 (Macro): 0.5616\n",
      "2025-05-01 21:05:51,778 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:05:51,779 - INFO - \tTrain Loss: 0.615\n",
      "2025-05-01 21:05:51,779 - INFO - \t Val. Loss: 0.791 | Val. F1 (Macro): 0.6138\n",
      "2025-05-01 21:05:52,136 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:05:52,137 - INFO - \tTrain Loss: 0.598\n",
      "2025-05-01 21:05:52,138 - INFO - \t Val. Loss: 0.820 | Val. F1 (Macro): 0.5960\n",
      "2025-05-01 21:05:52,500 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:05:52,501 - INFO - \tTrain Loss: 0.584\n",
      "2025-05-01 21:05:52,501 - INFO - \t Val. Loss: 0.818 | Val. F1 (Macro): 0.6135\n",
      "2025-05-01 21:05:52,855 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:05:52,856 - INFO - \tTrain Loss: 0.552\n",
      "2025-05-01 21:05:52,856 - INFO - \t Val. Loss: 0.762 | Val. F1 (Macro): 0.5758\n",
      "2025-05-01 21:05:53,238 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:05:53,238 - INFO - \tTrain Loss: 0.527\n",
      "2025-05-01 21:05:53,239 - INFO - \t Val. Loss: 0.737 | Val. F1 (Macro): 0.6720\n",
      "2025-05-01 21:05:53,243 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 21:05:53,546 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:05:53,547 - INFO - \tTrain Loss: 0.521\n",
      "2025-05-01 21:05:53,548 - INFO - \t Val. Loss: 0.760 | Val. F1 (Macro): 0.6557\n",
      "2025-05-01 21:05:53,874 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:05:53,875 - INFO - \tTrain Loss: 0.497\n",
      "2025-05-01 21:05:53,875 - INFO - \t Val. Loss: 0.755 | Val. F1 (Macro): 0.6769\n",
      "2025-05-01 21:05:54,236 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:05:54,236 - INFO - \tTrain Loss: 0.485\n",
      "2025-05-01 21:05:54,237 - INFO - \t Val. Loss: 0.787 | Val. F1 (Macro): 0.6810\n",
      "2025-05-01 21:05:54,544 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:05:54,545 - INFO - \tTrain Loss: 0.467\n",
      "2025-05-01 21:05:54,545 - INFO - \t Val. Loss: 0.776 | Val. F1 (Macro): 0.6564\n",
      "2025-05-01 21:05:54,847 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:05:54,847 - INFO - \tTrain Loss: 0.463\n",
      "2025-05-01 21:05:54,848 - INFO - \t Val. Loss: 0.742 | Val. F1 (Macro): 0.6800\n",
      "2025-05-01 21:05:55,153 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:05:55,154 - INFO - \tTrain Loss: 0.432\n",
      "2025-05-01 21:05:55,154 - INFO - \t Val. Loss: 0.751 | Val. F1 (Macro): 0.6881\n",
      "2025-05-01 21:05:55,470 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:05:55,470 - INFO - \tTrain Loss: 0.420\n",
      "2025-05-01 21:05:55,471 - INFO - \t Val. Loss: 0.791 | Val. F1 (Macro): 0.6929\n",
      "2025-05-01 21:05:55,774 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:05:55,774 - INFO - \tTrain Loss: 0.427\n",
      "2025-05-01 21:05:55,775 - INFO - \t Val. Loss: 0.751 | Val. F1 (Macro): 0.7113\n",
      "2025-05-01 21:05:56,087 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:05:56,088 - INFO - \tTrain Loss: 0.395\n",
      "2025-05-01 21:05:56,088 - INFO - \t Val. Loss: 0.800 | Val. F1 (Macro): 0.6931\n",
      "2025-05-01 21:05:56,398 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:05:56,399 - INFO - \tTrain Loss: 0.412\n",
      "2025-05-01 21:05:56,399 - INFO - \t Val. Loss: 0.746 | Val. F1 (Macro): 0.7046\n",
      "2025-05-01 21:05:56,781 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:05:56,782 - INFO - \tTrain Loss: 0.372\n",
      "2025-05-01 21:05:56,782 - INFO - \t Val. Loss: 0.783 | Val. F1 (Macro): 0.6878\n",
      "2025-05-01 21:05:57,156 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:05:57,157 - INFO - \tTrain Loss: 0.363\n",
      "2025-05-01 21:05:57,157 - INFO - \t Val. Loss: 0.808 | Val. F1 (Macro): 0.6963\n",
      "2025-05-01 21:05:57,520 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:05:57,521 - INFO - \tTrain Loss: 0.366\n",
      "2025-05-01 21:05:57,521 - INFO - \t Val. Loss: 0.856 | Val. F1 (Macro): 0.7023\n",
      "2025-05-01 21:05:57,904 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:05:57,906 - INFO - \tTrain Loss: 0.344\n",
      "2025-05-01 21:05:57,906 - INFO - \t Val. Loss: 0.815 | Val. F1 (Macro): 0.7040\n",
      "2025-05-01 21:05:57,925 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:05:57,970 - INFO - Test Set Performance:\n",
      "2025-05-01 21:05:57,970 - INFO - \tAccuracy: 0.7345\n",
      "2025-05-01 21:05:57,971 - INFO - \tF1 (Macro): 0.6600\n",
      "2025-05-01 21:05:57,971 - INFO - \tPrecision (Macro): 0.6834\n",
      "2025-05-01 21:05:57,972 - INFO - \tRecall (Macro): 0.6437\n",
      "2025-05-01 21:05:57,972 - INFO - \tF1 (Weighted): 0.7271\n",
      "2025-05-01 21:05:57,973 - INFO - \tPrecision (Weighted): 0.7249\n",
      "2025-05-01 21:05:57,973 - INFO - \tRecall (Weighted): 0.7345\n",
      "2025-05-01 21:05:57,974 - INFO - \tTest Loss: 0.772\n",
      "2025-05-01 21:05:57,974 - INFO - \tEval Time: 0.036s\n",
      "2025-05-01 21:05:57,976 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_BiLSTM_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:05:58,064 - INFO - Starting training for CNN (Learned Emb) on Financial News\n",
      "2025-05-01 21:05:58,066 - INFO - Using learned embeddings.\n",
      "2025-05-01 21:05:58,068 - INFO - Model: CNN (Learned Emb), Trainable Parameters: 405,703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: CNN (Learned Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:05:59,756 - INFO - Epoch: 01 | Time: 0m 2s\n",
      "2025-05-01 21:05:59,757 - INFO - \tTrain Loss: 1.094\n",
      "2025-05-01 21:05:59,757 - INFO - \t Val. Loss: 0.811 | Val. F1 (Macro): 0.4216\n",
      "2025-05-01 21:05:59,762 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:06:00,248 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:06:00,249 - INFO - \tTrain Loss: 0.954\n",
      "2025-05-01 21:06:00,249 - INFO - \t Val. Loss: 0.786 | Val. F1 (Macro): 0.4705\n",
      "2025-05-01 21:06:00,253 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:06:00,602 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:06:00,602 - INFO - \tTrain Loss: 0.892\n",
      "2025-05-01 21:06:00,603 - INFO - \t Val. Loss: 0.771 | Val. F1 (Macro): 0.4958\n",
      "2025-05-01 21:06:00,607 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:06:00,975 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:06:00,976 - INFO - \tTrain Loss: 0.838\n",
      "2025-05-01 21:06:00,976 - INFO - \t Val. Loss: 0.751 | Val. F1 (Macro): 0.5550\n",
      "2025-05-01 21:06:00,981 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:06:01,323 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:06:01,324 - INFO - \tTrain Loss: 0.822\n",
      "2025-05-01 21:06:01,324 - INFO - \t Val. Loss: 0.733 | Val. F1 (Macro): 0.5359\n",
      "2025-05-01 21:06:01,329 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:06:01,621 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:06:01,622 - INFO - \tTrain Loss: 0.786\n",
      "2025-05-01 21:06:01,622 - INFO - \t Val. Loss: 0.717 | Val. F1 (Macro): 0.5962\n",
      "2025-05-01 21:06:01,626 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 21:06:01,986 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:06:01,987 - INFO - \tTrain Loss: 0.755\n",
      "2025-05-01 21:06:01,987 - INFO - \t Val. Loss: 0.709 | Val. F1 (Macro): 0.5515\n",
      "2025-05-01 21:06:01,991 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:06:02,302 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:06:02,303 - INFO - \tTrain Loss: 0.745\n",
      "2025-05-01 21:06:02,303 - INFO - \t Val. Loss: 0.702 | Val. F1 (Macro): 0.6145\n",
      "2025-05-01 21:06:02,307 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 21:06:02,632 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:06:02,633 - INFO - \tTrain Loss: 0.719\n",
      "2025-05-01 21:06:02,634 - INFO - \t Val. Loss: 0.724 | Val. F1 (Macro): 0.6141\n",
      "2025-05-01 21:06:02,962 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:06:02,963 - INFO - \tTrain Loss: 0.688\n",
      "2025-05-01 21:06:02,964 - INFO - \t Val. Loss: 0.680 | Val. F1 (Macro): 0.6230\n",
      "2025-05-01 21:06:02,968 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 10)\n",
      "2025-05-01 21:06:03,269 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:06:03,270 - INFO - \tTrain Loss: 0.674\n",
      "2025-05-01 21:06:03,270 - INFO - \t Val. Loss: 0.682 | Val. F1 (Macro): 0.6389\n",
      "2025-05-01 21:06:03,573 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:06:03,574 - INFO - \tTrain Loss: 0.663\n",
      "2025-05-01 21:06:03,574 - INFO - \t Val. Loss: 0.675 | Val. F1 (Macro): 0.6319\n",
      "2025-05-01 21:06:03,578 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:06:03,990 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:06:03,990 - INFO - \tTrain Loss: 0.633\n",
      "2025-05-01 21:06:03,991 - INFO - \t Val. Loss: 0.664 | Val. F1 (Macro): 0.6532\n",
      "2025-05-01 21:06:03,995 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 21:06:04,306 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:06:04,307 - INFO - \tTrain Loss: 0.626\n",
      "2025-05-01 21:06:04,307 - INFO - \t Val. Loss: 0.660 | Val. F1 (Macro): 0.6348\n",
      "2025-05-01 21:06:04,312 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 21:06:04,608 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:06:04,608 - INFO - \tTrain Loss: 0.594\n",
      "2025-05-01 21:06:04,609 - INFO - \t Val. Loss: 0.652 | Val. F1 (Macro): 0.6473\n",
      "2025-05-01 21:06:04,613 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 21:06:04,953 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:06:04,954 - INFO - \tTrain Loss: 0.580\n",
      "2025-05-01 21:06:04,955 - INFO - \t Val. Loss: 0.651 | Val. F1 (Macro): 0.6501\n",
      "2025-05-01 21:06:04,959 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 21:06:05,295 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:06:05,296 - INFO - \tTrain Loss: 0.552\n",
      "2025-05-01 21:06:05,296 - INFO - \t Val. Loss: 0.645 | Val. F1 (Macro): 0.6602\n",
      "2025-05-01 21:06:05,300 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 21:06:05,636 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:06:05,637 - INFO - \tTrain Loss: 0.543\n",
      "2025-05-01 21:06:05,637 - INFO - \t Val. Loss: 0.669 | Val. F1 (Macro): 0.6579\n",
      "2025-05-01 21:06:05,910 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:06:05,911 - INFO - \tTrain Loss: 0.529\n",
      "2025-05-01 21:06:05,912 - INFO - \t Val. Loss: 0.640 | Val. F1 (Macro): 0.6644\n",
      "2025-05-01 21:06:05,916 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 21:06:06,328 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:06:06,329 - INFO - \tTrain Loss: 0.507\n",
      "2025-05-01 21:06:06,329 - INFO - \t Val. Loss: 0.647 | Val. F1 (Macro): 0.6747\n",
      "2025-05-01 21:06:06,654 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:06:06,655 - INFO - \tTrain Loss: 0.498\n",
      "2025-05-01 21:06:06,655 - INFO - \t Val. Loss: 0.652 | Val. F1 (Macro): 0.6770\n",
      "2025-05-01 21:06:06,955 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:06:06,956 - INFO - \tTrain Loss: 0.502\n",
      "2025-05-01 21:06:06,956 - INFO - \t Val. Loss: 0.655 | Val. F1 (Macro): 0.6794\n",
      "2025-05-01 21:06:07,257 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:06:07,258 - INFO - \tTrain Loss: 0.459\n",
      "2025-05-01 21:06:07,258 - INFO - \t Val. Loss: 0.656 | Val. F1 (Macro): 0.6828\n",
      "2025-05-01 21:06:07,536 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:06:07,536 - INFO - \tTrain Loss: 0.463\n",
      "2025-05-01 21:06:07,537 - INFO - \t Val. Loss: 0.647 | Val. F1 (Macro): 0.6917\n",
      "2025-05-01 21:06:07,813 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:06:07,813 - INFO - \tTrain Loss: 0.444\n",
      "2025-05-01 21:06:07,814 - INFO - \t Val. Loss: 0.646 | Val. F1 (Macro): 0.6869\n",
      "2025-05-01 21:06:08,208 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:06:08,209 - INFO - \tTrain Loss: 0.419\n",
      "2025-05-01 21:06:08,209 - INFO - \t Val. Loss: 0.662 | Val. F1 (Macro): 0.6841\n",
      "2025-05-01 21:06:08,536 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:06:08,537 - INFO - \tTrain Loss: 0.421\n",
      "2025-05-01 21:06:08,537 - INFO - \t Val. Loss: 0.662 | Val. F1 (Macro): 0.6840\n",
      "2025-05-01 21:06:08,869 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:06:08,870 - INFO - \tTrain Loss: 0.408\n",
      "2025-05-01 21:06:08,871 - INFO - \t Val. Loss: 0.661 | Val. F1 (Macro): 0.6895\n",
      "2025-05-01 21:06:09,221 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:06:09,222 - INFO - \tTrain Loss: 0.381\n",
      "2025-05-01 21:06:09,222 - INFO - \t Val. Loss: 0.679 | Val. F1 (Macro): 0.6901\n",
      "2025-05-01 21:06:09,544 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:06:09,544 - INFO - \tTrain Loss: 0.381\n",
      "2025-05-01 21:06:09,545 - INFO - \t Val. Loss: 0.673 | Val. F1 (Macro): 0.6903\n",
      "2025-05-01 21:06:09,558 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_CNN_(Learned_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:06:09,662 - INFO - Test Set Performance:\n",
      "2025-05-01 21:06:09,663 - INFO - \tAccuracy: 0.7387\n",
      "2025-05-01 21:06:09,664 - INFO - \tF1 (Macro): 0.6707\n",
      "2025-05-01 21:06:09,665 - INFO - \tPrecision (Macro): 0.7064\n",
      "2025-05-01 21:06:09,665 - INFO - \tRecall (Macro): 0.6492\n",
      "2025-05-01 21:06:09,666 - INFO - \tF1 (Weighted): 0.7288\n",
      "2025-05-01 21:06:09,667 - INFO - \tPrecision (Weighted): 0.7315\n",
      "2025-05-01 21:06:09,667 - INFO - \tRecall (Weighted): 0.7387\n",
      "2025-05-01 21:06:09,668 - INFO - \tTest Loss: 0.650\n",
      "2025-05-01 21:06:09,669 - INFO - \tEval Time: 0.096s\n",
      "2025-05-01 21:06:09,672 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_CNN_(Learned_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:06:09,754 - INFO - Starting training for MLP (Avg GloVe Emb) on Financial News\n",
      "2025-05-01 21:06:09,755 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 21:06:09,758 - INFO - Model: MLP (Avg GloVe Emb), Trainable Parameters: 8,643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: MLP (Avg GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:10,002 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:06:10,003 - INFO - \tTrain Loss: 0.977\n",
      "2025-05-01 21:06:10,003 - INFO - \t Val. Loss: 0.902 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:10,006 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:06:10,249 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:06:10,249 - INFO - \tTrain Loss: 0.910\n",
      "2025-05-01 21:06:10,250 - INFO - \t Val. Loss: 0.866 | Val. F1 (Macro): 0.2519\n",
      "2025-05-01 21:06:10,254 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:06:10,474 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:06:10,475 - INFO - \tTrain Loss: 0.893\n",
      "2025-05-01 21:06:10,475 - INFO - \t Val. Loss: 0.849 | Val. F1 (Macro): 0.3552\n",
      "2025-05-01 21:06:10,479 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:06:10,690 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:06:10,691 - INFO - \tTrain Loss: 0.862\n",
      "2025-05-01 21:06:10,692 - INFO - \t Val. Loss: 0.829 | Val. F1 (Macro): 0.3908\n",
      "2025-05-01 21:06:10,695 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:06:10,879 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:06:10,880 - INFO - \tTrain Loss: 0.845\n",
      "2025-05-01 21:06:10,880 - INFO - \t Val. Loss: 0.816 | Val. F1 (Macro): 0.4006\n",
      "2025-05-01 21:06:10,884 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:06:11,084 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:06:11,085 - INFO - \tTrain Loss: 0.828\n",
      "2025-05-01 21:06:11,086 - INFO - \t Val. Loss: 0.797 | Val. F1 (Macro): 0.3979\n",
      "2025-05-01 21:06:11,090 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 21:06:11,352 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:06:11,353 - INFO - \tTrain Loss: 0.806\n",
      "2025-05-01 21:06:11,354 - INFO - \t Val. Loss: 0.780 | Val. F1 (Macro): 0.4006\n",
      "2025-05-01 21:06:11,357 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:06:11,603 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:06:11,604 - INFO - \tTrain Loss: 0.798\n",
      "2025-05-01 21:06:11,605 - INFO - \t Val. Loss: 0.770 | Val. F1 (Macro): 0.4005\n",
      "2025-05-01 21:06:11,608 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 21:06:11,849 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:06:11,850 - INFO - \tTrain Loss: 0.787\n",
      "2025-05-01 21:06:11,851 - INFO - \t Val. Loss: 0.760 | Val. F1 (Macro): 0.4179\n",
      "2025-05-01 21:06:11,855 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 21:06:12,100 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:06:12,101 - INFO - \tTrain Loss: 0.776\n",
      "2025-05-01 21:06:12,102 - INFO - \t Val. Loss: 0.759 | Val. F1 (Macro): 0.4173\n",
      "2025-05-01 21:06:12,105 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 10)\n",
      "2025-05-01 21:06:12,362 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:06:12,363 - INFO - \tTrain Loss: 0.763\n",
      "2025-05-01 21:06:12,363 - INFO - \t Val. Loss: 0.744 | Val. F1 (Macro): 0.4210\n",
      "2025-05-01 21:06:12,368 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 11)\n",
      "2025-05-01 21:06:12,627 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:06:12,628 - INFO - \tTrain Loss: 0.760\n",
      "2025-05-01 21:06:12,629 - INFO - \t Val. Loss: 0.740 | Val. F1 (Macro): 0.4194\n",
      "2025-05-01 21:06:12,633 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:06:12,870 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:06:12,871 - INFO - \tTrain Loss: 0.749\n",
      "2025-05-01 21:06:12,871 - INFO - \t Val. Loss: 0.733 | Val. F1 (Macro): 0.4223\n",
      "2025-05-01 21:06:12,875 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 21:06:13,118 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:06:13,119 - INFO - \tTrain Loss: 0.748\n",
      "2025-05-01 21:06:13,119 - INFO - \t Val. Loss: 0.732 | Val. F1 (Macro): 0.4224\n",
      "2025-05-01 21:06:13,124 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 21:06:13,332 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:06:13,333 - INFO - \tTrain Loss: 0.741\n",
      "2025-05-01 21:06:13,333 - INFO - \t Val. Loss: 0.729 | Val. F1 (Macro): 0.4331\n",
      "2025-05-01 21:06:13,338 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 21:06:13,561 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:06:13,562 - INFO - \tTrain Loss: 0.733\n",
      "2025-05-01 21:06:13,563 - INFO - \t Val. Loss: 0.720 | Val. F1 (Macro): 0.4297\n",
      "2025-05-01 21:06:13,567 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 21:06:13,792 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:06:13,793 - INFO - \tTrain Loss: 0.726\n",
      "2025-05-01 21:06:13,795 - INFO - \t Val. Loss: 0.713 | Val. F1 (Macro): 0.4429\n",
      "2025-05-01 21:06:13,799 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 21:06:14,037 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:06:14,038 - INFO - \tTrain Loss: 0.723\n",
      "2025-05-01 21:06:14,039 - INFO - \t Val. Loss: 0.711 | Val. F1 (Macro): 0.4422\n",
      "2025-05-01 21:06:14,043 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 18)\n",
      "2025-05-01 21:06:14,259 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:06:14,260 - INFO - \tTrain Loss: 0.714\n",
      "2025-05-01 21:06:14,260 - INFO - \t Val. Loss: 0.708 | Val. F1 (Macro): 0.4362\n",
      "2025-05-01 21:06:14,265 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 21:06:14,457 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:06:14,458 - INFO - \tTrain Loss: 0.709\n",
      "2025-05-01 21:06:14,458 - INFO - \t Val. Loss: 0.704 | Val. F1 (Macro): 0.4405\n",
      "2025-05-01 21:06:14,463 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 20)\n",
      "2025-05-01 21:06:14,680 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:06:14,681 - INFO - \tTrain Loss: 0.698\n",
      "2025-05-01 21:06:14,681 - INFO - \t Val. Loss: 0.698 | Val. F1 (Macro): 0.4647\n",
      "2025-05-01 21:06:14,686 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 21)\n",
      "2025-05-01 21:06:14,884 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:06:14,885 - INFO - \tTrain Loss: 0.705\n",
      "2025-05-01 21:06:14,887 - INFO - \t Val. Loss: 0.698 | Val. F1 (Macro): 0.4856\n",
      "2025-05-01 21:06:14,891 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 22)\n",
      "2025-05-01 21:06:15,140 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:06:15,140 - INFO - \tTrain Loss: 0.701\n",
      "2025-05-01 21:06:15,141 - INFO - \t Val. Loss: 0.692 | Val. F1 (Macro): 0.4789\n",
      "2025-05-01 21:06:15,145 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 23)\n",
      "2025-05-01 21:06:15,388 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:06:15,389 - INFO - \tTrain Loss: 0.695\n",
      "2025-05-01 21:06:15,389 - INFO - \t Val. Loss: 0.696 | Val. F1 (Macro): 0.4910\n",
      "2025-05-01 21:06:15,638 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:06:15,639 - INFO - \tTrain Loss: 0.692\n",
      "2025-05-01 21:06:15,640 - INFO - \t Val. Loss: 0.695 | Val. F1 (Macro): 0.4636\n",
      "2025-05-01 21:06:15,893 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:06:15,893 - INFO - \tTrain Loss: 0.681\n",
      "2025-05-01 21:06:15,894 - INFO - \t Val. Loss: 0.687 | Val. F1 (Macro): 0.5049\n",
      "2025-05-01 21:06:15,898 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 26)\n",
      "2025-05-01 21:06:16,155 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:06:16,156 - INFO - \tTrain Loss: 0.686\n",
      "2025-05-01 21:06:16,156 - INFO - \t Val. Loss: 0.683 | Val. F1 (Macro): 0.4872\n",
      "2025-05-01 21:06:16,160 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 27)\n",
      "2025-05-01 21:06:16,399 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:06:16,400 - INFO - \tTrain Loss: 0.675\n",
      "2025-05-01 21:06:16,401 - INFO - \t Val. Loss: 0.680 | Val. F1 (Macro): 0.5164\n",
      "2025-05-01 21:06:16,405 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 28)\n",
      "2025-05-01 21:06:16,645 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:06:16,646 - INFO - \tTrain Loss: 0.681\n",
      "2025-05-01 21:06:16,646 - INFO - \t Val. Loss: 0.680 | Val. F1 (Macro): 0.5597\n",
      "2025-05-01 21:06:16,650 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt (Epoch 29)\n",
      "2025-05-01 21:06:16,890 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:06:16,891 - INFO - \tTrain Loss: 0.673\n",
      "2025-05-01 21:06:16,892 - INFO - \t Val. Loss: 0.682 | Val. F1 (Macro): 0.5559\n",
      "2025-05-01 21:06:16,906 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:06:16,949 - INFO - Test Set Performance:\n",
      "2025-05-01 21:06:16,949 - INFO - \tAccuracy: 0.6878\n",
      "2025-05-01 21:06:16,950 - INFO - \tF1 (Macro): 0.5735\n",
      "2025-05-01 21:06:16,950 - INFO - \tPrecision (Macro): 0.6003\n",
      "2025-05-01 21:06:16,951 - INFO - \tRecall (Macro): 0.5591\n",
      "2025-05-01 21:06:16,951 - INFO - \tF1 (Weighted): 0.6742\n",
      "2025-05-01 21:06:16,951 - INFO - \tPrecision (Weighted): 0.6691\n",
      "2025-05-01 21:06:16,952 - INFO - \tRecall (Weighted): 0.6878\n",
      "2025-05-01 21:06:16,953 - INFO - \tTest Loss: 0.701\n",
      "2025-05-01 21:06:16,953 - INFO - \tEval Time: 0.035s\n",
      "2025-05-01 21:06:16,956 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_MLP_(Avg_GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:06:17,044 - INFO - Starting training for CNN (GloVe Emb) on Financial News\n",
      "2025-05-01 21:06:17,045 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 21:06:17,048 - INFO - Model: CNN (GloVe Emb), Trainable Parameters: 121,203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: CNN (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:17,532 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:06:17,532 - INFO - \tTrain Loss: 0.934\n",
      "2025-05-01 21:06:17,532 - INFO - \t Val. Loss: 0.818 | Val. F1 (Macro): 0.4365\n",
      "2025-05-01 21:06:17,537 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:06:17,809 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:06:17,810 - INFO - \tTrain Loss: 0.834\n",
      "2025-05-01 21:06:17,810 - INFO - \t Val. Loss: 0.787 | Val. F1 (Macro): 0.4462\n",
      "2025-05-01 21:06:17,815 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:06:18,106 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:06:18,107 - INFO - \tTrain Loss: 0.797\n",
      "2025-05-01 21:06:18,107 - INFO - \t Val. Loss: 0.763 | Val. F1 (Macro): 0.4686\n",
      "2025-05-01 21:06:18,111 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:06:18,424 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:06:18,425 - INFO - \tTrain Loss: 0.773\n",
      "2025-05-01 21:06:18,425 - INFO - \t Val. Loss: 0.734 | Val. F1 (Macro): 0.5037\n",
      "2025-05-01 21:06:18,430 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:06:18,726 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:06:18,727 - INFO - \tTrain Loss: 0.742\n",
      "2025-05-01 21:06:18,729 - INFO - \t Val. Loss: 0.710 | Val. F1 (Macro): 0.5326\n",
      "2025-05-01 21:06:18,733 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:06:19,020 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:06:19,021 - INFO - \tTrain Loss: 0.720\n",
      "2025-05-01 21:06:19,022 - INFO - \t Val. Loss: 0.699 | Val. F1 (Macro): 0.5783\n",
      "2025-05-01 21:06:19,026 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 21:06:19,331 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:06:19,332 - INFO - \tTrain Loss: 0.693\n",
      "2025-05-01 21:06:19,332 - INFO - \t Val. Loss: 0.669 | Val. F1 (Macro): 0.5997\n",
      "2025-05-01 21:06:19,337 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:06:19,615 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:06:19,616 - INFO - \tTrain Loss: 0.690\n",
      "2025-05-01 21:06:19,616 - INFO - \t Val. Loss: 0.668 | Val. F1 (Macro): 0.6127\n",
      "2025-05-01 21:06:19,620 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 21:06:19,859 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:06:19,860 - INFO - \tTrain Loss: 0.666\n",
      "2025-05-01 21:06:19,860 - INFO - \t Val. Loss: 0.644 | Val. F1 (Macro): 0.6159\n",
      "2025-05-01 21:06:19,864 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 21:06:20,089 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:06:20,089 - INFO - \tTrain Loss: 0.661\n",
      "2025-05-01 21:06:20,090 - INFO - \t Val. Loss: 0.651 | Val. F1 (Macro): 0.5879\n",
      "2025-05-01 21:06:20,335 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:06:20,336 - INFO - \tTrain Loss: 0.640\n",
      "2025-05-01 21:06:20,336 - INFO - \t Val. Loss: 0.636 | Val. F1 (Macro): 0.6340\n",
      "2025-05-01 21:06:20,340 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 11)\n",
      "2025-05-01 21:06:20,598 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:06:20,599 - INFO - \tTrain Loss: 0.630\n",
      "2025-05-01 21:06:20,599 - INFO - \t Val. Loss: 0.621 | Val. F1 (Macro): 0.6457\n",
      "2025-05-01 21:06:20,604 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:06:20,854 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:06:20,855 - INFO - \tTrain Loss: 0.612\n",
      "2025-05-01 21:06:20,856 - INFO - \t Val. Loss: 0.625 | Val. F1 (Macro): 0.6587\n",
      "2025-05-01 21:06:21,109 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:06:21,110 - INFO - \tTrain Loss: 0.603\n",
      "2025-05-01 21:06:21,110 - INFO - \t Val. Loss: 0.610 | Val. F1 (Macro): 0.6438\n",
      "2025-05-01 21:06:21,114 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 21:06:21,356 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:06:21,357 - INFO - \tTrain Loss: 0.595\n",
      "2025-05-01 21:06:21,357 - INFO - \t Val. Loss: 0.604 | Val. F1 (Macro): 0.6493\n",
      "2025-05-01 21:06:21,362 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 21:06:21,616 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:06:21,617 - INFO - \tTrain Loss: 0.578\n",
      "2025-05-01 21:06:21,617 - INFO - \t Val. Loss: 0.603 | Val. F1 (Macro): 0.6360\n",
      "2025-05-01 21:06:21,622 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 21:06:21,904 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:06:21,905 - INFO - \tTrain Loss: 0.574\n",
      "2025-05-01 21:06:21,905 - INFO - \t Val. Loss: 0.600 | Val. F1 (Macro): 0.6502\n",
      "2025-05-01 21:06:21,909 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 17)\n",
      "2025-05-01 21:06:22,160 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:06:22,161 - INFO - \tTrain Loss: 0.572\n",
      "2025-05-01 21:06:22,162 - INFO - \t Val. Loss: 0.602 | Val. F1 (Macro): 0.6636\n",
      "2025-05-01 21:06:22,398 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:06:22,398 - INFO - \tTrain Loss: 0.561\n",
      "2025-05-01 21:06:22,399 - INFO - \t Val. Loss: 0.594 | Val. F1 (Macro): 0.6677\n",
      "2025-05-01 21:06:22,403 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 19)\n",
      "2025-05-01 21:06:22,661 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:06:22,661 - INFO - \tTrain Loss: 0.557\n",
      "2025-05-01 21:06:22,662 - INFO - \t Val. Loss: 0.596 | Val. F1 (Macro): 0.6622\n",
      "2025-05-01 21:06:22,962 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:06:22,963 - INFO - \tTrain Loss: 0.536\n",
      "2025-05-01 21:06:22,963 - INFO - \t Val. Loss: 0.590 | Val. F1 (Macro): 0.6681\n",
      "2025-05-01 21:06:22,968 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 21)\n",
      "2025-05-01 21:06:23,200 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:06:23,201 - INFO - \tTrain Loss: 0.547\n",
      "2025-05-01 21:06:23,201 - INFO - \t Val. Loss: 0.608 | Val. F1 (Macro): 0.6529\n",
      "2025-05-01 21:06:23,430 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:06:23,431 - INFO - \tTrain Loss: 0.522\n",
      "2025-05-01 21:06:23,431 - INFO - \t Val. Loss: 0.590 | Val. F1 (Macro): 0.6510\n",
      "2025-05-01 21:06:23,435 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 23)\n",
      "2025-05-01 21:06:23,659 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:06:23,660 - INFO - \tTrain Loss: 0.529\n",
      "2025-05-01 21:06:23,660 - INFO - \t Val. Loss: 0.593 | Val. F1 (Macro): 0.6646\n",
      "2025-05-01 21:06:23,925 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:06:23,926 - INFO - \tTrain Loss: 0.536\n",
      "2025-05-01 21:06:23,926 - INFO - \t Val. Loss: 0.583 | Val. F1 (Macro): 0.6548\n",
      "2025-05-01 21:06:23,931 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 25)\n",
      "2025-05-01 21:06:24,189 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:06:24,189 - INFO - \tTrain Loss: 0.517\n",
      "2025-05-01 21:06:24,190 - INFO - \t Val. Loss: 0.580 | Val. F1 (Macro): 0.6860\n",
      "2025-05-01 21:06:24,193 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt (Epoch 26)\n",
      "2025-05-01 21:06:24,450 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:06:24,450 - INFO - \tTrain Loss: 0.502\n",
      "2025-05-01 21:06:24,451 - INFO - \t Val. Loss: 0.590 | Val. F1 (Macro): 0.6969\n",
      "2025-05-01 21:06:24,725 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:06:24,726 - INFO - \tTrain Loss: 0.509\n",
      "2025-05-01 21:06:24,726 - INFO - \t Val. Loss: 0.586 | Val. F1 (Macro): 0.6563\n",
      "2025-05-01 21:06:25,006 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:06:25,007 - INFO - \tTrain Loss: 0.499\n",
      "2025-05-01 21:06:25,007 - INFO - \t Val. Loss: 0.599 | Val. F1 (Macro): 0.6856\n",
      "2025-05-01 21:06:25,263 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:06:25,264 - INFO - \tTrain Loss: 0.499\n",
      "2025-05-01 21:06:25,264 - INFO - \t Val. Loss: 0.584 | Val. F1 (Macro): 0.6561\n",
      "2025-05-01 21:06:25,277 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_CNN_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:06:25,316 - INFO - Test Set Performance:\n",
      "2025-05-01 21:06:25,317 - INFO - \tAccuracy: 0.7675\n",
      "2025-05-01 21:06:25,318 - INFO - \tF1 (Macro): 0.7073\n",
      "2025-05-01 21:06:25,319 - INFO - \tPrecision (Macro): 0.7510\n",
      "2025-05-01 21:06:25,319 - INFO - \tRecall (Macro): 0.6871\n",
      "2025-05-01 21:06:25,320 - INFO - \tF1 (Weighted): 0.7553\n",
      "2025-05-01 21:06:25,320 - INFO - \tPrecision (Weighted): 0.7659\n",
      "2025-05-01 21:06:25,321 - INFO - \tRecall (Weighted): 0.7675\n",
      "2025-05-01 21:06:25,322 - INFO - \tTest Loss: 0.613\n",
      "2025-05-01 21:06:25,322 - INFO - \tEval Time: 0.030s\n",
      "2025-05-01 21:06:25,325 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_CNN_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:06:25,425 - INFO - Starting training for LSTM (GloVe Emb) on Financial News\n",
      "2025-05-01 21:06:25,425 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 21:06:25,429 - INFO - Model: LSTM (GloVe Emb), Trainable Parameters: 109,251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: LSTM (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:25,712 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:06:25,713 - INFO - \tTrain Loss: 0.969\n",
      "2025-05-01 21:06:25,713 - INFO - \t Val. Loss: 0.915 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:25,717 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:06:26,051 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:06:26,051 - INFO - \tTrain Loss: 0.932\n",
      "2025-05-01 21:06:26,052 - INFO - \t Val. Loss: 0.902 | Val. F1 (Macro): 0.2706\n",
      "2025-05-01 21:06:26,055 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:06:26,351 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:06:26,352 - INFO - \tTrain Loss: 0.899\n",
      "2025-05-01 21:06:26,353 - INFO - \t Val. Loss: 0.886 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:26,357 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:06:26,610 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:06:26,610 - INFO - \tTrain Loss: 0.887\n",
      "2025-05-01 21:06:26,610 - INFO - \t Val. Loss: 0.860 | Val. F1 (Macro): 0.3701\n",
      "2025-05-01 21:06:26,615 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:06:26,849 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:06:26,850 - INFO - \tTrain Loss: 0.880\n",
      "2025-05-01 21:06:26,850 - INFO - \t Val. Loss: 0.881 | Val. F1 (Macro): 0.3844\n",
      "2025-05-01 21:06:27,085 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:06:27,086 - INFO - \tTrain Loss: 0.897\n",
      "2025-05-01 21:06:27,086 - INFO - \t Val. Loss: 0.877 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:27,346 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:06:27,346 - INFO - \tTrain Loss: 0.874\n",
      "2025-05-01 21:06:27,347 - INFO - \t Val. Loss: 0.854 | Val. F1 (Macro): 0.3839\n",
      "2025-05-01 21:06:27,350 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:06:27,581 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:06:27,582 - INFO - \tTrain Loss: 0.867\n",
      "2025-05-01 21:06:27,582 - INFO - \t Val. Loss: 0.875 | Val. F1 (Macro): 0.3914\n",
      "2025-05-01 21:06:27,812 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:06:27,813 - INFO - \tTrain Loss: 0.914\n",
      "2025-05-01 21:06:27,813 - INFO - \t Val. Loss: 0.902 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:28,039 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:06:28,040 - INFO - \tTrain Loss: 0.881\n",
      "2025-05-01 21:06:28,040 - INFO - \t Val. Loss: 0.860 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:28,284 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:06:28,285 - INFO - \tTrain Loss: 0.863\n",
      "2025-05-01 21:06:28,286 - INFO - \t Val. Loss: 0.875 | Val. F1 (Macro): 0.3919\n",
      "2025-05-01 21:06:28,564 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:06:28,564 - INFO - \tTrain Loss: 0.868\n",
      "2025-05-01 21:06:28,565 - INFO - \t Val. Loss: 0.851 | Val. F1 (Macro): 0.3888\n",
      "2025-05-01 21:06:28,568 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:06:28,841 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:06:28,842 - INFO - \tTrain Loss: 0.856\n",
      "2025-05-01 21:06:28,843 - INFO - \t Val. Loss: 0.848 | Val. F1 (Macro): 0.3827\n",
      "2025-05-01 21:06:28,847 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 21:06:29,119 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:06:29,120 - INFO - \tTrain Loss: 0.893\n",
      "2025-05-01 21:06:29,121 - INFO - \t Val. Loss: 0.914 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:29,393 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:06:29,394 - INFO - \tTrain Loss: 0.894\n",
      "2025-05-01 21:06:29,395 - INFO - \t Val. Loss: 0.877 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:29,700 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:06:29,701 - INFO - \tTrain Loss: 0.869\n",
      "2025-05-01 21:06:29,701 - INFO - \t Val. Loss: 0.885 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:29,971 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:06:29,972 - INFO - \tTrain Loss: 0.869\n",
      "2025-05-01 21:06:29,972 - INFO - \t Val. Loss: 0.876 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:30,255 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:06:30,256 - INFO - \tTrain Loss: 0.869\n",
      "2025-05-01 21:06:30,256 - INFO - \t Val. Loss: 0.872 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:30,540 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:06:30,541 - INFO - \tTrain Loss: 0.860\n",
      "2025-05-01 21:06:30,541 - INFO - \t Val. Loss: 0.860 | Val. F1 (Macro): 0.3940\n",
      "2025-05-01 21:06:30,853 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:06:30,854 - INFO - \tTrain Loss: 0.857\n",
      "2025-05-01 21:06:30,854 - INFO - \t Val. Loss: 0.852 | Val. F1 (Macro): 0.3941\n",
      "2025-05-01 21:06:31,141 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:06:31,141 - INFO - \tTrain Loss: 0.859\n",
      "2025-05-01 21:06:31,142 - INFO - \t Val. Loss: 0.888 | Val. F1 (Macro): 0.3736\n",
      "2025-05-01 21:06:31,427 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:06:31,429 - INFO - \tTrain Loss: 0.860\n",
      "2025-05-01 21:06:31,429 - INFO - \t Val. Loss: 0.871 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:31,706 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:06:31,708 - INFO - \tTrain Loss: 0.877\n",
      "2025-05-01 21:06:31,708 - INFO - \t Val. Loss: 0.857 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:31,985 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:06:31,986 - INFO - \tTrain Loss: 0.855\n",
      "2025-05-01 21:06:31,986 - INFO - \t Val. Loss: 0.854 | Val. F1 (Macro): 0.3948\n",
      "2025-05-01 21:06:32,271 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:06:32,272 - INFO - \tTrain Loss: 0.846\n",
      "2025-05-01 21:06:32,272 - INFO - \t Val. Loss: 0.845 | Val. F1 (Macro): 0.4023\n",
      "2025-05-01 21:06:32,276 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 25)\n",
      "2025-05-01 21:06:32,552 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:06:32,552 - INFO - \tTrain Loss: 0.852\n",
      "2025-05-01 21:06:32,553 - INFO - \t Val. Loss: 0.854 | Val. F1 (Macro): 0.4032\n",
      "2025-05-01 21:06:32,840 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:06:32,840 - INFO - \tTrain Loss: 0.851\n",
      "2025-05-01 21:06:32,842 - INFO - \t Val. Loss: 0.834 | Val. F1 (Macro): 0.4057\n",
      "2025-05-01 21:06:32,846 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt (Epoch 27)\n",
      "2025-05-01 21:06:33,140 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:06:33,141 - INFO - \tTrain Loss: 0.848\n",
      "2025-05-01 21:06:33,141 - INFO - \t Val. Loss: 0.900 | Val. F1 (Macro): 0.3860\n",
      "2025-05-01 21:06:33,419 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:06:33,420 - INFO - \tTrain Loss: 0.879\n",
      "2025-05-01 21:06:33,420 - INFO - \t Val. Loss: 0.867 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:33,689 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:06:33,689 - INFO - \tTrain Loss: 0.861\n",
      "2025-05-01 21:06:33,690 - INFO - \t Val. Loss: 0.854 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:33,703 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:06:33,739 - INFO - Test Set Performance:\n",
      "2025-05-01 21:06:33,739 - INFO - \tAccuracy: 0.6204\n",
      "2025-05-01 21:06:33,740 - INFO - \tF1 (Macro): 0.3894\n",
      "2025-05-01 21:06:33,741 - INFO - \tPrecision (Macro): 0.3687\n",
      "2025-05-01 21:06:33,741 - INFO - \tRecall (Macro): 0.4161\n",
      "2025-05-01 21:06:33,742 - INFO - \tF1 (Weighted): 0.5693\n",
      "2025-05-01 21:06:33,743 - INFO - \tPrecision (Weighted): 0.5288\n",
      "2025-05-01 21:06:33,743 - INFO - \tRecall (Weighted): 0.6204\n",
      "2025-05-01 21:06:33,744 - INFO - \tTest Loss: 0.865\n",
      "2025-05-01 21:06:33,745 - INFO - \tEval Time: 0.028s\n",
      "2025-05-01 21:06:33,748 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_LSTM_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:06:33,829 - INFO - Starting training for BiLSTM (GloVe Emb) on Financial News\n",
      "2025-05-01 21:06:33,829 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 21:06:33,835 - INFO - Model: BiLSTM (GloVe Emb), Trainable Parameters: 284,035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: BiLSTM (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:34,220 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:06:34,220 - INFO - \tTrain Loss: 0.916\n",
      "2025-05-01 21:06:34,221 - INFO - \t Val. Loss: 0.850 | Val. F1 (Macro): 0.3922\n",
      "2025-05-01 21:06:34,225 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:06:34,560 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:06:34,561 - INFO - \tTrain Loss: 0.845\n",
      "2025-05-01 21:06:34,562 - INFO - \t Val. Loss: 0.815 | Val. F1 (Macro): 0.3957\n",
      "2025-05-01 21:06:34,567 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:06:34,892 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:06:34,893 - INFO - \tTrain Loss: 0.814\n",
      "2025-05-01 21:06:34,895 - INFO - \t Val. Loss: 0.779 | Val. F1 (Macro): 0.4085\n",
      "2025-05-01 21:06:34,900 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 3)\n",
      "2025-05-01 21:06:35,297 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:06:35,298 - INFO - \tTrain Loss: 0.783\n",
      "2025-05-01 21:06:35,299 - INFO - \t Val. Loss: 0.754 | Val. F1 (Macro): 0.4359\n",
      "2025-05-01 21:06:35,303 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 4)\n",
      "2025-05-01 21:06:35,653 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:06:35,653 - INFO - \tTrain Loss: 0.774\n",
      "2025-05-01 21:06:35,654 - INFO - \t Val. Loss: 0.745 | Val. F1 (Macro): 0.4384\n",
      "2025-05-01 21:06:35,658 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:06:35,994 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:06:35,995 - INFO - \tTrain Loss: 0.757\n",
      "2025-05-01 21:06:35,995 - INFO - \t Val. Loss: 0.719 | Val. F1 (Macro): 0.4065\n",
      "2025-05-01 21:06:36,000 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 21:06:36,356 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:06:36,357 - INFO - \tTrain Loss: 0.741\n",
      "2025-05-01 21:06:36,357 - INFO - \t Val. Loss: 0.724 | Val. F1 (Macro): 0.4289\n",
      "2025-05-01 21:06:36,687 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:06:36,688 - INFO - \tTrain Loss: 0.726\n",
      "2025-05-01 21:06:36,688 - INFO - \t Val. Loss: 0.720 | Val. F1 (Macro): 0.4268\n",
      "2025-05-01 21:06:36,997 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:06:36,998 - INFO - \tTrain Loss: 0.712\n",
      "2025-05-01 21:06:36,998 - INFO - \t Val. Loss: 0.707 | Val. F1 (Macro): 0.4413\n",
      "2025-05-01 21:06:37,003 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 9)\n",
      "2025-05-01 21:06:37,289 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:06:37,290 - INFO - \tTrain Loss: 0.693\n",
      "2025-05-01 21:06:37,292 - INFO - \t Val. Loss: 0.734 | Val. F1 (Macro): 0.4576\n",
      "2025-05-01 21:06:37,584 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:06:37,584 - INFO - \tTrain Loss: 0.687\n",
      "2025-05-01 21:06:37,585 - INFO - \t Val. Loss: 0.672 | Val. F1 (Macro): 0.4718\n",
      "2025-05-01 21:06:37,589 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 11)\n",
      "2025-05-01 21:06:37,869 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:06:37,869 - INFO - \tTrain Loss: 0.677\n",
      "2025-05-01 21:06:37,870 - INFO - \t Val. Loss: 0.664 | Val. F1 (Macro): 0.5574\n",
      "2025-05-01 21:06:37,875 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:06:38,153 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:06:38,153 - INFO - \tTrain Loss: 0.673\n",
      "2025-05-01 21:06:38,155 - INFO - \t Val. Loss: 0.635 | Val. F1 (Macro): 0.5342\n",
      "2025-05-01 21:06:38,160 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 13)\n",
      "2025-05-01 21:06:38,489 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:06:38,490 - INFO - \tTrain Loss: 0.648\n",
      "2025-05-01 21:06:38,491 - INFO - \t Val. Loss: 0.628 | Val. F1 (Macro): 0.5822\n",
      "2025-05-01 21:06:38,495 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 14)\n",
      "2025-05-01 21:06:38,816 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:06:38,816 - INFO - \tTrain Loss: 0.621\n",
      "2025-05-01 21:06:38,819 - INFO - \t Val. Loss: 0.632 | Val. F1 (Macro): 0.5752\n",
      "2025-05-01 21:06:39,142 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:06:39,143 - INFO - \tTrain Loss: 0.619\n",
      "2025-05-01 21:06:39,144 - INFO - \t Val. Loss: 0.624 | Val. F1 (Macro): 0.5530\n",
      "2025-05-01 21:06:39,148 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 16)\n",
      "2025-05-01 21:06:39,437 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:06:39,437 - INFO - \tTrain Loss: 0.609\n",
      "2025-05-01 21:06:39,438 - INFO - \t Val. Loss: 0.645 | Val. F1 (Macro): 0.6199\n",
      "2025-05-01 21:06:39,757 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:06:39,758 - INFO - \tTrain Loss: 0.601\n",
      "2025-05-01 21:06:39,758 - INFO - \t Val. Loss: 0.648 | Val. F1 (Macro): 0.5664\n",
      "2025-05-01 21:06:40,039 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:06:40,040 - INFO - \tTrain Loss: 0.587\n",
      "2025-05-01 21:06:40,041 - INFO - \t Val. Loss: 0.634 | Val. F1 (Macro): 0.6052\n",
      "2025-05-01 21:06:40,330 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:06:40,331 - INFO - \tTrain Loss: 0.597\n",
      "2025-05-01 21:06:40,331 - INFO - \t Val. Loss: 0.591 | Val. F1 (Macro): 0.6312\n",
      "2025-05-01 21:06:40,336 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt (Epoch 20)\n",
      "2025-05-01 21:06:40,677 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:06:40,678 - INFO - \tTrain Loss: 0.582\n",
      "2025-05-01 21:06:40,678 - INFO - \t Val. Loss: 0.600 | Val. F1 (Macro): 0.6451\n",
      "2025-05-01 21:06:40,962 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:06:40,963 - INFO - \tTrain Loss: 0.561\n",
      "2025-05-01 21:06:40,963 - INFO - \t Val. Loss: 0.739 | Val. F1 (Macro): 0.6056\n",
      "2025-05-01 21:06:41,250 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:06:41,251 - INFO - \tTrain Loss: 0.573\n",
      "2025-05-01 21:06:41,251 - INFO - \t Val. Loss: 0.604 | Val. F1 (Macro): 0.6679\n",
      "2025-05-01 21:06:41,549 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:06:41,549 - INFO - \tTrain Loss: 0.572\n",
      "2025-05-01 21:06:41,549 - INFO - \t Val. Loss: 0.612 | Val. F1 (Macro): 0.6286\n",
      "2025-05-01 21:06:41,838 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:06:41,839 - INFO - \tTrain Loss: 0.537\n",
      "2025-05-01 21:06:41,839 - INFO - \t Val. Loss: 0.603 | Val. F1 (Macro): 0.6548\n",
      "2025-05-01 21:06:42,166 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:06:42,166 - INFO - \tTrain Loss: 0.547\n",
      "2025-05-01 21:06:42,167 - INFO - \t Val. Loss: 0.628 | Val. F1 (Macro): 0.6717\n",
      "2025-05-01 21:06:42,443 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:06:42,443 - INFO - \tTrain Loss: 0.544\n",
      "2025-05-01 21:06:42,444 - INFO - \t Val. Loss: 0.610 | Val. F1 (Macro): 0.6743\n",
      "2025-05-01 21:06:42,770 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:06:42,771 - INFO - \tTrain Loss: 0.526\n",
      "2025-05-01 21:06:42,772 - INFO - \t Val. Loss: 0.600 | Val. F1 (Macro): 0.6467\n",
      "2025-05-01 21:06:43,130 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:06:43,131 - INFO - \tTrain Loss: 0.524\n",
      "2025-05-01 21:06:43,131 - INFO - \t Val. Loss: 0.607 | Val. F1 (Macro): 0.6672\n",
      "2025-05-01 21:06:43,470 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:06:43,471 - INFO - \tTrain Loss: 0.496\n",
      "2025-05-01 21:06:43,472 - INFO - \t Val. Loss: 0.600 | Val. F1 (Macro): 0.6890\n",
      "2025-05-01 21:06:43,488 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:06:43,529 - INFO - Test Set Performance:\n",
      "2025-05-01 21:06:43,530 - INFO - \tAccuracy: 0.7290\n",
      "2025-05-01 21:06:43,530 - INFO - \tF1 (Macro): 0.6042\n",
      "2025-05-01 21:06:43,531 - INFO - \tPrecision (Macro): 0.6519\n",
      "2025-05-01 21:06:43,531 - INFO - \tRecall (Macro): 0.5839\n",
      "2025-05-01 21:06:43,532 - INFO - \tF1 (Weighted): 0.7111\n",
      "2025-05-01 21:06:43,533 - INFO - \tPrecision (Weighted): 0.7096\n",
      "2025-05-01 21:06:43,534 - INFO - \tRecall (Weighted): 0.7290\n",
      "2025-05-01 21:06:43,534 - INFO - \tTest Loss: 0.637\n",
      "2025-05-01 21:06:43,535 - INFO - \tEval Time: 0.032s\n",
      "2025-05-01 21:06:43,538 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_BiLSTM_(GloVe_Emb)_confusion_matrix.csv\n",
      "2025-05-01 21:06:43,620 - INFO - Starting training for CNN-LSTM (GloVe Emb) on Financial News\n",
      "2025-05-01 21:06:43,621 - INFO - Using pre-trained embeddings. Freeze: True\n",
      "2025-05-01 21:06:43,627 - INFO - Model: CNN-LSTM (GloVe Emb), Trainable Parameters: 314,135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: CNN-LSTM (GloVe Emb) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:44,021 - INFO - Epoch: 01 | Time: 0m 0s\n",
      "2025-05-01 21:06:44,022 - INFO - \tTrain Loss: 0.946\n",
      "2025-05-01 21:06:44,022 - INFO - \t Val. Loss: 0.871 | Val. F1 (Macro): 0.2485\n",
      "2025-05-01 21:06:44,027 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 1)\n",
      "2025-05-01 21:06:44,410 - INFO - Epoch: 02 | Time: 0m 0s\n",
      "2025-05-01 21:06:44,410 - INFO - \tTrain Loss: 0.855\n",
      "2025-05-01 21:06:44,411 - INFO - \t Val. Loss: 0.795 | Val. F1 (Macro): 0.4130\n",
      "2025-05-01 21:06:44,416 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 2)\n",
      "2025-05-01 21:06:44,764 - INFO - Epoch: 03 | Time: 0m 0s\n",
      "2025-05-01 21:06:44,765 - INFO - \tTrain Loss: 0.838\n",
      "2025-05-01 21:06:44,766 - INFO - \t Val. Loss: 0.848 | Val. F1 (Macro): 0.3389\n",
      "2025-05-01 21:06:45,135 - INFO - Epoch: 04 | Time: 0m 0s\n",
      "2025-05-01 21:06:45,136 - INFO - \tTrain Loss: 0.818\n",
      "2025-05-01 21:06:45,137 - INFO - \t Val. Loss: 0.805 | Val. F1 (Macro): 0.4281\n",
      "2025-05-01 21:06:45,553 - INFO - Epoch: 05 | Time: 0m 0s\n",
      "2025-05-01 21:06:45,554 - INFO - \tTrain Loss: 0.787\n",
      "2025-05-01 21:06:45,554 - INFO - \t Val. Loss: 0.755 | Val. F1 (Macro): 0.4102\n",
      "2025-05-01 21:06:45,559 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 5)\n",
      "2025-05-01 21:06:45,925 - INFO - Epoch: 06 | Time: 0m 0s\n",
      "2025-05-01 21:06:45,926 - INFO - \tTrain Loss: 0.798\n",
      "2025-05-01 21:06:45,926 - INFO - \t Val. Loss: 0.731 | Val. F1 (Macro): 0.4147\n",
      "2025-05-01 21:06:45,931 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 6)\n",
      "2025-05-01 21:06:46,290 - INFO - Epoch: 07 | Time: 0m 0s\n",
      "2025-05-01 21:06:46,290 - INFO - \tTrain Loss: 0.761\n",
      "2025-05-01 21:06:46,291 - INFO - \t Val. Loss: 0.715 | Val. F1 (Macro): 0.4232\n",
      "2025-05-01 21:06:46,296 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 7)\n",
      "2025-05-01 21:06:46,611 - INFO - Epoch: 08 | Time: 0m 0s\n",
      "2025-05-01 21:06:46,612 - INFO - \tTrain Loss: 0.760\n",
      "2025-05-01 21:06:46,612 - INFO - \t Val. Loss: 0.707 | Val. F1 (Macro): 0.4742\n",
      "2025-05-01 21:06:46,617 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 8)\n",
      "2025-05-01 21:06:46,933 - INFO - Epoch: 09 | Time: 0m 0s\n",
      "2025-05-01 21:06:46,933 - INFO - \tTrain Loss: 0.744\n",
      "2025-05-01 21:06:46,934 - INFO - \t Val. Loss: 0.711 | Val. F1 (Macro): 0.4173\n",
      "2025-05-01 21:06:47,291 - INFO - Epoch: 10 | Time: 0m 0s\n",
      "2025-05-01 21:06:47,292 - INFO - \tTrain Loss: 0.729\n",
      "2025-05-01 21:06:47,292 - INFO - \t Val. Loss: 0.712 | Val. F1 (Macro): 0.4334\n",
      "2025-05-01 21:06:47,593 - INFO - Epoch: 11 | Time: 0m 0s\n",
      "2025-05-01 21:06:47,595 - INFO - \tTrain Loss: 0.730\n",
      "2025-05-01 21:06:47,595 - INFO - \t Val. Loss: 0.698 | Val. F1 (Macro): 0.4323\n",
      "2025-05-01 21:06:47,600 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 11)\n",
      "2025-05-01 21:06:47,901 - INFO - Epoch: 12 | Time: 0m 0s\n",
      "2025-05-01 21:06:47,902 - INFO - \tTrain Loss: 0.717\n",
      "2025-05-01 21:06:47,902 - INFO - \t Val. Loss: 0.682 | Val. F1 (Macro): 0.4477\n",
      "2025-05-01 21:06:47,907 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 12)\n",
      "2025-05-01 21:06:48,210 - INFO - Epoch: 13 | Time: 0m 0s\n",
      "2025-05-01 21:06:48,211 - INFO - \tTrain Loss: 0.698\n",
      "2025-05-01 21:06:48,212 - INFO - \t Val. Loss: 0.687 | Val. F1 (Macro): 0.5627\n",
      "2025-05-01 21:06:48,532 - INFO - Epoch: 14 | Time: 0m 0s\n",
      "2025-05-01 21:06:48,533 - INFO - \tTrain Loss: 0.684\n",
      "2025-05-01 21:06:48,534 - INFO - \t Val. Loss: 0.688 | Val. F1 (Macro): 0.4541\n",
      "2025-05-01 21:06:48,834 - INFO - Epoch: 15 | Time: 0m 0s\n",
      "2025-05-01 21:06:48,835 - INFO - \tTrain Loss: 0.667\n",
      "2025-05-01 21:06:48,835 - INFO - \t Val. Loss: 0.681 | Val. F1 (Macro): 0.4566\n",
      "2025-05-01 21:06:48,840 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 15)\n",
      "2025-05-01 21:06:49,138 - INFO - Epoch: 16 | Time: 0m 0s\n",
      "2025-05-01 21:06:49,138 - INFO - \tTrain Loss: 0.652\n",
      "2025-05-01 21:06:49,138 - INFO - \t Val. Loss: 0.702 | Val. F1 (Macro): 0.5377\n",
      "2025-05-01 21:06:49,483 - INFO - Epoch: 17 | Time: 0m 0s\n",
      "2025-05-01 21:06:49,484 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 21:06:49,485 - INFO - \t Val. Loss: 0.712 | Val. F1 (Macro): 0.4582\n",
      "2025-05-01 21:06:49,785 - INFO - Epoch: 18 | Time: 0m 0s\n",
      "2025-05-01 21:06:49,786 - INFO - \tTrain Loss: 0.644\n",
      "2025-05-01 21:06:49,786 - INFO - \t Val. Loss: 0.680 | Val. F1 (Macro): 0.4572\n",
      "2025-05-01 21:06:49,791 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 18)\n",
      "2025-05-01 21:06:50,154 - INFO - Epoch: 19 | Time: 0m 0s\n",
      "2025-05-01 21:06:50,155 - INFO - \tTrain Loss: 0.609\n",
      "2025-05-01 21:06:50,155 - INFO - \t Val. Loss: 0.681 | Val. F1 (Macro): 0.5411\n",
      "2025-05-01 21:06:50,482 - INFO - Epoch: 20 | Time: 0m 0s\n",
      "2025-05-01 21:06:50,483 - INFO - \tTrain Loss: 0.614\n",
      "2025-05-01 21:06:50,484 - INFO - \t Val. Loss: 0.688 | Val. F1 (Macro): 0.5708\n",
      "2025-05-01 21:06:50,804 - INFO - Epoch: 21 | Time: 0m 0s\n",
      "2025-05-01 21:06:50,804 - INFO - \tTrain Loss: 0.594\n",
      "2025-05-01 21:06:50,805 - INFO - \t Val. Loss: 0.687 | Val. F1 (Macro): 0.5396\n",
      "2025-05-01 21:06:51,131 - INFO - Epoch: 22 | Time: 0m 0s\n",
      "2025-05-01 21:06:51,131 - INFO - \tTrain Loss: 0.578\n",
      "2025-05-01 21:06:51,132 - INFO - \t Val. Loss: 0.658 | Val. F1 (Macro): 0.5981\n",
      "2025-05-01 21:06:51,137 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 22)\n",
      "2025-05-01 21:06:51,441 - INFO - Epoch: 23 | Time: 0m 0s\n",
      "2025-05-01 21:06:51,442 - INFO - \tTrain Loss: 0.557\n",
      "2025-05-01 21:06:51,442 - INFO - \t Val. Loss: 0.662 | Val. F1 (Macro): 0.6005\n",
      "2025-05-01 21:06:51,820 - INFO - Epoch: 24 | Time: 0m 0s\n",
      "2025-05-01 21:06:51,821 - INFO - \tTrain Loss: 0.549\n",
      "2025-05-01 21:06:51,821 - INFO - \t Val. Loss: 0.694 | Val. F1 (Macro): 0.5884\n",
      "2025-05-01 21:06:52,135 - INFO - Epoch: 25 | Time: 0m 0s\n",
      "2025-05-01 21:06:52,136 - INFO - \tTrain Loss: 0.575\n",
      "2025-05-01 21:06:52,137 - INFO - \t Val. Loss: 0.643 | Val. F1 (Macro): 0.6289\n",
      "2025-05-01 21:06:52,141 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 25)\n",
      "2025-05-01 21:06:52,481 - INFO - Epoch: 26 | Time: 0m 0s\n",
      "2025-05-01 21:06:52,482 - INFO - \tTrain Loss: 0.530\n",
      "2025-05-01 21:06:52,483 - INFO - \t Val. Loss: 0.660 | Val. F1 (Macro): 0.6304\n",
      "2025-05-01 21:06:52,806 - INFO - Epoch: 27 | Time: 0m 0s\n",
      "2025-05-01 21:06:52,807 - INFO - \tTrain Loss: 0.536\n",
      "2025-05-01 21:06:52,807 - INFO - \t Val. Loss: 0.668 | Val. F1 (Macro): 0.6640\n",
      "2025-05-01 21:06:53,108 - INFO - Epoch: 28 | Time: 0m 0s\n",
      "2025-05-01 21:06:53,109 - INFO - \tTrain Loss: 0.517\n",
      "2025-05-01 21:06:53,109 - INFO - \t Val. Loss: 0.653 | Val. F1 (Macro): 0.6298\n",
      "2025-05-01 21:06:53,469 - INFO - Epoch: 29 | Time: 0m 0s\n",
      "2025-05-01 21:06:53,470 - INFO - \tTrain Loss: 0.514\n",
      "2025-05-01 21:06:53,470 - INFO - \t Val. Loss: 0.642 | Val. F1 (Macro): 0.6207\n",
      "2025-05-01 21:06:53,475 - INFO - Saved best model to ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt (Epoch 29)\n",
      "2025-05-01 21:06:53,830 - INFO - Epoch: 30 | Time: 0m 0s\n",
      "2025-05-01 21:06:53,831 - INFO - \tTrain Loss: 0.498\n",
      "2025-05-01 21:06:53,831 - INFO - \t Val. Loss: 0.654 | Val. F1 (Macro): 0.6333\n",
      "2025-05-01 21:06:53,847 - INFO - Loaded best model from ..\\models\\dl\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_best.pt for final test evaluation.\n",
      "2025-05-01 21:06:53,886 - INFO - Test Set Performance:\n",
      "2025-05-01 21:06:53,886 - INFO - \tAccuracy: 0.7180\n",
      "2025-05-01 21:06:53,887 - INFO - \tF1 (Macro): 0.6059\n",
      "2025-05-01 21:06:53,887 - INFO - \tPrecision (Macro): 0.6499\n",
      "2025-05-01 21:06:53,888 - INFO - \tRecall (Macro): 0.5850\n",
      "2025-05-01 21:06:53,888 - INFO - \tF1 (Weighted): 0.7025\n",
      "2025-05-01 21:06:53,889 - INFO - \tPrecision (Weighted): 0.7009\n",
      "2025-05-01 21:06:53,890 - INFO - \tRecall (Weighted): 0.7180\n",
      "2025-05-01 21:06:53,890 - INFO - \tTest Loss: 0.696\n",
      "2025-05-01 21:06:53,890 - INFO - \tEval Time: 0.030s\n",
      "2025-05-01 21:06:53,893 - INFO - Saved confusion matrix CSV to ..\\result\\financial_news\\Financial_News_CNN-LSTM_(GloVe_Emb)_confusion_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Loop through each dataset defined in the configuration ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    print(f\"\\n{'='*20} Processing Dataset: {dataset_name} {'='*20}\")\n",
    "    logging.info(f\"Processing Dataset: {dataset_name}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    train_df = load_data(config['train_path'])\n",
    "    val_df = load_data(config['val_path'])\n",
    "    test_df = load_data(config['test_path'])\n",
    "\n",
    "    if train_df is None or val_df is None or test_df is None:\n",
    "        logging.error(f\"Skipping dataset {dataset_name} due to data loading errors.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Build or Load Vocabulary\n",
    "    if os.path.exists(config['vocab_path']):\n",
    "        vocab = joblib.load(config['vocab_path'])\n",
    "        logging.info(f\"Loaded existing vocabulary from {config['vocab_path']}\")\n",
    "        # Check if special tokens exist, add if missing (backward compatibility)\n",
    "        if '<pad>' not in vocab: vocab['<pad>'] = 0\n",
    "        if '<unk>' not in vocab: vocab['<unk>'] = 1\n",
    "    else:\n",
    "        vocab = build_vocab(train_df[TEXT_COLUMN].tolist(), min_freq=MIN_WORD_FREQ)\n",
    "        joblib.dump(vocab, config['vocab_path'])\n",
    "        logging.info(f\"Built and saved vocabulary to {config['vocab_path']}\")\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    pad_idx = vocab['<pad>']\n",
    "\n",
    "    # 3. Create Datasets and DataLoaders\n",
    "    train_dataset = SentimentDataset(train_df[TEXT_COLUMN].tolist(), train_df[TARGET_COLUMN].tolist(), vocab)\n",
    "    val_dataset = SentimentDataset(val_df[TEXT_COLUMN].tolist(), val_df[TARGET_COLUMN].tolist(), vocab)\n",
    "    test_dataset = SentimentDataset(test_df[TEXT_COLUMN].tolist(), test_df[TARGET_COLUMN].tolist(), vocab)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    # 4. Load Pre-trained Embeddings (if needed)\n",
    "    glove_embeddings = None\n",
    "    if os.path.exists(GLOVE_PATH):\n",
    "        glove_embeddings = load_glove_embeddings(GLOVE_PATH, vocab, EMBEDDING_DIM)\n",
    "        if glove_embeddings is None:\n",
    "            logging.warning(\"Failed to load GloVe embeddings. Models requiring them will use learned embeddings.\")\n",
    "    else:\n",
    "        logging.warning(f\"GloVe path not found: {GLOVE_PATH}. Pre-trained embeddings disabled.\")\n",
    "\n",
    "\n",
    "    # --- Define Models to Run ---\n",
    "    models_to_run = {\n",
    "        # --- Mid Level ---\n",
    "        # Name: (ModelClass, {kwargs}, use_pretrained_embed, freeze_embed)\n",
    "        \"MLP (Avg Learned Emb)\": (MLPAveraged, {'hidden_dim1': 64, 'hidden_dim2': 32, 'dropout': DROPOUT, 'embedding_dim': LEARNED_EMBEDDING_DIM}, False, False),\n",
    "        \"RNN (Learned Emb)\": (RNNModel, {'hidden_dim': HIDDEN_DIM_RNN_LSTM, 'n_layers': N_LAYERS_RNN_LSTM, 'dropout': DROPOUT, 'bidirectional': False, 'embedding_dim': LEARNED_EMBEDDING_DIM}, False, False),\n",
    "        \"LSTM (Learned Emb)\": (LSTMModel, {'hidden_dim': HIDDEN_DIM_RNN_LSTM, 'n_layers': N_LAYERS_RNN_LSTM, 'dropout': DROPOUT, 'bidirectional': False, 'embedding_dim': LEARNED_EMBEDDING_DIM}, False, False),\n",
    "        \"BiLSTM (Learned Emb)\": (LSTMModel, {'hidden_dim': HIDDEN_DIM_RNN_LSTM, 'n_layers': N_LAYERS_RNN_LSTM, 'dropout': DROPOUT, 'bidirectional': True, 'embedding_dim': LEARNED_EMBEDDING_DIM}, False, False),\n",
    "        \"CNN (Learned Emb)\": (CNNModel, {'n_filters': N_FILTERS_CNN, 'filter_sizes': FILTER_SIZES_CNN, 'dropout': DROPOUT, 'embedding_dim': LEARNED_EMBEDDING_DIM}, False, False),\n",
    "\n",
    "        # --- Advanced Level (Using Pre-trained) ---\n",
    "        # Requires GloVe embeddings to be loaded successfully\n",
    "        \"MLP (Avg GloVe Emb)\": (MLPAveraged, {'hidden_dim1': 64, 'hidden_dim2': 32, 'dropout': DROPOUT, 'embedding_dim': EMBEDDING_DIM}, True, True), # Freeze GloVe\n",
    "        \"CNN (GloVe Emb)\": (CNNModel, {'n_filters': N_FILTERS_CNN, 'filter_sizes': FILTER_SIZES_CNN, 'dropout': DROPOUT, 'embedding_dim': EMBEDDING_DIM}, True, True), # Freeze GloVe\n",
    "        \"LSTM (GloVe Emb)\": (LSTMModel, {'hidden_dim': HIDDEN_DIM_RNN_LSTM, 'n_layers': N_LAYERS_RNN_LSTM, 'dropout': DROPOUT, 'bidirectional': False, 'embedding_dim': EMBEDDING_DIM}, True, True), # Freeze GloVe\n",
    "        \"BiLSTM (GloVe Emb)\": (LSTMModel, {'hidden_dim': HIDDEN_DIM_RNN_LSTM, 'n_layers': N_LAYERS_RNN_LSTM, 'dropout': DROPOUT, 'bidirectional': True, 'embedding_dim': EMBEDDING_DIM}, True, True), # Freeze GloVe\n",
    "        \"CNN-LSTM (GloVe Emb)\": (CNNLSTMModel, {'n_filters': N_FILTERS_CNN, 'filter_size_cnn': 3, 'hidden_dim_lstm': HIDDEN_DIM_RNN_LSTM, 'n_layers_lstm': N_LAYERS_RNN_LSTM, 'dropout': DROPOUT, 'embedding_dim': EMBEDDING_DIM}, True, True), # Freeze GloVe\n",
    "    }\n",
    "\n",
    "    # --- Loop through each model configuration ---\n",
    "    for model_name, (ModelClass, model_kwargs, use_pretrained, freeze_embed) in models_to_run.items():\n",
    "\n",
    "        # Skip models requiring GloVe if loading failed\n",
    "        if use_pretrained and glove_embeddings is None:\n",
    "            logging.warning(f\"Skipping model '{model_name}' as pre-trained GloVe embeddings were not loaded.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Training Model: {model_name} ---\")\n",
    "        logging.info(f\"Starting training for {model_name} on {dataset_name}\")\n",
    "        results = {\"Dataset\": dataset_name, \"Model\": model_name}\n",
    "\n",
    "        try:\n",
    "            # Instantiate model\n",
    "            current_embedding_dim = model_kwargs['embedding_dim'] # Get dim from kwargs\n",
    "            current_pretrained_embeddings = glove_embeddings if use_pretrained else None\n",
    "\n",
    "            model = ModelClass(\n",
    "                vocab_size=vocab_size,\n",
    "                output_dim=NUM_CLASSES,\n",
    "                pad_idx=pad_idx,\n",
    "                pretrained_embeddings=current_pretrained_embeddings,\n",
    "                freeze_embeddings=freeze_embed,\n",
    "                **model_kwargs # Pass specific model architecture args\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            # Count parameters\n",
    "            num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            logging.info(f\"Model: {model_name}, Trainable Parameters: {num_params:,}\")\n",
    "\n",
    "            # Define optimizer and criterion\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.CrossEntropyLoss().to(DEVICE) # Handles softmax internally\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            total_train_time = 0\n",
    "            model_save_path = os.path.join(config['model_dir'], f\"{dataset_name.replace(' ', '_')}_{model_name.replace(' ', '_')}_best.pt\")\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                start_epoch_time = time.time()\n",
    "\n",
    "                train_loss, train_time_epoch = train_epoch(model, train_loader, optimizer, criterion, DEVICE, GRADIENT_CLIP)\n",
    "                val_loss, val_metrics, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "                total_train_time += train_time_epoch\n",
    "                end_epoch_time = time.time()\n",
    "                epoch_mins, epoch_secs = divmod(end_epoch_time - start_epoch_time, 60)\n",
    "\n",
    "                logging.info(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {epoch_secs:.0f}s')\n",
    "                logging.info(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "                logging.info(f'\\t Val. Loss: {val_loss:.3f} | Val. F1 (Macro): {val_metrics[\"F1 (Macro)\"]:.4f}')\n",
    "\n",
    "                # Save best model based on validation loss\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    logging.info(f\"Saved best model to {model_save_path} (Epoch {epoch+1})\")\n",
    "\n",
    "            results[\"Train Time (Epoch, s)\"] = round(total_train_time / NUM_EPOCHS, 3) # Avg time per epoch\n",
    "\n",
    "            # Load best model and evaluate on Test set\n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "            logging.info(f\"Loaded best model from {model_save_path} for final test evaluation.\")\n",
    "\n",
    "            test_loss, test_metrics, test_eval_time, test_conf_matrix = evaluate(model, test_loader, criterion, DEVICE)\n",
    "            results.update(test_metrics)\n",
    "            results[\"Eval Time (s)\"] = round(test_eval_time, 3)\n",
    "\n",
    "            logging.info(\"Test Set Performance:\")\n",
    "            for key, value in test_metrics.items():\n",
    "                logging.info(f\"\\t{key}: {value:.4f}\")\n",
    "            logging.info(f\"\\tTest Loss: {test_loss:.3f}\")\n",
    "            logging.info(f\"\\tEval Time: {test_eval_time:.3f}s\")\n",
    "\n",
    "            # --- Save Confusion Matrix CSV ---\n",
    "            cm_filename = f\"{dataset_name.replace(' ', '_')}_{model_name.replace(' ', '_')}_confusion_matrix.csv\" # Change extension to .csv\n",
    "            cm_save_path = os.path.join(config['result_dir'], cm_filename)\n",
    "            try:\n",
    "                # Convert numpy array to DataFrame for better CSV formatting with labels\n",
    "                cm_df = pd.DataFrame(test_conf_matrix, \n",
    "                                    index=LABEL_MAP.keys(), # Rows are True Labels\n",
    "                                    columns=LABEL_MAP.keys()) # Columns are Predicted Labels\n",
    "                cm_df.index.name = 'True Label'\n",
    "                cm_df.columns.name = 'Predicted Label'\n",
    "                \n",
    "                # Save to CSV\n",
    "                cm_df.to_csv(cm_save_path, index=True, mode='w+') # index=True to include row/column names\n",
    "                \n",
    "                logging.info(f\"Saved confusion matrix CSV to {cm_save_path}\")\n",
    "            except Exception as cm_save_e:\n",
    "                logging.error(f\"Failed to save confusion matrix CSV for {model_name}: {cm_save_e}\")\n",
    "        # --- End Save Confusion Matrix CSV ---\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"!!! An error occurred while processing {model_name} for {dataset_name}: {e}\", exc_info=True) # Log traceback\n",
    "            # Record partial results if possible\n",
    "            results[\"Accuracy\"] = np.nan\n",
    "            results[\"F1 (Macro)\"] = np.nan\n",
    "            # Fill other metrics with NaN or error messages\n",
    "            for metric in METRICS_TO_CALCULATE:\n",
    "                if metric not in results:\n",
    "                    results[metric] = np.nan if metric not in [\"Train Time (Epoch, s)\", \"Eval Time (s)\"] else 0.0\n",
    "        finally:\n",
    "            all_results.append(results)\n",
    "            # Clean up memory\n",
    "            del model\n",
    "            if 'optimizer' in locals(): del optimizer\n",
    "            if 'criterion' in locals(): del criterion   \n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# --- Combine results into a DataFrame ---\n",
    "results_df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46116d",
   "metadata": {},
   "source": [
    "# 6. Results Summary and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7518b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Overall Deep Learning Results Summary =====\n",
      "           Dataset                  Model  Accuracy  F1 (Macro)  Precision (Macro)  Recall (Macro)  F1 (Weighted)  Precision (Weighted)  Recall (Weighted)  Train Time (Epoch, s)  Eval Time (s)\n",
      "0      Book Review  MLP (Avg Learned Emb)    0.8567      0.5505             0.6616          0.5465         0.8226                0.8193             0.8567                29.8710         3.1720\n",
      "1      Book Review      RNN (Learned Emb)    0.7976      0.2958             0.2659          0.3333         0.7079                0.6362             0.7976                50.9510         3.8210\n",
      "2      Book Review     LSTM (Learned Emb)    0.8615      0.5323             0.6266          0.5367         0.8205                0.8094             0.8615                83.1910        11.5990\n",
      "3      Book Review   BiLSTM (Learned Emb)    0.8660      0.5945             0.6987          0.5644         0.8389                0.8350             0.8660               143.5880        12.6790\n",
      "4      Book Review      CNN (Learned Emb)    0.8649      0.5969             0.7385          0.5712         0.8376                0.8434             0.8649                85.7120         8.1360\n",
      "5      Book Review    MLP (Avg GloVe Emb)    0.8142      0.4110             0.4839          0.4053         0.7552                0.7300             0.8142                27.3390         4.7410\n",
      "6      Book Review        CNN (GloVe Emb)    0.8376      0.4767             0.7359          0.4599         0.7881                0.8130             0.8376                58.3200         6.9520\n",
      "7      Book Review       LSTM (GloVe Emb)    0.8564      0.5272             0.5086          0.5474         0.8177                0.7825             0.8564                75.1230         7.6580\n",
      "8      Book Review     BiLSTM (GloVe Emb)    0.8800      0.6789             0.7189          0.6572         0.8694                0.8642             0.8800               552.9940        26.7580\n",
      "9      Book Review   CNN-LSTM (GloVe Emb)    0.8783      0.6723             0.7255          0.6400         0.8662                0.8610             0.8783               120.9310         9.4880\n",
      "10  Financial News  MLP (Avg Learned Emb)    0.6492      0.4223             0.3961          0.4522         0.6050                0.5665             0.6492                 0.2860         0.0280\n",
      "11  Financial News      RNN (Learned Emb)    0.5956      0.2550             0.4207          0.3358         0.4485                0.5408             0.5956                 0.2860         0.0290\n",
      "12  Financial News     LSTM (Learned Emb)    0.6699      0.4513             0.4225          0.4905         0.6340                0.6069             0.6699                 0.2720         0.0300\n",
      "13  Financial News   BiLSTM (Learned Emb)    0.7345      0.6600             0.6834          0.6437         0.7271                0.7249             0.7345                 0.3010         0.0360\n",
      "14  Financial News      CNN (Learned Emb)    0.7387      0.6707             0.7064          0.6492         0.7288                0.7315             0.7387                 0.3430         0.0960\n",
      "15  Financial News    MLP (Avg GloVe Emb)    0.6878      0.5735             0.6003          0.5591         0.6742                0.6691             0.6878                 0.2000         0.0350\n",
      "16  Financial News        CNN (GloVe Emb)    0.7675      0.7073             0.7510          0.6871         0.7553                0.7659             0.7675                 0.2360         0.0300\n",
      "17  Financial News       LSTM (GloVe Emb)    0.6204      0.3894             0.3687          0.4161         0.5693                0.5288             0.6204                 0.2390         0.0280\n",
      "18  Financial News     BiLSTM (GloVe Emb)    0.7290      0.6042             0.6519          0.5839         0.7111                0.7096             0.7290                 0.2810         0.0320\n",
      "19  Financial News   CNN-LSTM (GloVe Emb)    0.7180      0.6059             0.6499          0.5850         0.7025                0.7009             0.7180                 0.2990         0.0300\n",
      "\n",
      "Results for Book Review saved to ..\\result\\book_reviews\\Book_Review_dl_pytorch_results.csv\n",
      "\n",
      "Results for Financial News saved to ..\\result\\financial_news\\Financial_News_dl_pytorch_results.csv\n",
      "\n",
      "Combined results saved to ..\\result\\combined_dl_pytorch_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n===== Overall Deep Learning Results Summary =====\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1200)\n",
    "pd.set_option('display.max_colwidth', 80) # Adjust if needed\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Ensure all expected columns exist, fill with NaN if necessary\n",
    "for col in METRICS_TO_CALCULATE:\n",
    "    if col not in results_df.columns:\n",
    "        results_df[col] = np.nan\n",
    "\n",
    "# Reorder columns for clarity\n",
    "column_order = [\"Dataset\", \"Model\"] + METRICS_TO_CALCULATE\n",
    "# Filter out columns not present if something went wrong during creation\n",
    "column_order = [col for col in column_order if col in results_df.columns]\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# --- Save results to CSV for each dataset ---\n",
    "for dataset_name, config in DATASETS_TO_PROCESS.items():\n",
    "    dataset_results_df = results_df[results_df['Dataset'] == dataset_name]\n",
    "    if not dataset_results_df.empty:\n",
    "        results_filename = f\"{dataset_name.replace(' ', '_')}_dl_pytorch_results.csv\"\n",
    "        results_save_path = os.path.join(config['result_dir'], results_filename)\n",
    "        try:\n",
    "            dataset_results_df.to_csv(results_save_path, index=False, mode='w+')\n",
    "            print(f\"\\nResults for {dataset_name} saved to {results_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving results for {dataset_name} to {results_save_path}: {e}\")\n",
    "\n",
    "# --- Save combined results ---\n",
    "combined_results_path = os.path.join(RESULT_DIR, \"combined_dl_pytorch_results.csv\")\n",
    "try:\n",
    "    results_df.to_csv(combined_results_path, index=False, mode='w+')\n",
    "    print(f\"\\nCombined results saved to {combined_results_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving combined results to {combined_results_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
