{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c8df5bc",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04885117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import warnings\n",
    "import gc # Garbage collector\n",
    "\n",
    "# NLTK for basic tokenization and vocab building\n",
    "import nltk\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e854871",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c842e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "DATA_DIR = \"../data/processed\"\n",
    "MODEL_SAVE_DIR = \"../models/dl\"\n",
    "RESULTS_SAVE_DIR = \"../results\"\n",
    "RESULTS_CSV_FILE = os.path.join(RESULTS_SAVE_DIR, \"dl_results_summary.csv\")\n",
    "# --- Path to Pre-trained Embeddings (MODIFY IF USING GLOVE/WORD2VEC) ---\n",
    "# Download from https://nlp.stanford.edu/projects/glove/\n",
    "GLOVE_PATH = \"../embeddings/glove.6B.100d.txt\" # Example path\n",
    "\n",
    "# --- Experiment Setup ---\n",
    "DOMAINS = [\"book_reviews\", \"financial_news\"] # Add domain folder names\n",
    "# List models to run by their class names defined below\n",
    "DL_MODELS_TO_RUN = [\n",
    "    \"MLPClassifier\",\n",
    "    \"RNNClassifier\", # Simple RNN (Optional, usually worse than LSTM)\n",
    "    \"CNNClassifier\",\n",
    "    \"LSTMClassifier\",\n",
    "    \"BiLSTMClassifier\",\n",
    "    # Advanced Level (Requires EMBEDDING_TYPE='GloVe' or similar)\n",
    "    \"CNNStaticEmbClassifier\",\n",
    "    \"LSTMStaticEmbClassifier\",\n",
    "    \"CNNLSTMHybridClassifier\",\n",
    "    \"WordCharParallelClassifier\" # Most Complex\n",
    "]\n",
    "EMBEDDING_TYPE = \"Learned\" # Options: \"Learned\", \"GloVe\", \"FastText\", \"Word2Vec\"\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "VOCAB_SIZE = 20000       # Max vocabulary size\n",
    "EMBEDDING_DIM = 100      # Dimension for embeddings (MATCH PRE-TRAINED if using GloVe etc.)\n",
    "HIDDEN_DIM_RNN = 128     # RNN/LSTM hidden units\n",
    "NUM_FILTERS_CNN = 100    # CNN filters per kernel size\n",
    "KERNEL_SIZES_CNN = [3, 4, 5] # CNN kernel sizes\n",
    "CHAR_EMBEDDING_DIM = 25  # Character embedding dimension\n",
    "CHAR_CNN_FILTERS = 50    # Filters for character CNN\n",
    "CHAR_KERNEL_SIZE = 3     # Kernel size for character CNN\n",
    "OUTPUT_DIM = 1           # Binary classification (1 output neuron)\n",
    "DROPOUT_PROB = 0.5\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5           # Adjust based on convergence\n",
    "PATIENCE = 2             # For early stopping\n",
    "\n",
    "# --- Reproducibility & Device ---\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_SAVE_DIR, exist_ok=True)\n",
    "for domain in DOMAINS:\n",
    "    os.makedirs(os.path.join(MODEL_SAVE_DIR, domain), exist_ok=True)\n",
    "    os.makedirs(os.path.join(RESULTS_SAVE_DIR, domain), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecbc70",
   "metadata": {},
   "source": [
    "# 3. Data Loading and Preprocessing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb398ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(domain_name):\n",
    "    \"\"\"Loads train, validation, and test data for a given domain.\"\"\"\n",
    "    # (Same function as in ML template - loads train, val, test)\n",
    "    print(f\"\\nLoading data for domain: {domain_name}...\")\n",
    "    try:\n",
    "        train_path = os.path.join(DATA_DIR, domain_name, \"train.csv\")\n",
    "        val_path = os.path.join(DATA_DIR, domain_name, \"validation.csv\")\n",
    "        test_path = os.path.join(DATA_DIR, domain_name, \"test.csv\")\n",
    "\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        val_df = pd.read_csv(val_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "\n",
    "        if 'text' not in train_df.columns or 'label' not in train_df.columns: raise ValueError(\"Missing 'text' or 'label' column in train data\")\n",
    "        if 'text' not in val_df.columns or 'label' not in val_df.columns: raise ValueError(\"Missing 'text' or 'label' column in val data\")\n",
    "        if 'text' not in test_df.columns or 'label' not in test_df.columns: raise ValueError(\"Missing 'text' or 'label' column in test data\")\n",
    "\n",
    "        train_df['text'].fillna('', inplace=True)\n",
    "        val_df['text'].fillna('', inplace=True)\n",
    "        test_df['text'].fillna('', inplace=True)\n",
    "\n",
    "        print(f\"Train shape: {train_df.shape}, Val shape: {val_df.shape}, Test shape: {test_df.shape}\")\n",
    "        return train_df, val_df, test_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data for {domain_name}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"Basic word tokenizer using NLTK.\"\"\"\n",
    "    return word_tokenize(text.lower()) # Lowercase during tokenization\n",
    "\n",
    "def build_vocab(texts, max_size):\n",
    "    \"\"\"Builds vocabulary from training texts.\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        word_counts.update(simple_tokenizer(text))\n",
    "    most_common_words = word_counts.most_common(max_size - 2) # Reserve for pad/unk\n",
    "    vocab = {word: i+2 for i, (word, _) in enumerate(most_common_words)}\n",
    "    vocab['<pad>'] = 0\n",
    "    vocab['<unk>'] = 1\n",
    "    return vocab, len(vocab)\n",
    "\n",
    "def text_to_sequence(texts, vocab):\n",
    "    \"\"\"Converts texts to sequences of integers using the vocab.\"\"\"\n",
    "    return [[vocab.get(token, vocab['<unk>']) for token in simple_tokenizer(text)] for text in texts]\n",
    "\n",
    "def pad_sequences(sequences, max_len, padding_value=0):\n",
    "    \"\"\"Pads sequences to max_len.\"\"\"\n",
    "    padded = np.full((len(sequences), max_len), padding_value, dtype=np.int64)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = min(len(seq), max_len)\n",
    "        if seq_len > 0:\n",
    "            padded[i, :seq_len] = seq[:seq_len]\n",
    "    return padded\n",
    "\n",
    "# --- Character Preprocessing (for WordCharParallelClassifier) ---\n",
    "def build_char_vocab(texts):\n",
    "    char_counts = Counter()\n",
    "    for text in texts:\n",
    "        char_counts.update(list(text)) # Count individual characters\n",
    "    # Keep all unique characters found + pad/unk\n",
    "    chars = sorted(char_counts.keys())\n",
    "    char_vocab = {char: i+2 for i, char in enumerate(chars)}\n",
    "    char_vocab['<c_pad>'] = 0\n",
    "    char_vocab['<c_unk>'] = 1\n",
    "    return char_vocab, len(char_vocab)\n",
    "\n",
    "def word_to_char_sequence(word, char_vocab, max_word_len):\n",
    "    \"\"\"Converts a word to a padded sequence of character indices.\"\"\"\n",
    "    seq = [char_vocab.get(char, char_vocab['<c_unk>']) for char in word]\n",
    "    padded_seq = np.full(max_word_len, char_vocab['<c_pad>'], dtype=np.int64)\n",
    "    seq_len = min(len(seq), max_word_len)\n",
    "    if seq_len > 0:\n",
    "        padded_seq[:seq_len] = seq[:seq_len]\n",
    "    return padded_seq\n",
    "\n",
    "def texts_to_char_sequences(texts, char_vocab, max_seq_len, max_word_len):\n",
    "    \"\"\"Converts list of texts to padded char sequences for each word.\"\"\"\n",
    "    char_sequences = np.full((len(texts), max_seq_len, max_word_len), char_vocab['<c_pad>'], dtype=np.int64)\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = simple_tokenizer(text)\n",
    "        num_tokens = min(len(tokens), max_seq_len)\n",
    "        for j in range(num_tokens):\n",
    "            char_sequences[i, j, :] = word_to_char_sequence(tokens[j], char_vocab, max_word_len)\n",
    "    return char_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6412be",
   "metadata": {},
   "source": [
    "# 4. Loading Pre-trained Embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, word_to_idx, embedding_dim):\n",
    "    \"\"\"Loads GloVe embeddings into a NumPy matrix.\"\"\"\n",
    "    if not os.path.exists(glove_path):\n",
    "        print(f\"Warning: GloVe path not found: {glove_path}. Using random embeddings.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GloVe file: {e}\")\n",
    "        return None\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocab_size = len(word_to_idx)\n",
    "    # Initialize with small random values\n",
    "    embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim)).astype(np.float32)\n",
    "    hits = 0\n",
    "    for word, i in word_to_idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will keep their random init.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "\n",
    "    print(f\"Converted {hits} of {vocab_size} words ({hits/vocab_size*100:.2f}%)\")\n",
    "    return torch.tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6545c",
   "metadata": {},
   "source": [
    "# 5. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, word_sequences, labels, char_sequences=None):\n",
    "        self.word_sequences = word_sequences\n",
    "        self.labels = labels\n",
    "        self.char_sequences = char_sequences\n",
    "        self.has_char = char_sequences is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'words': torch.tensor(self.word_sequences[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float) # For BCEWithLogitsLoss\n",
    "            # Use torch.long for CrossEntropyLoss if multi-class\n",
    "        }\n",
    "        if self.has_char:\n",
    "            item['chars'] = torch.tensor(self.char_sequences[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cab96",
   "metadata": {},
   "source": [
    "# 6. Model Definitions (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e87c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. MLP on Averaged Embeddings ---\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = False # Freeze pre-trained typically\n",
    "        self.fc1 = nn.Linear(embedding_dim, embedding_dim // 2)\n",
    "        self.fc2 = nn.Linear(embedding_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, words):\n",
    "        # words shape: (batch_size, seq_len)\n",
    "        embedded = self.dropout(self.embedding(words))\n",
    "        # embedded shape: (batch_size, seq_len, emb_dim)\n",
    "        # Average pooling - need to handle padding\n",
    "        mask = (words != 0).unsqueeze(-1).float() # Mask for non-padding tokens\n",
    "        summed = torch.sum(embedded * mask, dim=1)\n",
    "        counts = mask.sum(dim=1)\n",
    "        counts = torch.clamp(counts, min=1e-9) # Avoid division by zero\n",
    "        pooled = summed / counts\n",
    "        # pooled shape: (batch_size, emb_dim)\n",
    "        hidden = torch.relu(self.fc1(pooled))\n",
    "        output = self.fc2(self.dropout(hidden))\n",
    "        return output\n",
    "\n",
    "# --- 2. Simple RNN ---\n",
    "class RNNClassifier(nn.Module):\n",
    "     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None: self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "     def forward(self, words):\n",
    "        embedded = self.dropout(self.embedding(words))\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # Use the hidden state of the last time step\n",
    "        return self.fc(hidden.squeeze(0)) # Squeeze layer dim\n",
    "\n",
    "# --- 3. CNN Classifier ---\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None: self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, words):\n",
    "        embedded = self.dropout(self.embedding(words)).permute(0, 2, 1) # (B, Emb, Seq)\n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs] # List[(B, Filters, Seq')]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[-1]).squeeze(-1) for conv in conved] # List[(B, Filters)]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=-1)) # (B, Filters * Num_Kernels)\n",
    "        return self.fc(cat)\n",
    "\n",
    "# --- 4. LSTM Classifier ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None: self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout if dropout > 0 else 0, num_layers=1) # Simple 1 layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, words):\n",
    "        embedded = self.dropout(self.embedding(words)) # (B, Seq, Emb)\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: (num_layers * num_directions, batch, hidden_dim)\n",
    "        # Use the hidden state of the last layer\n",
    "        last_hidden = self.dropout(hidden[-1,:,:]) # (B, Hidden)\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "# --- 5. BiLSTM Classifier ---\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None: self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout if dropout > 0 else 0, num_layers=1, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, words):\n",
    "        embedded = self.dropout(self.embedding(words)) # (B, Seq, Emb)\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden shape: (num_layers * 2, batch, hidden_dim)\n",
    "        # Concatenate the final forward (hidden[-2]) and backward (hidden[-1]) hidden states\n",
    "        last_hidden_concat = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) # (B, Hidden*2)\n",
    "        return self.fc(last_hidden_concat)\n",
    "\n",
    "# --- 6. CNN-LSTM Hybrid Classifier ---\n",
    "class CNNLSTMHybridClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_size, hidden_dim_lstm, output_dim, dropout, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None: self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.conv = nn.Conv1d(embedding_dim, n_filters, kernel_size=filter_size)\n",
    "        self.lstm = nn.LSTM(n_filters, hidden_dim_lstm, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim_lstm * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, words):\n",
    "        embedded = self.dropout(self.embedding(words)).permute(0, 2, 1) # (B, Emb, Seq)\n",
    "        conved = torch.relu(self.conv(embedded)) # (B, Filters, Seq')\n",
    "        conved = conved.permute(0, 2, 1) # (B, Seq', Filters) - Ready for LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(conved)\n",
    "        hidden_concat = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden_concat)\n",
    "\n",
    "# --- 7. Word + Character Parallel Classifier ---\n",
    "class WordCharParallelClassifier(nn.Module):\n",
    "    def __init__(self, word_vocab_size, char_vocab_size, word_emb_dim, char_emb_dim,\n",
    "                 hidden_dim_lstm, char_cnn_filters, char_kernel_size, output_dim, dropout,\n",
    "                 pretrained_word_embeddings=None):\n",
    "        super().__init__()\n",
    "        # Word Stream\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_emb_dim, padding_idx=0)\n",
    "        if pretrained_word_embeddings is not None: self.word_embedding.weight.data.copy_(pretrained_word_embeddings)\n",
    "        self.word_lstm = nn.LSTM(word_emb_dim, hidden_dim_lstm, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Character Stream\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
    "        self.char_conv = nn.Conv1d(char_emb_dim, char_cnn_filters, kernel_size=char_kernel_size)\n",
    "        # Optional: Add an LSTM after char CNN\n",
    "        # self.char_lstm = nn.LSTM(char_cnn_filters, hidden_dim_lstm // 2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Combined Feed Forward\n",
    "        # Adjust input size based on whether char_lstm is used\n",
    "        # fc_input_dim = hidden_dim_lstm * 2 + hidden_dim_lstm # if char_lstm used\n",
    "        fc_input_dim = hidden_dim_lstm * 2 + char_cnn_filters # if only char_cnn used\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, words, chars):\n",
    "        # words: (batch_size, seq_len)\n",
    "        # chars: (batch_size, seq_len, word_len)\n",
    "\n",
    "        # --- Word Stream ---\n",
    "        word_embedded = self.dropout(self.word_embedding(words)) # (B, Seq, WordEmb)\n",
    "        _, (word_hidden, _) = self.word_lstm(word_embedded)\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        word_features = self.dropout(torch.cat((word_hidden[-2,:,:], word_hidden[-1,:,:]), dim=1)) # (B, Hidden*2)\n",
    "\n",
    "        # --- Character Stream ---\n",
    "        batch_size, seq_len, word_len = chars.shape\n",
    "        chars_embedded = self.dropout(self.char_embedding(chars)) # (B, Seq, WordLen, CharEmb)\n",
    "        # Reshape for Conv1d: Treat SeqLen*WordLen as the sequence dimension\n",
    "        chars_embedded = chars_embedded.view(batch_size * seq_len, word_len, CHAR_EMBEDDING_DIM)\n",
    "        chars_embedded = chars_embedded.permute(0, 2, 1) # (B*Seq, CharEmb, WordLen)\n",
    "\n",
    "        char_conved = torch.relu(self.char_conv(chars_embedded)) # (B*Seq, CharFilters, WordLen')\n",
    "        # Max pool over word length dimension\n",
    "        char_pooled = torch.max_pool1d(char_conved, char_conved.shape[-1]).squeeze(-1) # (B*Seq, CharFilters)\n",
    "        # Reshape back to sequence level\n",
    "        char_word_features = char_pooled.view(batch_size, seq_len, CHAR_CNN_FILTERS) # (B, Seq, CharFilters)\n",
    "\n",
    "        # Option: Average pool over sequence length for char features\n",
    "        char_features = torch.mean(char_word_features, dim=1) # (B, CharFilters)\n",
    "        # Option: Feed char_word_features into another LSTM (more complex)\n",
    "        # _, (char_hidden, _) = self.char_lstm(char_word_features)\n",
    "        # char_features = self.dropout(torch.cat((char_hidden[-2,:,:], char_hidden[-1,:,:]), dim=1)) # (B, Hidden)\n",
    "\n",
    "        # --- Fusion ---\n",
    "        combined_features = torch.cat((word_features, char_features), dim=1)\n",
    "\n",
    "        # --- Classification ---\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ed883",
   "metadata": {},
   "source": [
    "# 7. Training and Evaluation Functions (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions train_epoch and evaluate remain largely the same as in the previous template.\n",
    "# Make sure the evaluate function returns the dictionary of metrics.\n",
    "# Minor modification needed in evaluate for WordCharParallelClassifier if used\n",
    "# (to pass both words and chars from the batch).\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion, model_name=\"\"):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    is_parallel_model = model_name == \"WordCharParallelClassifier\"\n",
    "\n",
    "    for batch in iterator:\n",
    "        words = batch['words'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if is_parallel_model:\n",
    "            chars = batch['chars'].to(DEVICE)\n",
    "            predictions = model(words, chars).squeeze(1)\n",
    "        else:\n",
    "            predictions = model(words).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        predicted_classes = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (predicted_classes == labels).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, model_name=\"\"):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    is_parallel_model = model_name == \"WordCharParallelClassifier\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            words = batch['words'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            if is_parallel_model:\n",
    "                chars = batch['chars'].to(DEVICE)\n",
    "                predictions = model(words, chars).squeeze(1)\n",
    "            else:\n",
    "                predictions = model(words).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            predicted_classes = torch.round(torch.sigmoid(predictions))\n",
    "            correct = (predicted_classes == labels).float()\n",
    "            acc = correct.sum() / len(correct)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            all_preds.extend(predicted_classes.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "    eval_results = {\n",
    "        \"Loss\": epoch_loss / len(iterator),\n",
    "        \"Accuracy\": accuracy_score(all_labels, all_preds), # Use sklearn accuracy\n",
    "        \"Precision (Macro)\": report['macro avg']['precision'],\n",
    "        \"Recall (Macro)\": report['macro avg']['recall'],\n",
    "        \"F1 (Macro)\": report['macro avg']['f1-score'],\n",
    "        \"Precision (Weighted)\": report['weighted avg']['precision'],\n",
    "        \"Recall (Weighted)\": report['weighted avg']['recall'],\n",
    "        \"F1 (Weighted)\": report['weighted avg']['f1-score'],\n",
    "    }\n",
    "    print(f'\\tEval Loss: {eval_results[\"Loss\"]:.3f} | Eval Acc: {eval_results[\"Accuracy\"]*100:.2f}%')\n",
    "    # print(classification_report(all_labels, all_preds, zero_division=0)) # Optional: print full report\n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd734f",
   "metadata": {},
   "source": [
    "# 8. Main Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c452fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b228dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load GloVe Embeddings Once if needed ---\n",
    "pretrained_embedding_matrix = None\n",
    "if EMBEDDING_TYPE == \"GloVe\":\n",
    "    # Need vocab built first - build dummy vocab just to get size estimate?\n",
    "    # Or build vocab inside domain loop and load GloVe there?\n",
    "    # Let's load inside the loop for simplicity, though slightly inefficient.\n",
    "    pass # Will load inside loop\n",
    "\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    train_df, val_df, test_df = load_data(domain)\n",
    "    if train_df is None: continue\n",
    "\n",
    "    print(f\"\\n{'='*10} Processing Domain: {domain} {'='*10}\")\n",
    "\n",
    "    # --- Build Vocabularies ---\n",
    "    word_vocab, current_word_vocab_size = build_vocab(train_df['text'], VOCAB_SIZE)\n",
    "    print(f\"Word Vocabulary size: {current_word_vocab_size}\")\n",
    "    # Build char vocab if needed for parallel model\n",
    "    char_vocab, current_char_vocab_size = None, 0\n",
    "    if \"WordCharParallelClassifier\" in DL_MODELS_TO_RUN:\n",
    "        char_vocab, current_char_vocab_size = build_char_vocab(train_df['text'])\n",
    "        print(f\"Character Vocabulary size: {current_char_vocab_size}\")\n",
    "\n",
    "    # --- Load Pre-trained Embeddings (if specified) ---\n",
    "    pretrained_embedding_matrix = None\n",
    "    if EMBEDDING_TYPE == \"GloVe\":\n",
    "        pretrained_embedding_matrix = load_glove_embeddings(GLOVE_PATH, word_vocab, EMBEDDING_DIM)\n",
    "        if pretrained_embedding_matrix is not None:\n",
    "            pretrained_embedding_matrix = pretrained_embedding_matrix.to(DEVICE) # Move to device once\n",
    "\n",
    "    # --- Convert Texts to Sequences ---\n",
    "    train_word_seqs = text_to_sequence(train_df['text'], word_vocab)\n",
    "    val_word_seqs = text_to_sequence(val_df['text'], word_vocab)\n",
    "    test_word_seqs = text_to_sequence(test_df['text'], word_vocab)\n",
    "\n",
    "    # Determine max sequence length (words)\n",
    "    # Consider calculating based on 95th percentile for efficiency\n",
    "    # max_len_word = int(np.percentile([len(s) for s in train_word_seqs], 95))\n",
    "    max_len_word = max(len(s) for s in train_word_seqs) if train_word_seqs else 50 # Fallback length\n",
    "    max_len_word = min(max_len_word, 300) # Cap max length\n",
    "    print(f\"Max sequence length (words): {max_len_word}\")\n",
    "\n",
    "    # Pad word sequences\n",
    "    X_train_word_pad = pad_sequences(train_word_seqs, max_len_word)\n",
    "    X_val_word_pad = pad_sequences(val_word_seqs, max_len_word)\n",
    "    X_test_word_pad = pad_sequences(test_word_seqs, max_len_word)\n",
    "\n",
    "    # --- Prepare Character Sequences (if needed) ---\n",
    "    X_train_char_pad, X_val_char_pad, X_test_char_pad = None, None, None\n",
    "    if \"WordCharParallelClassifier\" in DL_MODELS_TO_RUN:\n",
    "        # Determine max word length (chars)\n",
    "        # max_len_char = int(np.percentile([len(w) for text in train_df['text'] for w in simple_tokenizer(text)], 95))\n",
    "        all_tokens = [token for text in train_df['text'] for token in simple_tokenizer(text)]\n",
    "        max_len_char = max(len(w) for w in all_tokens) if all_tokens else 15 # Fallback length\n",
    "        max_len_char = min(max_len_char, 25) # Cap max char length\n",
    "        print(f\"Max word length (chars): {max_len_char}\")\n",
    "\n",
    "        X_train_char_pad = texts_to_char_sequences(train_df['text'], char_vocab, max_len_word, max_len_char)\n",
    "        X_val_char_pad = texts_to_char_sequences(val_df['text'], char_vocab, max_len_word, max_len_char)\n",
    "        X_test_char_pad = texts_to_char_sequences(test_df['text'], char_vocab, max_len_word, max_len_char)\n",
    "\n",
    "    # --- Get Labels ---\n",
    "    y_train = train_df['label'].values\n",
    "    y_val = val_df['label'].values\n",
    "    y_test = test_df['label'].values\n",
    "\n",
    "    # --- Create Datasets and DataLoaders ---\n",
    "    train_dataset = SentimentDataset(X_train_word_pad, y_train, X_train_char_pad)\n",
    "    val_dataset = SentimentDataset(X_val_word_pad, y_val, X_val_char_pad)\n",
    "    test_dataset = SentimentDataset(X_test_word_pad, y_test, X_test_char_pad)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # --- Loop Through Models ---\n",
    "    for model_name in DL_MODELS_TO_RUN:\n",
    "        print(f\"\\n--- Running Model: {model_name} | Embedding: {EMBEDDING_TYPE} ---\")\n",
    "\n",
    "        current_embedding_matrix = None\n",
    "        if EMBEDDING_TYPE != \"Learned\":\n",
    "             current_embedding_matrix = pretrained_embedding_matrix # Use loaded GloVe/etc.\n",
    "             if current_embedding_matrix is None and model_name not in [\"MLPClassifier\", \"WordCharParallelClassifier\"]: # MLP/Parallel handle None\n",
    "                print(f\"Skipping {model_name} as pre-trained embeddings ({EMBEDDING_TYPE}) failed to load.\")\n",
    "                continue\n",
    "\n",
    "        # Instantiate model\n",
    "        try:\n",
    "            if model_name == \"MLPClassifier\": model = MLPClassifier(current_word_vocab_size, EMBEDDING_DIM, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix)\n",
    "            elif model_name == \"RNNClassifier\": model = RNNClassifier(current_word_vocab_size, EMBEDDING_DIM, HIDDEN_DIM_RNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix)\n",
    "            elif model_name == \"CNNClassifier\": model = CNNClassifier(current_word_vocab_size, EMBEDDING_DIM, NUM_FILTERS_CNN, KERNEL_SIZES_CNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix)\n",
    "            elif model_name == \"LSTMClassifier\": model = LSTMClassifier(current_word_vocab_size, EMBEDDING_DIM, HIDDEN_DIM_RNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix)\n",
    "            elif model_name == \"BiLSTMClassifier\": model = BiLSTMClassifier(current_word_vocab_size, EMBEDDING_DIM, HIDDEN_DIM_RNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix)\n",
    "            elif model_name == \"CNNStaticEmbClassifier\": model = CNNClassifier(current_word_vocab_size, EMBEDDING_DIM, NUM_FILTERS_CNN, KERNEL_SIZES_CNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix) # Alias\n",
    "            elif model_name == \"LSTMStaticEmbClassifier\": model = LSTMClassifier(current_word_vocab_size, EMBEDDING_DIM, HIDDEN_DIM_RNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix) # Alias\n",
    "            elif model_name == \"CNNLSTMHybridClassifier\": model = CNNLSTMHybridClassifier(current_word_vocab_size, EMBEDDING_DIM, NUM_FILTERS_CNN, KERNEL_SIZES_CNN[0], HIDDEN_DIM_RNN, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix) # Using first kernel size for CNN part\n",
    "            elif model_name == \"WordCharParallelClassifier\":\n",
    "                if char_vocab is None:\n",
    "                    print(\"Skipping WordCharParallelClassifier as char vocab not built.\")\n",
    "                    continue\n",
    "                model = WordCharParallelClassifier(current_word_vocab_size, current_char_vocab_size, EMBEDDING_DIM, CHAR_EMBEDDING_DIM, HIDDEN_DIM_RNN, CHAR_CNN_FILTERS, CHAR_KERNEL_SIZE, OUTPUT_DIM, DROPOUT_PROB, current_embedding_matrix)\n",
    "            else:\n",
    "                print(f\"Model {model_name} not defined. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            model = model.to(DEVICE)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.BCEWithLogitsLoss().to(DEVICE) # Binary classification\n",
    "\n",
    "            # --- Training Loop with Early Stopping ---\n",
    "            best_val_loss = float('inf')\n",
    "            epochs_no_improve = 0\n",
    "            total_train_time = 0\n",
    "            model_save_path = os.path.join(MODEL_SAVE_DIR, domain, f\"{model_name.lower()}_{EMBEDDING_TYPE.lower()}.pt\")\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                t_epoch_start = time()\n",
    "                train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, model_name)\n",
    "                val_results = evaluate(model, val_loader, criterion, model_name)\n",
    "                val_loss = val_results[\"Loss\"]\n",
    "                epoch_time = time() - t_epoch_start\n",
    "                total_train_time += epoch_time\n",
    "\n",
    "                print(f'Epoch: {epoch+1:02}/{NUM_EPOCHS} | Time: {epoch_time:.2f}s')\n",
    "                print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "                # Val results printed in evaluate()\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    epochs_no_improve = 0\n",
    "                    print(f\"\\tValidation loss decreased ({best_val_loss:.4f}). Saving model...\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"\\tValidation loss did not improve. Patience: {epochs_no_improve}/{PATIENCE}\")\n",
    "\n",
    "                if epochs_no_improve >= PATIENCE:\n",
    "                    print(\"\\tEarly stopping!\")\n",
    "                    break\n",
    "\n",
    "            # --- Final Evaluation on Test Set ---\n",
    "            print(f\"\\nLoading best model from {model_save_path} and evaluating on test set...\")\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(model_save_path))\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Best model file not found at {model_save_path}. Evaluating last state.\")\n",
    "                # If no model was saved (e.g., validation loss never improved),\n",
    "                # the model variable holds the state from the last epoch.\n",
    "\n",
    "            t_eval_start = time()\n",
    "            test_results = evaluate(model, test_loader, criterion, model_name)\n",
    "            eval_time = time() - t_eval_start\n",
    "            print(f\"Final Test Evaluation Time: {eval_time:.2f}s\")\n",
    "\n",
    "            # Store results\n",
    "            final_model_results = {\n",
    "                \"Domain\": domain,\n",
    "                \"Model\": model_name,\n",
    "                \"Embedding\": EMBEDDING_TYPE,\n",
    "                \"Accuracy\": test_results[\"Accuracy\"],\n",
    "                \"Precision (Macro)\": test_results[\"Precision (Macro)\"],\n",
    "                \"Recall (Macro)\": test_results[\"Recall (Macro)\"],\n",
    "                \"F1 (Macro)\": test_results[\"F1 (Macro)\"],\n",
    "                \"Precision (Weighted)\": test_results[\"Precision (Weighted)\"],\n",
    "                \"Recall (Weighted)\": test_results[\"Recall (Weighted)\"],\n",
    "                \"F1 (Weighted)\": test_results[\"F1 (Weighted)\"],\n",
    "                \"Train Time (s)\": total_train_time,\n",
    "                \"Eval Time (s)\": eval_time,\n",
    "                \"Best Val Loss\": best_val_loss if best_val_loss != float('inf') else np.nan\n",
    "            }\n",
    "            all_results_list.append(final_model_results)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR running model {model_name} on domain {domain}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Print detailed error traceback\n",
    "\n",
    "        finally:\n",
    "            # Clean up GPU memory after each model run\n",
    "            del model\n",
    "            del optimizer\n",
    "            del criterion\n",
    "            if 'batch' in locals(): del batch\n",
    "            if 'words' in locals(): del words\n",
    "            if 'labels' in locals(): del labels\n",
    "            if 'chars' in locals(): del chars\n",
    "            if 'predictions' in locals(): del predictions\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3a793",
   "metadata": {},
   "source": [
    "# 9. Aggregate and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bee234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Experiment Finished ---\")\n",
    "if all_results_list:\n",
    "    results_df = pd.DataFrame(all_results_list)\n",
    "    # Define desired column order\n",
    "    cols_order = [\"Domain\", \"Model\", \"Embedding\", \"Accuracy\",\n",
    "                  \"F1 (Macro)\", \"Precision (Macro)\", \"Recall (Macro)\",\n",
    "                  \"F1 (Weighted)\", \"Precision (Weighted)\", \"Recall (Weighted)\",\n",
    "                  \"Train Time (s)\", \"Eval Time (s)\", \"Best Val Loss\"]\n",
    "    # Ensure all columns exist\n",
    "    for col in cols_order:\n",
    "        if col not in results_df.columns:\n",
    "            results_df[col] = np.nan\n",
    "    results_df = results_df[cols_order] # Reorder\n",
    "\n",
    "    print(\"\\nAggregated Results:\")\n",
    "    print(results_df.to_string()) # Print full dataframe\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df.to_csv(RESULTS_CSV_FILE, index=False)\n",
    "    print(f\"\\nResults saved to {RESULTS_CSV_FILE}\")\n",
    "else:\n",
    "    print(\"No results were generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
